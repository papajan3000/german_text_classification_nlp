{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 6 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Kapitelübersicht <a class=\"anchor\" id=\"6-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt hatten wir bei unseren Klassifizierungen nur den Vectorizer ausgetauscht. In diesem Kapitel schauen wir uns ein neues Klassifizierungsverfahren an, <b>Logistic Regression</b>. Wir werden Logistic Regression mit Naive Bayes vergleichen und dafür die Evaluierungstechniken aus dem vorherigen Kapitel nutzen.\n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[6.1. Kapitelübersicht](#6-1)<br>\n",
    "[6.2. Linear Regression](#6-2)<br>\n",
    "[6.3. Logistic Regression](#6-3)<br>\n",
    "[6.3.1. Intuition der Logistic Regression](#6-3-1)<br>\n",
    "[6.3.2. Voraussagen mit Logistic Regression ](#6-3-2)<br>\n",
    "[6.3.3. Logistic Regression mit mehr als zwei Klassen](#6-3-3)<br>\n",
    "[6.4. Logistic Regression vs. Naive Bayes](#6-4)<br>\n",
    "\n",
    "Am Ende dieses Kapitel werden wir folgende Themen behandelt und/oder vertieft haben:\n",
    "- Intuition der Linear Regression\n",
    "- Logistic Regression: Intuition und Formel\n",
    "- Logistische Funktion bzw. Sigmoid Funktion\n",
    "- Softmax Funktion\n",
    "- Maximum likelihood estimation\n",
    "- Multiple Binary Logistic Regression\n",
    "- Multinomial Logistic Regression\n",
    "- Implementierung von Logistic Regression in Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Linear Regression<a class=\"anchor\" id=\"6-2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Regression</b> ist ein Überbegriff für statistische Analyseverfahren, bei der die Beziehung von Variablen gefunden und modelliert werden soll. Eines der bekanntesten Regressionverfahren ist <b>Linear Regression</b>. Um <b>Logistic Regression</b> besser verstehen zu können, ist ein Vergleich mit dem ähnlichen Modell Linear Regression sinnvoll. Dabei soll nur die grobe Intuition der Linear Regression dargestellt werden, die von der Logistic Regression übernommen und weitergeführt wird.<br>\n",
    "<b>Linear Regression</b> (deutsch: Lineare Regression) ist eigentlich eine Technik aus der Statistik. Im Bereich des Machine Learning ist es nicht unüblich, dass Techniken aus der Statistik für Modellierungen verwendet werden. Es gibt verschiedene Varianten der Linear Regression, wir schauen uns hier nur die Simple Linear Regression an. <b>Simple Linear Regression</b> (deutsch: Lineare Einfachregression) ist ein <i>lineares Modell</i>, welches annimmt, dass eine lineare Beziehung zwischen einer Inputvariablen $ x $ und einer einzelnen Outputvariable $ y $ herrscht. Dieses Modell kann folgendermaßen repräsentiert werden:<br>\n",
    "\n",
    "$ y = B0 + B1 \\cdot x $<br>\n",
    "\n",
    "y = Output, der vorausgesagt wird<br>\n",
    "x = Input<br>\n",
    "B0 = Bias Koeffizient<br>\n",
    "B1 = Koeffizient für x<br>\n",
    "\n",
    "Wir können diese Formel benutzen, um Voraussagen für Variablen zu treffen. Angenommen, wir versuchen das Gewicht eines Menschen anhand seiner Größe vorauszusagen. Das Gewicht ist hier $y$ und die Größe $x$. Wir setzen dies in unsere Formel ein:<br>\n",
    "\n",
    "$ Gewicht = B0 + B1 \\cdot Größe $<br>\n",
    "\n",
    "Für B0 und B1 setzen wir nun 0.1 und 0.5. Das Ziel der Simple Linear Regression ist es, diese beiden Werte nach und nach anzupassen, damit sie besser auf unsere Daten passen.<br>\n",
    "\n",
    "$ Gewicht = 0.1 + 0.5 \\cdot 182 $<br>\n",
    "$ Gewicht = 91.1 $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphisch kann man sich die Simple Linear Regression folgendermaßen vorstellen:\n",
    "\n",
    "<img src=\"https://files.realpython.com/media/fig-lin-reg.a506035b654a.png\" alt=\"Linear Regression example\" width=\"450px\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Logistic Regression <a class=\"anchor\" id=\"6-3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Regression</b> (deutsch: Logistische Regression) ist eigentlich ein mathematisches Modell, welches aus der Statistik kommt und auf der <b>logistischen Funktion</b> (auch <b>Sigmoid Funktion</b> genannt) basiert. Diese Funktion ist eine \"S\"-förmige Kurve und verwandelt jeden Input-Wert in einen Wert zwischen 0 und 1. \n",
    "<br>\n",
    "\n",
    "$ f(x) = \\frac{1}{1\\ +\\ e^{-x}}$<br>\n",
    "\n",
    "\n",
    "<img src=\"https://hvidberrrg.github.io/deep_learning/activation_functions/assets/sigmoid_function.png\" alt=\"logistic function\" style=\"width:350px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Logistic Regression berechnet die <i>Wahrscheinlichkeit</i>, dass ein Input der vorgegebenen Klasse zugehört. Diese vorgegebene Klasse ist oft einfach die erste Klasse. Ist bei einer binären Klassifikation die Wahrscheinlichkeit $ P > 0.5 $, wurde die vorgegebene Klasse vorausgesagt, ist $ P <= 0.5 $, wurde sie nicht vorausgesagt. Auch wenn die Logistische Regression das Wort \"Regression\" im Namen hat, ist es kein Regressionsverfahren, sondern ein Klassifizierungsverfahren.<br>\n",
    "\n",
    "Die Formel für die Logistic Regression ähnelt der <b>Linear Regression</b>:<br>\n",
    "\n",
    "$ y = \\frac{e^{(B0\\ +\\ B1 \\cdot x)}}{1\\ +\\ e^{(B0\\ +\\ B1 \\cdot x)}}$<br>\n",
    "\n",
    "$B0$ ist erneut der Bias Koeffizient und $B1$ ist der Koeffizient für den einfachen Inputwert $x$. Der große Unterschied zur Linear Regression besteht darin, dass man als Ergebnis der Logistic Regression einen Wert zwischen 0 und 1 erhält, während bei der Linear Regression das Ergebnis ein numerischer Wert ist, der kleiner als 0 oder größer als 1 sein kann. Da uns die Logistic Regression noch einmal beim Kapitel über Deep Learning begegnen wird (Kapitel 11 TODO), schauen wir uns die Formel und die Intuition dahinter genauer an. Dies machen wir anhand eines Beispiels.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. Intuition der Logistic Regression <a class=\"anchor\" id=\"6-3-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angenommen wir haben zwei Klassen: \"Erwachsener\" und \"Kind\". Wir wollen nun anhand der Körpergröße die Wahrscheinlichkeit $P$ modellieren, ob es sich bei einer gegebenen Körpergröße um einen Erwachsenen oder ein Kind handelt. Dies können wir so formulieren:<br>\n",
    "\n",
    "$ P(Klasse=Erwachsener|Größe) $<br>\n",
    "\n",
    "Alternativ können wir die Wahrscheinlichkeit auch so formulieren, dass ein Input $x$ zu einer Klasse $y$ gehört. Die Klasse \"Erwachsener\" ist hier gleich 1 und die Klasse \"Kind\" ist hier gleich 0. Der Input $x$ ist hier gleich der Körpergröße.<br>\n",
    "\n",
    "$ P(x) = P(y=1|x) $<br>\n",
    "\n",
    "Das setzen wir in die Formel für Logistic Regression ein und erhalten folgende Gleichung:<br>\n",
    "\n",
    "$ P(x) = \\frac{e^{(B0\\ +\\ B1 \\cdot x)}}{1\\ +\\ e^{(B0\\ +\\ B1 \\cdot x)}}$<br>\n",
    "\n",
    "Ohne auf die Mathematik einzugehen, die dahinter steht, können wir diese Gleichung in folgende Gleichung umformen ($ln$ ist hier der natürliche Logarithmus):<br>\n",
    "\n",
    "$ ln(\\frac{P(x)}{1\\ -\\ P(x)}) = B0\\ +\\ B1 \\cdot x $<br>\n",
    "\n",
    "Somit gleicht der rechte Teil der Gleichung der Formel für die <b> Simple Linear Regression</b>. Der linke Teil ist der Logarithmus der Wahrscheinlichkeit für die vorgegebene Klasse (hier \"Erwachsener\").<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun ein Problem: $x$ und $y$ sind bekannte Werte, doch wie bestimmen wir die Betawerte $B0$ und $B1$? Diese beiden Koeffizienten werden durch unsere Trainingsdaten \"erlernt\". Dazu brauchen wir einen <b>Lernalgorithmus</b>. Wir verwenden die <b>Maximum-likelihood estimation</b>-Methode (kurz: MLE). Dieser Algorithmus hat seine Ursprünge ebenfalls in der Statistik und ist ein parametrisches Schätzverfahren. Auch hier gehen wir nicht näher auf die Mathematik hinter MLE ein. Für den Moment reicht es zu wissen, dass MLE mithilfe eines <i>Minimierungsalgorithmus</i> versucht, die optimalen Werte für die Koeffizienten für die Trainingsdaten zu finden, indem die Fehlerrate der falschen Zuweisungen minimiert wird. Optimale Werte für die Koeffizienten wären in unserem Fall Werte, die sehr nah an 1 (hier: \"Erwachsener\") und sehr nahe an 0 (hier: \"Kind\") liegen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. Voraussagen mit Logistic Regression <a class=\"anchor\" id=\"6-3-2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nehmen nun an, dass wir die <b>MLE</b>-Methode auf unseren Datensatz angewendet haben. Als Ergebnis erhielten wir für $B0 = -100$ und für $B1 = 0.6$. Wir bekommen nun die Korpergröße \"150cm\" und sollen anhand unseres Logistic Regression Modells voraussagen, ob dies eher die Größe eines Erwachsenen oder eines Kindes ist. Dies schreiben wir folgendermaßen:<br>\n",
    "\n",
    "$ P(Erwachsener|Größe=150)$<br>\n",
    "\n",
    "Nun setzen wir dies in die Formel für Logistic Regression ein:<br>\n",
    "\n",
    "$ y = \\frac{e^{(B0\\ +\\ B1 \\cdot x)}}{1\\ +\\ e^{(B0\\ +\\ B1 \\cdot x)}}\n",
    "    \\\\ = \\frac{e^{(-100\\ +\\ 0.6 \\cdot 150)}}{1\\ +\\ e^{(-100\\ +\\ 0.6 \\cdot 150)}}\n",
    "    \\\\ = 0.000045398$ <br>\n",
    "    \n",
    "Die Wahrscheinlichkeit, dass die Person mit der Größe 150cm ein Erwachsener ist, tendiert laut unser Formel gegen 0. Nehmen wir als zweites Beispiel eine Person mit der Größe 180cm:<br>\n",
    "\n",
    "$ P(Erwachsener|Größe=180)$<br>\n",
    "\n",
    "Nun setzen wir dies in die Formel für Logistic Regression ein:<br>\n",
    "\n",
    "$ y = \\frac{e^{(B0\\ +\\ B1 \\cdot x)}}{1\\ +\\ e^{(B0\\ +\\ B1 \\cdot x)}}\n",
    "    \\\\ = \\frac{e^{(-100\\ +\\ 0.6 \\cdot 180)}}{1\\ +\\ e^{(-100\\ +\\ 0.6 \\cdot 180)}}\n",
    "    \\\\ = 0,99966465$ <br>\n",
    "    \n",
    "Die Wahrscheinlichkeit, dass die Person mit der Größe 180cm ein Erwachsener ist, tendiert laut unser Formel gegen 1, d.h. die Wahrscheinlichkeit ist sehr hoch, dass sie ein Erwachsener ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3. Logistic Regression mit mehr als zwei Klassen <a class=\"anchor\" id=\"6-3-3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem Beispiel hatten wir die <b>binary Logistic Regression</b> (deutsch: binäre Logistische Regression) behandelt, die nur mit zwei Klassen arbeitet. <b>Multiclass Logistic Regression</b> kann Voraussagen für mehr als zwei Klassen treffen. Es existieren mehrere Ansätze, Multiclass Logistic Regression anzuwenden:<br>\n",
    "\n",
    "<b>Multiple Binary Logistic Regression</b> (auch <b>One vs. Rest Logistic Regression</b> genannt): Hierbei wird auf die binary Logistic Regression zurückgegriffen: Bei der Voraussage einer Klasse werden alle anderen Klasse temporär in einer Klasse zusammengefasst. Dies wird für jede Klasse wiederholt und für jede Kombination wird eine binary Logistic Regression berechnet. Die Voraussage mit dem höchsten Wert wird dann als Klassenzuweisung verwendet.<br>\n",
    "\n",
    "\n",
    "<b>Multinomial Logistic Regression</b>: Bei der Multinomial Logistic Regression wird die logistische Funktion bzw. die Sigmoid-Funktion durch die <b>Softmax-Funktion</b> ausgetauscht. Die Formel für die Softmax-Funktion ist:<br>\n",
    "\n",
    "$ \\text{Softmax}(y_i) = \\frac{e^{y_i}}{\\sum_{j} e^{y_j}}$ <br>\n",
    "\n",
    "Die Softmax-Funktion wandelt Zahlen in Wahrscheinlichkeiten um, deren Summe 1.0 ergibt. Es ist eine Normalisierungsfunktion, die gebraucht wird, wenn mehr als zwei Klassen klassifiziert werden müssen. Für einen Input $x$ wird ein Wert $s_k(x)$ für jede Klasse $k$ berechnet. Bei 10 Klassen erhalten wir also 10 Werte. Die Wahrscheinlichkeit für jede Klasse wird dann mit der Softmax-Funktion berechnet, der die Werte $s_k(x)$ für jede Klasse $k$ übergeben werden.<br>\n",
    "\n",
    "Der Unterschied zwischen diesen beiden Methoden ist, dass bei der Multiple Binary Classification für jede Klasse ein Klassifikationsverfahren berechnet wird und diese dann kombiniert werden. Bei der Multinomial Logistic Regression hingegen werden alle Klassen nur einem Klassifizierungsverfahren übergeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Logistic Regression vs. Naive Bayes <a class=\"anchor\" id=\"6-4\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression wird ähnlich wie Naive Bayes implementiert. Die Ausführung des Codeblocks dauert ein paar Sekunden. Anschließend implementieren wir Naive Bayes noch einmal und vergleichen die beiden Verfahren miteinander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit Logistic Regression ist 0.904.\n",
      "\n",
      "Der F1-score für die Klassifizierung mit Logistic regression ist 0.934.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
    "vector  = TfidfVectorizer().fit_transform(corpus[\"text\"])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "lr_classifier = LogisticRegression()\n",
    "lr = lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# cross validation des Trainingsdatensatzes\n",
    "lr_scores = cross_val_score(lr_classifier, vector, labels, cv=3)\n",
    "lr_mean = np.mean(lr_scores)\n",
    "\n",
    "print(\"Der Mittelwert der cross validation bei der  Klassifizierung \" \n",
    "      + f\" mit Logistic Regression ist {str(np.around(lr_mean, decimals=3))}.\"\n",
    "      + \"\\n\")\n",
    "\n",
    "\n",
    "# F1-score des Testdatensatzes\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "lr_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(\"Der F1-score für die Klassifizierung mit Logistic regression ist \"\n",
    "      + f\"{str(np.around(lr_f1, decimals=3))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit Multinomial Naive Bayes ist 0.859.\n",
      "\n",
      "Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist 0.87.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb = mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# cross validation des Trainingsdatensatzes\n",
    "mnb_scores = cross_val_score(mnb_classifier, vector, labels, cv=3)\n",
    "mnb_mean = np.mean(mnb_scores)\n",
    "\n",
    "print(\"Der Mittelwert der cross validation bei der  Klassifizierung \" \n",
    "      + f\" mit Multinomial Naive Bayes ist {str(np.around(mnb_mean, decimals=3))}.\"\n",
    "      + \"\\n\")\n",
    "\n",
    "\n",
    "# F1-score des Testdatensatzes\n",
    "y_pred = mnb_classifier.predict(X_test)\n",
    "mnb_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(\"Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist \"\n",
    "      + f\"{str(np.around(mnb_f1, decimals=3))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Werte zeigen eindeutig, dass Logistic Regression besser klassifiziert als Naive Bayes. Wir erhalten einen F1-score von 0.934 und bei der cross validation einen Mittelwert von 0.904."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression wird im Bereich des NLP häufig als Alternative zum Naive Bayes verwendet. Der Vorteil liegt darin, dass Logistic Regression keine Unabhängigkeit der Features voraussetzt (siehe Kapitel 3). Logistic Regression ist jedoch deutlicher komplexer als Naive Bayes und braucht mehr Zeit bei der Ausführung. Vor allem bei vielen Klassen kann Logistic Regression sehr viel Zeit in Anspruch nehmen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
