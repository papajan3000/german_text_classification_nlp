{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Kapitelübersicht <a class=\"anchor\" id=\"10-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis zu diesem Kapitel hatten wir immer eine Bag-of-Words Repräsentation für unsere Daten benutzt. In Kapitel 4 hatten wir diese durch eine Tf-Idf-Gewichtung erweitert. In diesem Kapitel werden wir ... TODO\n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[10.1. Kapitelübersicht](#10-1)<br>\n",
    "\n",
    "\n",
    "Am Ende dieses Kapitel werden wir folgende Themen behandelt und/oder vertieft haben:\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Grenzen des Bag-of-Words Modell <a class=\"anchor\" id=\"10-2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Bag-of-Words Modell ist sehr einfach zu verstehen und simpel zu implementieren. Es ist flexibel und kann z.B. mit der tf-idf-Gewichtung erweitert werden. Jedoch ist das Bag-of-Words Modell auch limitiert: Eines der größten Schwächen ist die Missachtung der semantischen Beziehung von Wörtern. Synonyme werden ignoriert. Weiterhin hat das Bag-of-Words Modell das Problem, dass je größer das Vokabular wird, desto größer können einzelne Wortvektoren werden. Diese sind sparse, da sie fast nur aus Nullen bestehen. TODO MEHR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MACHINE LEARNING MASTERY\n",
    "\n",
    "Das Bag-of-Word-Modell ist sehr einfach zu verstehen und zu implementieren und bietet viel Flexibilität bei der Anpassung Ihrer spezifischen Textdaten.\n",
    "\n",
    "Es wurde mit großem Erfolg bei Vorhersageproblemen wie Sprachmodellierung und Dokumentationsklassifizierung eingesetzt.\n",
    "\n",
    "Dennoch weist es einige Mängel auf, wie z.\n",
    "\n",
    "    Wortschatz: Der Wortschatz erfordert eine sorgfältige Gestaltung, insbesondere um die Größe zu verwalten, was sich auf die Sparsamkeit der Dokumentendarstellungen auswirkt.\n",
    "    Sparsity: Sparse Repräsentationen sind sowohl aus rechnerischen Gründen (Raum- und Zeitkomplexität) als auch aus Informationsgründen schwieriger zu modellieren, wenn die Modelle die Herausforderung haben, so wenig Informationen in einem so großen Repräsentationsraum zu nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word Embedding</b> (deutsch: Worteinbettung) ist die Sammelbezeichnung für eine Reihe von Sprachmodellierungstechniken. Sie bieten eine andere Form der Wortrepräsentation als das Bag-of-Words-Modell, bei der Wörter mit einer ähnlichen Bedeutung ähnlich dargestellt werden, wobei der <b>Kontext</b> der Wörter berücksichtigt wird. Word Embeddings repräsentieren Wörter als Vektoren in einem multidimensionalen semantischen Raum. Diesen Raum kann man sich etwa folgendermaßen vorstellen:<br><br>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1291/1*Fat62b1ZITOFMPXTcHNkLw.jpeg\" alt=\"Word Embedding Space\" width=\"600px;\"/><br>\n",
    "\n",
    "In diesem Raum werden Wörter, die ähnlich zueinander sind, näher beieinander platziert. \n",
    "\n",
    "TODO MEHR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4. Nutzung der Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO erklären:\n",
    "- word2vec\n",
    "- Glove\n",
    "- fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec, Word2Vec, FastText\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text in Lowercase\n",
    "#corpus[\"text\"] = corpus[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fernsehserie nach Staat</td>\n",
       "      <td>107</td>\n",
       "      <td>Tāmar und Schawqīya (arabisch تامر وشوقية, DMG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fernsehserie nach Staat</td>\n",
       "      <td>518</td>\n",
       "      <td>Achtung: Streng geheim! (Originaltitel: Missio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fernsehserie nach Staat</td>\n",
       "      <td>877</td>\n",
       "      <td>Alien Surfgirls ist eine australische Jugendse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fernsehserie nach Staat</td>\n",
       "      <td>1199</td>\n",
       "      <td>All Saints ist eine australische Fernsehserie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fernsehserie nach Staat</td>\n",
       "      <td>142</td>\n",
       "      <td>Alle lieben Diggy ist eine australische Zeiche...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   category  length  \\\n",
       "id                                    \n",
       "0   Fernsehserie nach Staat     107   \n",
       "1   Fernsehserie nach Staat     518   \n",
       "2   Fernsehserie nach Staat     877   \n",
       "3   Fernsehserie nach Staat    1199   \n",
       "4   Fernsehserie nach Staat     142   \n",
       "\n",
       "                                                 text  \n",
       "id                                                     \n",
       "0   Tāmar und Schawqīya (arabisch تامر وشوقية, DMG...  \n",
       "1   Achtung: Streng geheim! (Originaltitel: Missio...  \n",
       "2   Alien Surfgirls ist eine australische Jugendse...  \n",
       "3   All Saints ist eine australische Fernsehserie ...  \n",
       "4   Alle lieben Diggy ist eine australische Zeiche...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = []\n",
    "\n",
    "for article in corpus[\"text\"]:\n",
    "    text.append(sent_tokenize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FastText(text, size=100, window=5, min_count=5, workers=6, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chr.).', 0.9991431832313538),\n",
       " ('Jahrtausends v. Chr.', 0.9991098046302795),\n",
       " ('Chr.)', 0.9991024732589722),\n",
       " ('Jahrhunderts v. Chr.', 0.9990993738174438),\n",
       " ('Jahrhundert.', 0.9990963935852051),\n",
       " ('v. Chr.', 0.9990937113761902),\n",
       " ('Jahrhundert zurück.', 0.9990932941436768),\n",
       " ('Jahrtausend v. Chr.', 0.9990925192832947),\n",
       " ('Chr.', 0.9990907907485962),\n",
       " ('Jahrhundert v. Chr.', 0.9990905523300171)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar(\"jahrhundert\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tutorialdata/embeddings/smallwikicorpus_v2_word2vec.bin') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('tutorialdata/embeddings/smallwikicorpus_v2_embeddings.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "class MyDataframeCorpus(object):\n",
    "    def __init__(self, source_df, text_col, tag_col):\n",
    "        self.source_df = source_df\n",
    "        self.text_col = text_col\n",
    "        self.tag_col = tag_col\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, row in self.source_df.iterrows():\n",
    "            yield TaggedDocument(words=simple_preprocess(row[self.text_col]), \n",
    "                                 tags=[row[self.tag_col]])\n",
    "\n",
    "corpus_for_doc2vec = MyDataframeCorpus(corpus, 'text', 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "text = []\n",
    "for i, row in corpus.iterrows():\n",
    "    text.append(TaggedDocument(words=simple_preprocess(row[\"text\"]), tags=[row[\"category\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_documents = text[:5000]\n",
    "test_documents = text[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, vector_size=300, \n",
    "                negative=5, hs=0, min_count=2, sample = 0, \n",
    "                workers=6, alpha=0.025, min_alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab([x for x in train_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "train_documents  = utils.shuffle(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_documents, total_examples=len(train_documents), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for movie plots0.03398558187435633\n",
      "Testing F1 score for movie plots: 0.034716454602837114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_train, X_train = vector_for_learning(model, train_documents)\n",
    "y_test, X_test = vector_for_learning(model, test_documents)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy for movie plots%s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score for movie plots: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for movie plots0.12976313079299692\n",
      "Testing F1 score for movie plots: 0.14011525020086085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy for movie plots%s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score for movie plots: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= []\n",
    "\n",
    "for i, row in corpus.iterrows():\n",
    "    a.append(TaggedDocument(words=simple_preprocess(row[\"text\"]), tags=[row[\"category\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: hier weiter, ganz wichtig!!!!!!! hat funktioniert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-8048e0230490>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8048e0230490>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    array_length = 20 * 300embedding_features = pd.DataFrame()\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "array_length = 20 * 300\n",
    "embedding_features = pd.DataFrame()\n",
    "for document in corpus[\"text\":\n",
    "    # Saving the first 20 words of the document as a sequence\n",
    "    words = text_to_word_sequence(document)[0:20] \n",
    "    \n",
    "    # Retrieving the vector representation of each word and \n",
    "    # appending it to the feature vector \n",
    "    feature_vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            feature_vector = np.append(feature_vector, \n",
    "                                       np.array(embeds[word]))\n",
    "        except KeyError:\n",
    "            # In the event that a word is not included in our \n",
    "            # dictionary skip that word\n",
    "            pass    # If the text has less then 20 words, fill remaining vector with\n",
    "    # zeros\n",
    "    zeroes_to_add = array_length - len(feature_vector)\n",
    "    feature_vector = np.append(feature_vector, \n",
    "                               np.zeros(zeroes_to_add)\n",
    "                               ).reshape((1,-1))\n",
    "    \n",
    "    # Append the document feature vector to the feature table\n",
    "    embedding_features = embedding_features.append( \n",
    "                                     pd.DataFrame(feature_vector))\n",
    "print embedding_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit Multinomial Naive Bayes ist 0.859.\n",
      "\n",
      "Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist 0.87.\n"
     ]
    }
   ],
   "source": [
    "# Korpus laden\n",
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\", index_col=0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
    "vector  = TfidfVectorizer().fit_transform(corpus[\"text\"])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "\n",
    "  \n",
    "# Multinomial Naive Bayes\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb = mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# cross validation des Trainingsdatensatzes\n",
    "mnb_scores = cross_val_score(mnb_classifier, vector, labels, cv=3)\n",
    "mnb_mean = np.mean(mnb_scores)\n",
    "\n",
    "print(\"Der Mittelwert der cross validation bei der  Klassifizierung \" \n",
    "      + f\" mit Multinomial Naive Bayes ist {str(np.around(mnb_mean, decimals=3))}.\"\n",
    "      + \"\\n\")\n",
    "\n",
    "\n",
    "# F1-score des Testdatensatzes\n",
    "y_pred = mnb_classifier.predict(X_test)\n",
    "mnb_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(\"Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist \"\n",
    "      + f\"{str(np.around(mnb_f1, decimals=3))}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 2. 2. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = [corpus[\"text\"][1][:100], corpus[\"text\"][2][:100], corpus[\"text\"][3][:100]]\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let It Roll : Songs by George Harrison ist das dritte Kompilationsalbum von George Harrison nach der',\n",
       " 'Lieder wie Orkane ist das dritte offizielle Best-of-Album der deutschen Rockband Böhse Onkelz . Es e']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[corpus[\"text\"][1][:100], corpus[\"text\"][2][:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Stick Goes Boom : The Anthology ist eine Best-of-Kompilation der schweizerischen Hard-Rock-Band\n",
      "[28, 29, 30, 31, 32, 33, 1, 34, 7, 8, 35, 2, 36, 37, 38, 39]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_docs = [corpus[\"text\"][1][:100], corpus[\"text\"][2][:100], corpus[\"text\"][3][:100]]\n",
    "test_docs = [corpus[\"text\"][4][:100], corpus[\"text\"][5][:100]]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_docs)\n",
    "X_test = tokenizer.texts_to_sequences(test_docs)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(train_docs[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "corpus = corpus.join(corpus.text.apply(sent_tokenize).rename('sentences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [text for text in corpus[\"sentences\"]]\n",
    "train = l[:4800]\n",
    "test = l[4800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lieder wie Orkane ist das dritte offizielle Best-of-Album der deutschen Rockband Böhse Onkelz .', 'Es erschien am 2 .', 'Dezember 2011 über das bandeigene Label Rule23 Recordings .', 'Die Kompilation enthält vier CDs und eine DVD mit insgesamt 50 Titeln.Das Album enthält Titel von vielen verschiedenen Alben der Band .', 'Darunter auch Live-Aufnahmen der Lieder Erinnerungen aus Berlin und Nichts ist für die Ewigkeit aus Dortmund .', 'Außerdem sind vier zuvor unveröffentlichte Live-Mitschnitte der Songs Ich bin in dir , Auf gute Freunde , Ein langer Weg und Der Platz neben mir I & II von einem Konzert der Gruppe auf dem Loreley-Felsen im Jahr 2003 enthalten .', 'Diese sind auch visuell auf der DVD festgehalten.Das Albumcover ist schwarz-weiß gehalten .', 'Es zeigt einen gemalten schwarzen Tornado ( Orkan ) , auf dessen Spitze die Gesichter der vier Bandmitglieder , jeweils mit Sonnenbrille , zu sehen sind - von links nach rechts : Stephan Weidner , Kevin Russell , Matthias Röhr und Peter Schorowsky .', 'Links unten im Bild befinden sich die schwarzen Schriftzüge Böhse Onkelz und Lieder wie Orkane .', 'Der Hintergrund ist komplett weiß.Die Kompilation stieg in der 51 .', 'Kalenderwoche des Jahres 2011 auf Platz 7 in die deutschen Albumcharts ein und hielt sich elf Wochen in den Top 100.Thomas Kupfer vom Musikmagazin Rock Hard bezeichnete das Album als \" wertig aufgemachtes Package \" , das auch \" Songs , die eher unter der Rubrik \" vergessene und übersehene Perlen \" firmierten \" enthalte .', '== Einzelnachweise ==']\n",
      "[1248, 440, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train)\n",
    "X_test = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1191 1192 1193 1194 1195  262 1196 1197 1198 1199 1200 1201 1202 1203\n",
      " 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213  438 1214 1215 1216\n",
      " 1217 1218 1219 1220  438 1221 1222 1223 1224 1225 1226 1227 1228 1229\n",
      " 1230 1231 1232    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-26627ad683cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train2 = tokenizer.texts_to_sequences(X_train)\n",
    "X_test2 = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(X_train[2])\n",
    "print(X_train2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STRG+F \"Embedding\" --> auch für DL: https://machinelearningmastery.com/start-here/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICH: hier auch weitere text cleaning dinger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/effectively-pre-processing-the-text-data-part-1-text-cleaning-9ecae119cb3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Boxplots interpretiert werden, kann <a href=\"https://de.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review\">hier</a> nachgelesen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c28db2c950d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"InlineBackend.figure_format = 'svg'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msmaller_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Album nach Typ\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()\n",
    "plt.figure(figsize=(12, 6))\n",
    "smaller_corpus = corpus.loc[corpus[\"category\"] == \"Album nach Typ\"]\n",
    "print(sns.boxplot(y=\"category\", x=\"length\", data=corpus).set(title=\"Länge der Texte in Zeichen\", \n",
    "                                                       xlabel='Anzahl der Zeichen', ylabel=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.codecentric.de/2019/03/natural-language-processing-basics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import doc2vec, FastText\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def read_dataset(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataset.text, dataset.category, random_state=0, test_size=0.1)\n",
    "    x_train = label_sentences(x_train, 'Train')\n",
    "    x_test = label_sentences(x_test, 'Test')\n",
    "    all_data = x_train + x_test\n",
    "    return x_train, x_test, y_train, y_test, all_data\n",
    "\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the LabeledSentence method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the review.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.LabeledSentence(v.split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "\n",
    "def get_vectors(doc2vec_model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"Get vectors from trained doc2vec model.\n",
    "    \n",
    "        Args:\n",
    "            doc2vec_model: Trained Doc2Vec model.\n",
    "            corpus_size: Size of the data.\n",
    "            vectors_size: Size of the embedding vectors.\n",
    "            vectors_type: Training or Testing vectors.\n",
    "        Returns:\n",
    "            List of vectors.\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = doc2vec_model.docvecs[prefix]\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def train_doc2vec(corpus):\n",
    "    print(\"Building Doc2Vec vocabulary\")\n",
    "    d2v = doc2vec.Doc2Vec(min_count=1,  # Ignores all words with total frequency lower than this\n",
    "                          window=15,  # The maximum distance between the current and predicted word within a sentence\n",
    "                          vector_size=300,  # Dimensionality of the generated feature vectors\n",
    "                          workers=5,  # Number of worker threads to train the model\n",
    "                          alpha=0.025,  # The initial learning rate\n",
    "                          min_alpha=0.00025,  # Learning rate will linearly drop to min_alpha as training progresses\n",
    "                          dm=1)  # dm defines the training algorithm. If dm=1 means 'distributed memory' (PV-DM)\n",
    "                                 # and dm =0 means 'distributed bag of words' (PV-DBOW)\n",
    "    d2v.build_vocab(corpus)\n",
    "\n",
    "    print(\"Training Doc2Vec model\")\n",
    "    # 10 epochs take around 10 minutes on my machine (i7), if you have more time/computational power make it 20\n",
    "    for epoch in range(10):\n",
    "        print('Training iteration #{0}'.format(epoch))\n",
    "        d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "        # shuffle the corpus\n",
    "        random.shuffle(corpus)\n",
    "        # decrease the learning rate\n",
    "        d2v.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        d2v.min_alpha = d2v.alpha\n",
    "\n",
    "    logging.info(\"Saving trained Doc2Vec model\")\n",
    "    d2v.save(\"d2v.model\")\n",
    "    return d2v\n",
    "\n",
    "def train_fasttext(corpus):\n",
    "    fasttext = FastText(size=100, window=5, min_count=5)#, workers=6, sg=1)\n",
    "        \n",
    "    fasttext.build_vocab(corpus_file=corpus)\n",
    "    print(\"Training Fasttext model\")\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        print('Training iteration #{0}'.format(epoch))\n",
    "        fasttext.train(corpus_file=corpus, total_examples=len(corpus), epochs=fasttext.epochs)#total_examples=fasttext.corpus_count, epochs=fasttext.epochs)\n",
    "        # shuffle the corpus\n",
    "        random.shuffle(corpus)\n",
    "        # decrease the learning rate\n",
    "        fasttext.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        fasttext.min_alpha = fasttext.alpha\n",
    "        \n",
    "    logging.info(\"Saving trained FastText model\")\n",
    "    fasttext.save(\"fasttext.model\")\n",
    "    return fasttext\n",
    "\n",
    "\n",
    "def train_classifier(d2v, training_vectors, training_labels, scores_dict, classification_method):\n",
    "    logging.info(\"Classifier training\")\n",
    "    train_vectors = get_vectors(d2v, len(training_vectors), 300, 'Train')\n",
    "    \n",
    "    \n",
    "    if classification_method == \"svm\":\n",
    "        model = SVC(gamma=\"auto\", C=2.0, coef0=0.0, kernel=\"linear\")\n",
    "        model.fit(train_vectors, np.array(training_labels))\n",
    "        training_predictions = model.predict(train_vectors)\n",
    "    elif classification_method == \"logistic_regression\":\n",
    "        model = LogisticRegression()\n",
    "        model.fit(train_vectors, np.array(training_labels))\n",
    "        training_predictions = model.predict(train_vectors)    \n",
    "    \n",
    "    \n",
    "    logging.info('Training predicted classes: {}'.format(np.unique(training_predictions)))\n",
    "    logging.info('Training accuracy: {}'.format(accuracy_score(training_labels, training_predictions)))\n",
    "    scores_dict['Training accuracy'] = accuracy_score(training_labels, training_predictions)\n",
    "    logging.info('Training F1 score: {}'.format(f1_score(training_labels, training_predictions, average='weighted')))\n",
    "    scores_dict['Training F1 score'] = f1_score(training_labels, training_predictions, average='weighted')\n",
    "    \n",
    "    return model, scores_dict\n",
    "\n",
    "\n",
    "def test_classifier(d2v, classifier, testing_vectors, testing_labels, scores_dict):\n",
    "    logging.info(\"Classifier testing\")\n",
    "    test_vectors = get_vectors(d2v, len(testing_vectors), 300, 'Test')\n",
    "    testing_predictions = classifier.predict(test_vectors)\n",
    "    logging.info('Testing predicted classes: {}'.format(np.unique(testing_predictions)))\n",
    "    logging.info('Testing accuracy: {}'.format(accuracy_score(testing_labels, testing_predictions)))\n",
    "    scores_dict['Testing accuracy'] = accuracy_score(testing_labels, testing_predictions)\n",
    "    logging.info('Testing F1 score: {}'.format(f1_score(testing_labels, testing_predictions, average='weighted')))\n",
    "    scores_dict['Testing F1 score'] = f1_score(testing_labels, testing_predictions, average='weighted')\n",
    "    \n",
    "    return scores_dict\n",
    "    \n",
    "    \n",
    "\n",
    "def doc2vec_classification(path, classification_method):\n",
    "    \"\"\"\n",
    "    scores_dict = {}\n",
    "    x_train, x_test, y_train, y_test, all_data = read_dataset(path)\n",
    "    print(all_data[:100])\n",
    "    d2v_model = train_doc2vec(all_data)\n",
    "    classifier, scores_dict = train_classifier(d2v_model, x_train, y_train, scores_dict, classification_method)\n",
    "    scores_dict = test_classifier(d2v_model, classifier, x_test, y_test, scores_dict)\n",
    "    return scores_dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    scores_dict = {}\n",
    "    x_train, x_test, y_train, y_test, all_data = read_dataset(path)\n",
    "    d2v_model = train_fasttext(all_data)\n",
    "    classifier, scores_dict = train_classifier(d2v_model, x_train, y_train, scores_dict, classification_method)\n",
    "    scores_dict = test_classifier(d2v_model, classifier, x_test, y_test, scores_dict)\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alles über embeddings: https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe\n",
    "- relativ einfache elmo embeddings: https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
