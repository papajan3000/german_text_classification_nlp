{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 11 - Logistic Regression als simples Neuronales Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. Kapitelübersicht <a class=\"anchor\" id=\"11-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir uns der Implementierung von Neuronalen Netzen in Keras widmen, werden wir in diesem Kapitel das Klassifizierungsverfahren <b>Logistic Regression</b> als Neuronales Netz implementieren. Der eigentliche <a href=\"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb\">Code</a> wurde aus dem Coursera Kurs \"Deep Learning\" entnommen. Dort wurde in der ersten <a href=\"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb\">Programmieraufgabe</a> ein Datensatz mit Katzenbildern klassifiziert, wobei Bilder die Klassen \"Katze\" oder \"Keine Katze\" besaßen. Logistic Regression wurde dort als einfaches simples Neuronales Netz interpretiert. Der Code funktioniert sehr gut mit unserem Korpus, jedoch musste dieser vorher auf zwei Kategorien reduziert werden, da eine Multiclass Klassifizierung als Anschauungsbeispiel zu komplex wäre. Die Idee dieses Kapitels ist es, einen Einblick in die Implementation eines Neuronalen Netzes zu erhalten. Dieser Einblick umfasst die Grundschritte der Neuronalen Netze wie Initalisierung, Forward Propagation und Backward Propagation. Später werden wir auf die Deep-Learning-Bibliothek <b>Keras</b> zurückgreifen, bei der jedoch die zentralen Schritten des Trainings eines Neuronalen Netzes weniger präsent sind und die Layer in kurzen Ausdrücken zusammengefasst sind. \n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[11.1. Kapitelübersicht](#11-1)<br>\n",
    "[11.2. Reduzierung des Korpus auf 2 Klassen](#11-2)<br>\n",
    "[11.3. Implementierung des Neuronalen Netzes](#11-3)<br>\n",
    "[11.3.1. Die Sigmoid-Funktion](#11-3-1)<br>\n",
    "[11.3.2. Parameterinitalisierung mit Nullen](#11-3-2)<br>\n",
    "[11.3.3. Forward und Backward propagation](#11-3-3)<br>\n",
    "[11.3.4. Optimierung](#11-3-4)<br>\n",
    "[11.3.5. Vorrausage](#11-3-5)<br>\n",
    "[11.3.6. Das Modell](#11-3-6)<br>\n",
    "[11.4. Mögliche Fehler](#11-4)<br>\n",
    "\n",
    "\n",
    "Am Ende dieses Kapitel werden wir folgende Themen behandelt und/oder vertieft haben:\n",
    "- Umwandlung von Sparse Matrizen zu Dense Matrizen\n",
    "- Implementierungen Neuronaler Netze in `numpy` in fünf Schritten:\n",
    "    - Implementierung der Sigmoid-Funktion\n",
    "    - Parameterinitalisierung mit Nullen\n",
    "    - Forward Propagation und Backward Propagation\n",
    "    - Implementierung der Aktivierungsfunktion\n",
    "- Voraussagen von Neuronalen Netzen\n",
    "- Visualisierung der Cost-Score Kurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. Reduzierung des Korpus auf 2 Klassen <a class=\"anchor\" id=\"11-2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unser Korpus mit dem später entwickelten binären Classifier trainieren zu können, müssen wir es auf zwei Klassen reduzieren. Dazu wählen wir die Kategorien \"Computerspiel nach Plattform\" und \"Internet\" aus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laden und Reduzierung des Korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2646</td>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>353</td>\n",
       "      <td>Ballyhoo ist ein Computerspiel der US-amerikan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2644</td>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>379</td>\n",
       "      <td>Balance of Power ist ein Computer-Strategiespi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2645</td>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>415</td>\n",
       "      <td>Ballblazer ist ein Zweispieler-Computer-Sports...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2647</td>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>417</td>\n",
       "      <td>Barbarian : The Ultimate Warrior und Barbarian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2652</td>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>359</td>\n",
       "      <td>Beyond Zork ( kompletter Titel Beyond Zork : T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                      category  length  \\\n",
       "0  2646  Computerspiel nach Plattform     353   \n",
       "1  2644  Computerspiel nach Plattform     379   \n",
       "2  2645  Computerspiel nach Plattform     415   \n",
       "3  2647  Computerspiel nach Plattform     417   \n",
       "4  2652  Computerspiel nach Plattform     359   \n",
       "\n",
       "                                                text  \n",
       "0  Ballyhoo ist ein Computerspiel der US-amerikan...  \n",
       "1  Balance of Power ist ein Computer-Strategiespi...  \n",
       "2  Ballblazer ist ein Zweispieler-Computer-Sports...  \n",
       "3  Barbarian : The Ultimate Warrior und Barbarian...  \n",
       "4  Beyond Zork ( kompletter Titel Beyond Zork : T...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_computer = corpus[corpus.category == \"Computerspiel nach Plattform\"]\n",
    "corpus_internet = corpus[corpus.category == \"Internet\"]\n",
    "tinycorpus = corpus_computer.append(corpus_internet, ignore_index=True)\n",
    "tinycorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>342</td>\n",
       "      <td>Internet</td>\n",
       "      <td>182</td>\n",
       "      <td>Soribada ( koreanisch  'Ton herunterladen ' ) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>343</td>\n",
       "      <td>Internet</td>\n",
       "      <td>766</td>\n",
       "      <td>Streetspotr ist eine Plattform für standortbas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>344</td>\n",
       "      <td>Internet</td>\n",
       "      <td>158</td>\n",
       "      <td>The Real News ist ein globaler Online-Videonac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>333</td>\n",
       "      <td>Internet</td>\n",
       "      <td>930</td>\n",
       "      <td>OnLive war eine interaktive Kompressionsmethod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>290</td>\n",
       "      <td>Internet</td>\n",
       "      <td>396</td>\n",
       "      <td>Clickjacking ist eine Technik , bei der ein Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  category  length                                               text\n",
       "395  342  Internet     182  Soribada ( koreanisch  'Ton herunterladen ' ) ...\n",
       "396  343  Internet     766  Streetspotr ist eine Plattform für standortbas...\n",
       "397  344  Internet     158  The Real News ist ein globaler Online-Videonac...\n",
       "398  333  Internet     930  OnLive war eine interaktive Kompressionsmethod...\n",
       "399  290  Internet     396  Clickjacking ist eine Technik , bei der ein Co..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinycorpus.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufteilung in Trainings- und Testdatensätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein wichtige Veränderung ist hier die Umwandlung der Sparse Matrix `vector` in eine Dense Matrix. Dies erfolgt mit `toarray()`. Ohne diese würde das Neuronale Netz nicht arbeiten können (der erste Fehler würde schon bei der Implementierung der Sigmoid-Funktion erfolgen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = tinycorpus[\"category\"]\n",
    "vector  = TfidfVectorizer().fit_transform(tinycorpus[\"text\"]).toarray()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Umformung der Trainingsdatensätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eines der häufigsten Fehler, die bei der Arbeit mit Neuronalen Netzen gemacht wird, ist die falsche Benutzung von Matrix- und Vektordimensionen. Schauen wir uns die Dimensionen der Trainings- und Testdatensätze an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 31654)\n",
      "(80, 31654)\n",
      "(320,)\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train` hat die Dimension `(320, 31654)`. Diese hat die Form `(n,m)`, wobei `n` für die Anzahl der Zeilen und `m` für die Anzahl der Spalten steht. Die Matrix hat also 320 Zeilen und 31654 Spalten. Um unser Modell nutzen zu können, muss jede Spalte einen Datensatz repräsentieren. Bis jetzt repräsentieren die Zeilen jedoch einen Datensatz, weshalb wir die Matrix <u>transponieren</u> (= Zeilen werden zu Spalten und umgekehrt) müssen. Dies können wir mit dem Befehl `.T` durchführen. Da die Labels eindimensionale Matrizen, also Vektoren sind, müssen wir diese nicht transponieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31654, 320)\n",
      "(31654, 80)\n",
      "(320,)\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. Implementierung des Neuronalen Netzes <a class=\"anchor\" id=\"11-3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden nun Logistic Regression als simples Neuronales Netz mit `numpy` implementieren. Dazu brauchen wir nur fünf Funktionen. Diese werden ausführlich erklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1. Die Sigmoid-Funktion <a class=\"anchor\" id=\"11-3-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir in Kapitel 6 gesehen haben, basiert Logistic Regression auf der <b>Sigmoid-Funktion</b> (oder <b>Logistische Funktion</b>). Diese wandelt jeden Eingabewert in eine Zahl zwischen 0 und 1 um.<br>\n",
    "\n",
    "$ f(x) = \\frac{1}{1\\ +\\ e^{-x}}$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999546021312976\n",
      "0.5002499999791666\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(10))\n",
    "print(sigmoid(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],\n",
       "       ...,\n",
       "       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Implementation ist ziemlich simpel. Bereits hier zeigt sich jedoch, wieso wir die Daten gerne als Dense-Matrix vorliegen hätten. Versuchen wir nun einmal, die Daten als Sparse Matrix zu übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type csr_matrix which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: exp not found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5616ccd47d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msparse_vector\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtinycorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b683d5dd8fa4>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type csr_matrix which has no callable exp method"
     ]
    }
   ],
   "source": [
    "sparse_vector  = TfidfVectorizer().fit_transform(tinycorpus[\"text\"])\n",
    "sigmoid(sparse_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erhalten eine Fehlermeldung, da die numpy-Funktion `exp` nicht mit Sparse Matrizen arbeiten kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2. Parameterinitalisierung mit Nullen <a class=\"anchor\" id=\"11-3-2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes müssen wir die Parameter `w` und `b` mit Nullen initalisieren. Dafür müssen wir jedoch zunächst klären, was `w` und `b` überhaupt sind. Schauen wir uns noch einmal die Formel für Logistic Regression aus Kapitel 6 an:<br>\n",
    "\n",
    "$ y = \\frac{e^{(B0\\ +\\ B1 \\cdot x)}}{1\\ +\\ e^{(B0\\ +\\ B1 \\cdot x)}}$<br>\n",
    "\n",
    "Diese Formel ist eine Erweiterung der Formel für <b>Linear Regression</b>, die folgendermaßen aussieht:<br>\n",
    "\n",
    "$ y = B0 + B1 \\cdot x $<br>\n",
    "\n",
    "$y$ war der Output, der vorausgesagt werden soll und $x$ der Input. $B0$ war der Bias Koeffizient und $B1$ der Koeffizient für $x$. Diese Formel kann auch etwas verändert geschrieben werden:<br>\n",
    "\n",
    "$ y = w^T \\cdot x\\ +\\ b $<br>\n",
    "\n",
    "$b$ ist hier der Biaskoeffizient $B0$. $B1$ wird durch eine transponierte Gewichtsmatrix $w$ repräsentiert. Wie in Kapitel 6 sind diese Koeffizienten die Werte, die vom unseren Modell erlernt werden. In Kapitel 6 hatten wir diesen Lernprozess ausgeblendet, in diesem Kapitel werden wir in den nächsten Abschnitten genauer auf ihn eingehen. Die Formel ist nun aber die Formel für die Lineare Regression. Die Formel für die Logistic Regression sieht folgendermaßen aus:<br>\n",
    "\n",
    "$ y = \\frac{e^{(w^T \\cdot x\\ +\\ b)}}{1\\ +\\ e^{(w^T \\cdot x\\ +\\ b)}}$<br>\n",
    "\n",
    "\n",
    "Etwas allgemeiner können wir die Formel folgendermaßen formulieren ($\\sigma$ steht für die Sigmoid-Funktion):<br>\n",
    "\n",
    "$ y = \\sigma(w^T \\cdot x\\ +\\ b) $<br>\n",
    "\n",
    "$\\sigma$ ist eine sogenannte <b>Aktivierungsfunktion</b>. Neben der Sigmoid-Funktion hatten wir noch die <b>Softmax-Funktion</b> als Aktivierungsfunktion für <b>Multinomial Logistic Regression</b> kennengelernt. Noch allgemeiner könnten wir die Funktion folgendermaßen schreiben, wobei $a$ für eine beliebige Aktivierungsfunktion steht:\n",
    "\n",
    "$ y = a(w^T \\cdot x\\ +\\ b) $<br>\n",
    "\n",
    "Diese Formel wird in jedem Neuron unseres Neuronalen Netzwerks angewendet. Graphisch können wir uns das folgendermaßen vorstellen, wobei $a$ die Aktivierungsfunktion ist und $y$ die Funktion:\n",
    "\n",
    "<img src=\"tutorialdata/img/nn4_2_edit.png\" alt=\"Neural Network with 2 layers and Neuron labeling\" align=\"center\" width=\"600px;\"><br>\n",
    "\n",
    "`w` und `b` sind also Koeffizienten, die mithilfe eines Neuronalen Netzes erlernt werden. Jedoch müssen wir mit irgendwelchen Werten beginnen, weshalb wir sie hier mit Nullen initalisieren. `b` können wir dabei einfach gleich Null setzen, da `b` ein Skalar (= einzelner Wert) ist. `w` ist jedoch kein Skalar, sondern eine Matrix, dessen Länge bzw. Dimension wir mit `dim` angeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden später dieser Funktion `dim = 320` übergeben, da wir für jeden Trainingsdatensatz einen Eintrag mit einer Null erstellen wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_with_zeros(320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3. Forward und Backward propagation <a class=\"anchor\" id=\"11-3-3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir unsere Gewichte `w` und `b` initalisiert. Diese müssen jedoch angepasst werden, damit unser Modell sinnvolle Repräsentationen liefert. Würden wir sie nicht verändern, würden unsere Ergebnisse immer gleich bleiben und unser Modell wäre sinnlos. Das Modell muss die Gewichte `w` und `b` also <u>lernen</u>. Dieser Lernprozess findet in der sogenannten <b>Trainingsschleife</b> (englisch: training loop) statt. Diese lässt sich am besten mit der folgenden Grafik beschreiben:<br>\n",
    "\n",
    "<img src=\"tutorialdata/img/neuronal_network.png\" alt=\"Neuronal network\" align=\"center\" width=\"300px;\"><br><div style=\"text-align: center; font-size:10px;\">CHOLLET, Francois, Deep Learning mit Python und Keras, übers. von Knut LORENZEN, New York 2018, S. 31.</div></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst übergeben wir unsere Daten dem Modell (<b>Eingabe X</b>). Die Daten werden als Matrix repräsentiert, bei der jede Spalte für ein Trainingsbeispiel steht. In unserem Fall stellen 320 Spalten die 320 Trainingsbeispiele dar. Unser Input Layer hat also 320 Dimension, also auch 320 Neuronen. Nun bestimmen wir `w` und `b`, am Anfang noch mit Nullen oder zufälligen Werten. Das Ergebnis ist die Funktion $y$. Jedes unserer Neuronen aktiviert diese Funktion dann mithilfe einer <b>Aktivierungsfunktion $a$</b>. Das können wir hier sehen:<br>\n",
    "<img src=\"tutorialdata/img/nn4_2_edit.png\" alt=\"Neural Network with 2 layers and Neuron labeling\" align=\"center\" width=\"400px;\">\n",
    "\n",
    "Am Ende dieses Durchlaufs erhalten wir eine Voraussage $\\hat{y}$ (bis jetzt nannten wir dies `y_pred`). Stimmt diese mit dem tatsächlichen Label $y$ (nicht zu verwechseln mit der Funktion $y$, welche aus diesem Grund auch oft $z$ genannt wird) überein, hat unser Modell das Label richtig vorausgesagt. Zu Beginn ist es jedoch sehr unwahrscheinlich, dass unser Modell mit zufälligen Gewichten oder Gewichten bestehend aus Nullen das richtige Label voraussagt. Wir müssen also berechnen, wie sehr das Modell mit der Voraussage falsch gelegen hat. Dies wird mit der <b>Loss Function</b> (deutsch: <b>Verlustfunktion</b>) berechnet. Eine Loss Function berechnet jedoch nur den <b>Loss score</b> (deutsch: Verlustscore) von einem einzigen Trainingsbeispiel. Um die Loss scores aller Trainingsbeispiele zu berechnen, brauchen wir eine <b>Cost Function</b>, die den Durschnitt aller Loss scores für den gesamten Trainingsdatensatz berechnet.<br> Dieser gesamte Schritt wird <b>Forward Propagation</b> genannt. Eine gute Visualisierung bietet dieses <a href=\"https://www.youtube.com/watch?v=UJwK6jAStmg\">Video</a>.<br>\n",
    "\n",
    "Anhand des Loss scores können nun die Koeffizienten `w` und `b` aktualisiert werden. Diesen Vorgang nennt man <b>Backward Propagation</b>. Wie genau diese Backward Propagation funktioniert, wird in den Kapiteln 2.4.1 - 2.4.4 von Chollets Buch sehr gut erklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Aufgabe:</b> Kapitel 2 - Bevor es losgeht: die mathematischen Bausteine eines NNs <br>\n",
    "    \n",
    "Lesen Sie noch einmal die Kapitel 2.4.1 - 2.4.4. Versuchen Sie in ein oder zwei Sätzen zu erklären, was <b>gradient descent</b> (deutsch: Gradientenabstieg) ist und wie es funktioniert.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun jedoch zur Funktion `propagate`. Sie nimmt als Input die Gewichtsmatrix `w`, den Biaskoeffizienten `b`, die Trainingsdaten `X_train` und die Trainingslabel `y_train`. Wir bestimmen zunächst die Anzahl der Trainingsbeispiele `m`. Danach berechnen wir die Aktivierungsfunktion `A`. Nach dem ersten Aufruf sieht die Matrix `A` folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = initialize_with_zeros(X_train.shape[0])\n",
    "A = sigmoid(np.dot(w.T, X_train) + b)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir brauchen nun also eine Cost Function, die den Loss score für alle Trainingsbeispiele berechnet. Die Auswahl der Loss Function bzw. der Cost Function hängt von der jeweiligen Klassifizierung ab:\n",
    "- <b>binary cross entropy</b> für binäre Klassifizierung\n",
    "- <b>cross entropy</b> für Multiclass Klassifizierung\n",
    "\n",
    "Da wir hier eine binäre Klassifikation durchführen, verwenden wir die <b>binary cross entropy</b>. Die Formel dafür lautet: <br>\n",
    "\n",
    "$ \\text{binary cross entropy} = - (y\\ \\cdot log(\\hat{y}) + (1−y)\\ \\cdot log(1−\\hat{y}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Aufgabe:</b> Cross entropy <br>\n",
    "    \n",
    "Cross entropy ist uns in einer anderen Form bereits in Kapitel 6 begegnet und zwar in Form der <b>Maximum likehood estimation</b>-Methode. Lesen Sie den <a href=\"https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\">Blog-Eintrag</a> von Jason Brownlee zu <b>Cross entropy</b>. Schauen Sie sich danach die Formel für <b>binary cross entropy</b> an:<br>\n",
    "    \n",
    "$ \\text{binary cross entropy} = - (y\\ \\cdot log(\\hat{y}) + (1−y)\\ \\cdot log(1−\\hat{y}))$<br>\n",
    "\n",
    "$y$: das wahre Label<br>\n",
    "$\\hat{y}$: das vorgesagte Label<br>\n",
    "\n",
    "Setzen Sie folgende Werte für $y$ und $\\text{y}$ ein. Was beobachten Sie? Gehen Sie davon aus, dass $log(0)$ berechnet werden kann und $log(0) = -14$. <i>Tipp</i>: $log(1) = 0$.<br>\n",
    "\n",
    "a) $y = 0$; $\\hat{y} = 0$<br>\n",
    "b) $y = 1$; $\\hat{y} = 1$<br>\n",
    "c) $y = 0$; $\\hat{y} = 1$<br>\n",
    "d) $y = 1$; $\\hat{y} = 0$<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Loss Function für binary cross entropy ist:<br>\n",
    "\n",
    "$ L(\\hat{y}, y) = - (y\\ \\cdot log(\\hat{y}) + (1−y)\\ \\cdot log(1−\\hat{y}))$.<br><br>\n",
    "\n",
    "Die Formel für die Cost Function ist:<br>\n",
    "\n",
    "$ J(w, b)\\ =\\ -\\frac{1}{m} \\sum_{i=1}^m \\cdot\\ L(\\hat{y}, y)\\ =\\ -\\frac{1}{m} \\sum_{i=1}^m \\cdot\\ (y^{(i)}\\ \\cdot log(\\hat{y}^{(i)}) + (1−y^{(i)})\\ \\cdot log(1−\\hat{y}^{(i)}))$<br><br>\n",
    "\n",
    "Diese wird in der Funktion `propagate` in der Variable `cost` gespeichert. `np.squeeze` entfernt hier eindimensionale Einträge aus der Matrix. Damit garantieren wir, dass die Cost Function wirklich nur aus einem Wert besteht und keine redundante Achse enthält. Damit ist die Forward Propagation abgeschlossen. Nun implementieren wir die partiellen Ableitungen für `w` und `b`. Die Formeln dafür lauten folgendermaßen:<br>\n",
    "\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}\\ \\text{X_train}\\ \\cdot(A-\\text{y_train})^T$<br>\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\cdot\\ (a^{(i)}-y^{(i)})$<br>\n",
    "\n",
    "Diese speichern wir im Dictionary `grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X_train, Y_train):\n",
    "   \n",
    "    # m: Anzahl der Trainingsbeispiele\n",
    "    m = X_train.shape[1]\n",
    "    \n",
    "    ### Forward Propagation ###\n",
    "    \n",
    "    # Aktivierungsfunktion / Formel für Logistic Regression\n",
    "    A = sigmoid(np.dot(w.T, X_train) + b) \n",
    "    \n",
    "    # Cost Function\n",
    "    cost = (- 1 / m) * np.sum(Y_train * np.log(A) + (1 - Y_train) * (np.log(1 - A)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    \n",
    "    ### Backward Propagation ###\n",
    "    \n",
    "    # partiellen Ableitungen\n",
    "    dw = (1 / m) * np.dot(X_train, (A - Y_train).T)\n",
    "    db = (1 / m) * np.sum(A - Y_train)\n",
    "\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4. Optimierung <a class=\"anchor\" id=\"11-3-4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun mit `propagate` eine Funktion implementiert, die Forward und Backward Propagation implementiert. Jetzt müssen wir die `optimize` Funktion implementieren, die für mehrere Dinge verantwortlich ist:\n",
    "- Optimieren über mehrere Iterationen (durch `num_iterations`). Eine Iteration ist ein Durchlaufen der Trainingsschleife.\n",
    "- Aktualisierung von `w` und `b` mithilfe einer <b>Learning Rate</b> (deutsch: Lernrate). Diese Learning Rate ist ein Hyperparameter, der angibt, wie sehr wir die Gewichtsmatrix `w` und den Bias-Koeffizienten `b` in unserem Neuronalen Netz anpassen wollen.\n",
    "- Dokumentation des Cost-Scores alle hundert Iterationen.\n",
    "- Ausgabe des Cost-Scores alle hundert Trainingsbeispiele. Dies kann über `print_cost` gesteuert und \"abgeschaltet\" werden.\n",
    "- Speicherung von `w` und `b` in einem Parameter-Dictionary `params`, nachdem die Iterationen abgeschlossen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False):\n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grads, cost = propagate(w, b, X_train, Y_train)\n",
    "\n",
    "        # Ableitungen aus dem Grads-Dictionary\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        \n",
    "        # Aktualisierung von w und b\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Dokumentiert die Cost-Scores in einer Liste\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Bei jedem 100ten Trainingsbeispiel wird der Cost-Score ausgegeben\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.5. Voraussage <a class=\"anchor\" id=\"11-3-5\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt muss noch eine `predict`-Funktion implementiert werden, die wie die `predict`-Funktion von <b>Scikit learn</b> funktioniert. Wie Voraussagen mithilfe von Logistic Regression berechnet werden, hatten wir uns bereits in Kapitel 6.3.2 angeschaut. Alle Werte unserer Matrix `A`, die größer als $0.5$ sind, werden zu einer $1$ umgewandelt und alle anderen Werte zu einer $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X_train):\n",
    "    \n",
    "    m = X_train.shape[1]\n",
    "    y_pred = np.zeros((1, m))\n",
    "    w = w.reshape(X_train.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X_train) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        y_pred[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.6. Das Modell <a class=\"anchor\" id=\"11-3-6\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt werden alle Funktionen in der Funktion `lr_model` zusammengeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    model = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ec544d23b40d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                   print_cost = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-71c2f9cf904c>\u001b[0m in \u001b[0;36mlr_model\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_with_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d6215ade4931>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Ableitungen aus dem Grads-Dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d03c16488ed5>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(w, b, X_train, Y_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Cost Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_fill_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mmasked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "model = lr_model(X_train, \n",
    "                  y_train, \n",
    "                  X_test, \n",
    "                  y_test, \n",
    "                  num_iterations = 2000, \n",
    "                  learning_rate = 0.1, \n",
    "                  print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun visualisieren wir den Lernverlauf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "costs = np.squeeze(model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('Cost-Score')\n",
    "plt.xlabel('Iterations (pro Hunderte)')\n",
    "plt.title(\"Learning rate = \" + str(model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umso länger wir trainierten, umso geringer wurde der Cost-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Aufgabe:</b> Anspassung der Learning Rate und Iterationenanzahl <br>\n",
    "    \n",
    "Probieren Sie Werte für die Learning Rate aus und passen sie die Iterationenanzahl beliebig an. Visualiseren Sie jede Anpassung und vergleichen Sie die Graphen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Mögliche Fehler <a class=\"anchor\" id=\"11-4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fehlercode mit \"sparse\" in der Beschreibung → Daten wurden nicht mithilfe von `toarray()` in eine Dense Matrix (genauer eine ndarray-Repräsentation) transformiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
