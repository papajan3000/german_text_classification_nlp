{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 3 - Die \"klassischen\" Textklassifizierungsverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir unterteilen die Klassifizierungsmethoden in <b>probabilistische</b> und <b>nicht-probabilistische</b> Klassifizierungsverfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.? Grundbausteine eines Klassifizierungsverfahren bei sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.? Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Verfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das <b>Naive Bayes</b> Verfahren ist ein Klassifikationsverfahren, welches auf dem <b>Bayesschen Theorem</b> (auch <b>\"Satz von Bayes\"</b> genannt) von Thomas Bayes basiert. Beim Naive Bayes Verfahren wird <u>naiv</u> (deshalb auch der Name) angenommen, dass ein jedes Attribut nur vom Klassenattribut abhängt. Obwohl das in der Realität selten der Fall ist, liefert Naive Bayes in der Praxis meist gute Werte.<br>\n",
    "Das Naive Bayes Verfahren besteht nicht nur aus einem einzigen Algorithmus, sondern vielmehr aus einer Familie von Algorithmen, die alle auf dem gleichen Prinzip beruhen. Alle Algorithmen basieren jedoch auf der probabilistischen Methode. Wie wir im vorherigen Kapitel gesehen haben, bedeutet dies, dass Naive Bayes beispielsweise die Zugehörigkeit einer Kategorie zu einem Text für jede Kategorie berechnet und die höchste Wahrscheinlichkeit als Textkategorie zurückgibt. Die Wahrscheinlichkeiten werden durch das <b>Bayessche Theorem</b> ermittelt. Die Formel für das Theorem lautet: <br>\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A) \\ \\cdot \\ P(A)}{P(B)} $ &nbsp; [<sup>1</sup>](#fn1) <br>\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<span id=\"fn1\" style=\"font-size:8pt; line-height:1\"><sup style=\"font-size:5pt\">1</sup> &nbsp; Zwei anschauliche Rechenbeispiele, welche die Formel vertiefen, findet man auf der Wikipedia Seite vom Satz von Bayes: https://de.wikipedia.org/wiki/Satz_von_Bayes.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICH: binär und multipel!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein einfaches Beispiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um uns mit dem Naive Bayes Verfahren vertraut zu machen, werden wir zunächst ein einfaches Beispiel[<sup>2</sup>](#fn2) durchrechnen, bevor wir das Verfahren auf unseren Datensatz anwenden. Dazu werden wir das <b>Multinomial Naive Bayes</b> Verfahren nutzen, eine der vielen Varianten von Naive Bayes. Dieses Verfahren wird genutzt, wenn eine multinomiale Verteilung der Daten vorliegt und funktioniert z.B. gut mit Textdaten, bei denen man die Anzahl der Wörter bestimmen kann\n",
    "\n",
    "\n",
    "ICH: https://stats.stackexchange.com/questions/269024/confused-among-gaussian-multinomial-and-binomial-naive-bayes-for-text-classific\n",
    "\n",
    "hier weiter\n",
    "\n",
    "Der Grund, wieso wir dieses Verfahren nutzen, . Schauen wir uns zunächst den Datensatz an. \n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<span id=\"fn2\" style=\"font-size:8pt; line-height:1\"><sup style=\"font-size:5pt\">2</sup> &nbsp; Dieses Beispiel wurde von dem Naive Bayes Tutorial der Plattform <b>MonkeyLearn</b> übernommen, übersetzt und leicht abgewandelt: https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Kategorie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ein großartiges Spiel</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Das Urteil ist beschlossen</td>\n",
       "      <td>Kein Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ein sehr faires Match</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ein faires aber langweiliges Spiel</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Es ist ein knappes Urteil</td>\n",
       "      <td>Kein Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ein sehr knappes Spiel</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Text   Kategorie\n",
       "1               Ein großartiges Spiel       Sport\n",
       "2          Das Urteil ist beschlossen  Kein Sport\n",
       "3               Ein sehr faires Match       Sport\n",
       "4  Ein faires aber langweiliges Spiel       Sport\n",
       "5           Es ist ein knappes Urteil  Kein Sport\n",
       "6              Ein sehr knappes Spiel           ?"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news = {1 : [\"Ein großartiges Spiel\", \"Sport\"], \n",
    "        2 : [\"Das Urteil ist beschlossen\", \"Kein Sport\"],\n",
    "        3 : [\"Ein sehr faires Match\", \"Sport\"],\n",
    "        4 : [\"Ein faires aber langweiliges Spiel\", \"Sport\"],\n",
    "        5 : [\"Es ist ein knappes Urteil\", \"Kein Sport\"],\n",
    "        6 : [\"Ein sehr knappes Spiel\", \"?\"]}\n",
    "simple_df = pd.DataFrame.from_dict(news, orient=\"index\", columns=[\"Text\", \"Kategorie\"])\n",
    "\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen eine Tabelle, in der kurze Nachrichtenartikel in eine von zwei Kategorien eingeteilt wurden: \"Sport\" oder \"Kein Sport\". Der sechste Artikel \"Ein sehr knappes Spiel\" wurde in keine Kategorie eingeteilt. Unsere Aufgabe ist es jetzt, mithilfe des <b>Multinomial Naive Bayes</b> Klassifikationsverfahren die Wahrscheinlichkeiten zu berechnen, ob der Text \"Ein sehr knappes Spiel\" in die Kategorie \"Sport\" oder \"Kein Sport\" eingeteilt wird. Die <u>größere</u> Wahrscheinlichkeit bestimmt, zu welcher Kategorie der Satz gehört.\n",
    "Mathematisch wird das folgenderweise ausgedrückt: <br>\n",
    "\n",
    "$ P(Sport|Ein\\ sehr\\ knappes\\ Spiel) $ <br>\n",
    "$ P(Kein\\ Sport|Ein\\ sehr\\ knappes\\ Spiel) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun ein Problem: Da unsere Daten aus Texten bestehen, können wir nicht mit ihnen rechnen und sie deshalb nicht in die Formel des Bayesschen Theorems einsetzen. Deshalb müssen wir zunächst <b>\"Feature Engineering\"</b> betreiben, einen zentralen Schritt bei der Nutzung von Machine Learning Algorithmen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> Feature Engineering\n",
    "\n",
    "TO-DO: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir müssen uns dazu entschieden, welche Features wir aus den Texten extrahieren wollen. Würde unsere Datensatz aus Personen bestehen und wir würden eine Klassifikation mit den Klassen \"Gesund\" und \"Nicht Gesund\" durchführen wollen, könnten wir als Features die Körpergröße, das Gewicht oder das Geschlecht nehmen und solche Features wie Vornamen, Lieblingsfarbe oder Sprachkenntnisse könnten wir weglassen.<br>\n",
    "In unserem Fall haben wir jedoch keine numerischen Features, sondern nur Text. Um den Text in die Formel des <b>Bayesschen Theorems</b> einsetzen zu können, müssen wir den Text in Zahlen transformieren. Eine Möglichkeit bestünde darin, die einfachen Worthäufigkeiten jedes Textes zu nehmen. Dieses Verfahren nennt sich <b>Bag-of-Words</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> Bag-of-Words Modell\n",
    "\n",
    "Ein <b>Bag-of-Words</b> Modell (kurz: BoW) ist eine Methode, um Features aus einem Text zu extrahieren, damit man diese für eine Modellierung, wie hier für einen Machine Learning-Algorithmus, verwenden kann. Die Idee hierbei ist, dass Dokumente, die eine ähnliche Anzahl der gleichen Wörter haben, ähnlichen Inhalt haben.\n",
    "\n",
    "Der Bag-of-Words-Ansatz ist simpel und flexibel und kann vielseitig verwendet werden. Er beschreibt das Auftreten von Wörtern in einem Dokument und besteht aus einem Vokabular der bekannten Wörter und einem Maß für das Vorkommen der bekannten Wörter. Die Bezeichnung \"bag\" (deutsch: <i>Sack</i>) soll darauf hinweisen, dass alle Informationen über die Struktur oder Reihenfolge der Wörter im Dokument verworfen werden. Es ist also nicht mehr möglich, die Lage eines Wortes nach der Umwandlung eines Textes in einen bag-of-words zu bestimmen.\n",
    "\n",
    "Das Bag-of-Words-Modell kann erweitert werden, etwa durch <b>tf-idf</b> oder <b>N-Gramme</b> (n-grams).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir aber unseren Text in ein <b>Bag-of-words</b>-Modell überführen, setzen wir zunächst den Satz \"Ein sehr knappes Spiel\" in die Formel für das <b>Bayessche Theorem</b> ein. Wir erhalten somit:\n",
    "\n",
    "$ P(Sport|Ein\\ sehr\\ knappes\\ Spiel) = \\frac{P(Ein\\ sehr\\ knappes\\ Spiel|Sport) \\ \\cdot \\ P(Sport)}{P(Ein\\ sehr\\ knappes\\ Spiel)}  $ <br>\n",
    "\n",
    "\n",
    "$ P(Kein\\ Sport|Ein\\ sehr\\ knappes\\ Spiel) = \\frac{P(Ein\\ sehr\\ knappes\\ Spiel|Kein\\ Sport) \\ \\cdot \\ P(Kein\\ Sport)}{P(Ein\\ sehr\\ knappes\\ Spiel)}  $ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir bei unserem Vergleich nur herausfinden wollen, welche Kategorie die höhere Wahrscheinlichkeit hat, können wir zur Vereinfachung den Divisor weglassen und erhalten somit die folgenden gekürzten Formeln:\n",
    "\n",
    "$ P(Sport|Ein\\ sehr\\ knappes\\ Spiel) = P(Ein\\ sehr\\ knappes\\ Spiel|Sport) \\ \\cdot \\ P(Sport)  $ <br>\n",
    "\n",
    "\n",
    "$ P(Kein\\ Sport|Ein\\ sehr\\ knappes\\ Spiel) = P(Ein\\ sehr\\ knappes\\ Spiel|Kein\\ Sport) \\ \\cdot \\ P(Kein\\ Sport)  $ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt müssten wir eigentlich nur noch zählen, wie oft der Satz \"Ein sehr knappes Spiel\" unter der Kategorie \"Sport\" oder \"Kein Sport\" auftritt und das durch die Gesamtanzahl aller Texte teilen. Somit würden wir $ P(Ein\\ sehr\\ knappes\\ Spiel|Sport) $ oder $ P(Ein\\ sehr\\ knappes\\ Spiel|Kein\\ Sport) $ berechnen können. Das Problem ist jedoch, dass dieser Satz in den anderen Texten gar nicht vorkommt und die Wahrscheinlichkeit für beide $ P = 0 $ wäre. Unser Modell wäre nur sinnvoll, wenn wir sehr große Texte hätten und die Texte den exakten Satz beinhalten würden.<br>\n",
    "\n",
    "Wir müssen uns deshalb den <i>naiven</i> Teil vom <b>Naive Bayes</b> Verfahren zu Nutze machen. Anstatt dass wir den gesamten Satz betrachten, betrachten wir die einzelnen Wörter <u>unabhängig</u> voneinander. Sätze wie \"ich gehe nicht nach hause\" und \"nicht nach hause gehe ich\" werden nicht unterschieden, für unser Modell sind es die gleichen Sätze, da sie die gleichen Wörter beinhalten.<br>\n",
    "\n",
    "Mithilfe dieser Annahme können wir nun die Wörter in $ P(Ein\\ sehr\\ knappes\\ Spiel) $ einsetzen und erhalten:<br>\n",
    "\n",
    "$ P(Ein\\ sehr\\ knappes\\ Spiel) = P(Ein) \\cdot P(sehr) \\cdot P(knappes) \\cdot P(Spiel) $ <br>\n",
    "\n",
    "Das setzen wir nun in $ P(Ein\\ sehr\\ knappes\\ Spiel|Sport) $ ein und erhalten somit:<br>\n",
    "\n",
    "$ P(Ein\\ sehr\\ knappes\\ Spiel|Sport) = P(Ein|Sport) \\cdot P(sehr|Sport) \\cdot P(knappes|Sport) \\cdot P(Spiel|Sport) $ <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir endlich unsere Wahrscheinlichkeiten berechnenen, indem wir die Wörter in unseren Daten zusammenzählen. Dafür berechnen wir zunächst für jede der beiden Kategorien ihre <i>A-priori</i>-Wahrscheinlichkeit: Die Wahrscheinlickeit, dass ein Text in unserem Datensatz eine der beiden Kategorien angehört,[<sup>3</sup>](#fn3) ist für $ P(Sport) $ somit $ \\frac{3}{5} $ und für $ P(Kein\\ Sport) $ ist sie $ \\frac{2}{5}$.\n",
    "\n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<span id=\"fn3\" style=\"font-size:8pt; line-height:1\"><sup style=\"font-size:5pt\">3</sup> &nbsp; Ausgenommen wird hier natürlich der Satz \"Ein sehr knappes Spiel\", der (noch) keiner Kategorie zugewiesen ist und deshalb nicht berücksichtigt wird.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> <i>A-priori</i>-Wahrscheinlichkeit<br>\n",
    "\n",
    "Die A-priori-Wahrscheinlichkeit ist ein <b>Wahrscheinlichkeitswert</b>, der aufgrund von Vorwissen gewonnen werden kann. Möchte man beispielsweise die Wahrscheinlichkeit eines Würfelwurfs berechnen, kann man schon vor dem Wurf sagen, dass die Wahrscheinlichkeit des Auftretens einer bestimmten Zahl $ \\frac{1}{6} $ ist, da man vorher wusste, dass der Würfel sechs Seiten hat. Das gleiche gilt für einen Münzwurf, da wir dort wissen, dass eine Münze nur zwei Seiten hat und deshalb die Wahrscheinlichkeit $ \\frac{1}{2} $ ist (natürlich nur wenn man davon ausgeht, dass eine Münze bei dieser Annahme nicht auf der Kante landen darf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Kategorie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ein großartiges Spiel</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Das Urteil ist beschlossen</td>\n",
       "      <td>Kein Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ein sehr faires Match</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ein faires aber langweiliges Spiel</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Es ist ein knappes Urteil</td>\n",
       "      <td>Kein Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ein sehr knappes Spiel</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Text   Kategorie\n",
       "1               Ein großartiges Spiel       Sport\n",
       "2          Das Urteil ist beschlossen  Kein Sport\n",
       "3               Ein sehr faires Match       Sport\n",
       "4  Ein faires aber langweiliges Spiel       Sport\n",
       "5           Es ist ein knappes Urteil  Kein Sport\n",
       "6              Ein sehr knappes Spiel           ?"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun berechnen wir nacheinander die Wahrscheinlichkeit $ P $ der einzelnen Wörter, angefangen mit $ P(Ein|Sport) $. Dafür zählen wir zunächst, wie oft das Wort \"Ein\" in Texten der Kategorie \"Sport\" vorkommt (3). Das teilen wir durch die Gesamtanzahl der Wörter aus den Texten der Kategorie \"Sport\" (12). Wir erhalten also $ P(Ein|Sport) = \\frac{3}{12}$.<br>\n",
    "Diese Berechnung führen wir nun für jedes der Wörter des Satzes \"Ein sehr knappes Spiel\" durch und erhalten folgende Ergebnisse:<br>\n",
    "\n",
    "$ P(sehr|Sport) = \\frac{1}{12}$<br>\n",
    "$ P(knappes|Sport) = \\frac{0}{12} = 0 $<br>\n",
    "$ P(Spiel|Sport) = \\frac{2}{12}$<br>\n",
    "\n",
    "Wir haben nun jedoch ein Problem: Das Wort \"knappes\" kommt in keinem der Texte aus der Kategorie \"Sport\" vor und die Wahrscheinlichkeit ist somit 0. Würden nun aber die Berechnung $ P(Ein|Sport) \\cdot P(sehr|Sport) \\cdot P(knappes|Sport) \\cdot P(Spiel|Sport) $ ausführen, würde der gesamte Ausdruck 0 werden, da $ P(Ein|Sport) \\cdot P(sehr|Sport) \\cdot 0 \\cdot P(Spiel|Sport) $.<br>\n",
    "Wir müssen dieses Problem umgehen, da wir ansonsten nur Sätze klassifizieren könnten, bei denen alle Wörter in jeder Kategorie vorkommen müssten. Wir wenden das sogenannte <b>Laplace smoothing</b> an, was bedeutet, dass wir zu jedem gezählten Wort 1 addieren, damit es nie 0 werden kann. Damit wäre es jedoch möglich, dass wir eine Wahrscheinlichkeit > 1 erhalten könnten, wenn alle Texte einer Kategorie nur aus einem Wort bestehen. Würden alle Texte der Kategorie \"Sport\" also aus dem einzelnen Wort \"Spiel\" bestehen, wäre $ P(Spiel|Sport) = \\frac{3+1}{3} = \\frac{4}{3} > 1$. Dieser Fall ist sehr unwahrscheinlich, trotzdem addieren wir im Divisor die Anzahl aller möglicher Wörter, nicht nur die der Kategorie \"Sport\", sondern auch der Kategorie \"Kein Sport\". Dieses Vokabular besteht in unserem Fall aus 14 Wörtern.<br>\n",
    "$ P(Ein|Sport) $ mit Laplace Smoothing wäre also $ P(Ein|Sport)  = \\frac{3+1}{12+14} = \\frac{4}{26} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vokabular: {'sehr', 'faires', 'match', 'knappes', 'langweiliges', 'urteil', 'aber', 'spiel', 'beschlossen', 'ist', 'großartiges', 'es', 'das', 'ein'}\n",
      "\n",
      "Größe des Vokabulars: 14\n"
     ]
    }
   ],
   "source": [
    "smaller_simple_df = simple_df[:-1]\n",
    "\n",
    "results = set()\n",
    "smaller_simple_df['Text'].str.lower().str.split().apply(results.update)\n",
    "print(\"Vokabular: \" + str(results) +\"\\n\")\n",
    "print(\"Größe des Vokabulars: \" + str(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun berechnen wir für jedes Wort des Satzes \"Ein sehr knappes Spiel\" die Wahrscheinlichkeit für beide Kategorien.\n",
    "\n",
    "<table style=\"width:75%\" align=\"left\">\n",
    "  <tr>\n",
    "    <th>Wort</th>\n",
    "    <th>$ P(wort|Sport) $</th>\n",
    "    <th>$ P(wort|Kein\\ Sport) $</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ein</td>\n",
    "    <td>$ \\frac{3+1}{12+14} = \\frac{4}{26}$</td>\n",
    "    <td>$ \\frac{1+1}{9+14} = \\frac{2}{23}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sehr</td>\n",
    "    <td>$ \\frac{1+1}{12+14} = \\frac{2}{26}$</td>\n",
    "    <td>$ \\frac{0+1}{9+14} = \\frac{1}{23}$</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>knappes</td>\n",
    "    <td>$ \\frac{0+1}{12+14} = \\frac{1}{26}$</td>\n",
    "    <td>$ \\frac{1+1}{9+14} = \\frac{2}{23}$</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Spiel</td>\n",
    "    <td>$ \\frac{2+1}{12+14} = \\frac{3}{26} $</td>\n",
    "    <td>$ \\frac{0+1}{9+14} = \\frac{1}{23}$</td>\n",
    "  </tr>\n",
    "</table><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt multiplizieren wir die Wahrscheinlichkeiten für jede Kategorie:<br>\n",
    "\n",
    "$  P(Ein|Sport) \\cdot P(sehr|Sport) \\cdot P(knappes|Sport) \\cdot P(Spiel|Sport) \\cdot P(Sport) \n",
    "\\\\ = \\frac{4}{26} \\cdot \\frac{2}{26} \\cdot \\frac{1}{26} \\cdot \\frac{3}{26} \\cdot \\frac{3}{5} \n",
    "\\\\ = 0,000031512$ <br>\n",
    "\n",
    "$  P(Ein|Kein\\ Sport) \\cdot P(sehr|Kein\\ Sport) \\cdot P(knappes|Kein\\ Sport) \\cdot P(Spiel|Kein\\ Sport) \\cdot P(Kein\\ Sport) \n",
    "\\\\ = \\frac{2}{23} \\cdot \\frac{1}{23} \\cdot \\frac{2}{23} \\cdot \\frac{1}{23} \\cdot \\frac{2}{5} \n",
    "\\\\ = 0,000005718$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da 0,000031512 > 0,000005718, ist die Wahrscheinlichkeit, dass der Satz \"Ein sehr knappes Spiel\" der Kategorie \"Sport\" angehört, höher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liegt an unseren Daten (die den Daten unseres Korpus(?) ähneln). Unsere Daten sind nämlich <b>diskrete</b> Daten. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Diskrete vs continous... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words hat einige Probleme:\n",
    "- goodnote\n",
    "- https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probleme/Erläuterungen für Bericht:\n",
    "- wie weit Feature Engineering beschreiben, wenn Fokus später auf DL liegt?\n",
    "- Wieviele Möglichkeiten von BoW beschreiben?\n",
    "- Beispiel von Monkey Learning war gut, musste aber übersetzt werden. hier ist die deutsche sprache wieder problematisch, wegen den konjugation etc. einige wörter wurden ausgetauscht\n",
    "- die idee bei diesem teil war es, dass sehr einfach ein paar grundzüge und herangehensweisen von klassifikation erklärt werden können\n",
    "    - wahl des verfahrens: wurde hier abgenommen, wird später dann genauer erläutert\n",
    "    - inhalt: was ist das verfahren überhaupt? eine auseinandersetzung mit verfahren ist vorteilhaft, es wurde aber wert darauf gelegt, dass es nicht zu tief in die materie geht, vor allem was den mathematischen aspekt angeht. deshalb wurde die mathematik einfach gehalten und ausreichend erklärt.\n",
    "    - mathematik: jedes verfahren basiert natürlich auf einer mathematik. geisteswissenschaftler, also auch dhler, scheuen sich aber etwas davor, da der umgang mit formeln nicht geübt ist wie bei informatikern, mathematiker oder allgemein naturwissenschaftlern. hier ist es also wichtig, die mathematik gut und einfach zu erklären und vor allem mathematik, die nicht gebraucht wird, wegzulassen. zu viel sollte m.E. jedoch nicht weggelassen werden, da ein basis verständnis hilft, die verfahren zu verstehen. da in späteren kapiteln der anteil der mathematik zunehmen würde, wurden hier basis rechnung ausführlich erklärt, damit der leser ein verständnis dafür bekommt, ohne abgeschreckt zu werden.\n",
    "    - info boxen: für diese wurde sich bewusst entschieden, da sie nicht direkt in den text gepasst haben, aber wichtig sind. feature engineering und bow wurden mit einem satz im \"fließtext\" erklärt und in der info box etwas ausführlicher beleuchtet.\n",
    "    - einfaches bespiel ohne programmierung: ich finde, dass rechenbeispiele sehr gut helfen, ein verfahren zu verstehen. hätte ich hier direkt mit einem sci kit learn beispiel angefangen, wäre ein tieferes verständnis von naive bayes nicht gegeben. der leser könnte zwar daten klassifizieren, was sci kit learn aber eigentlich im hintergrund macht, bleibt ihm verwehrt. deswegen wurde hier sehr sehr ausfürlich das verfahren per Hand gerechnet. der leser versteht so, was eigentlich gemacht werden und kann sich vorstellen, wieso ein programmiermehtode sinnvoll ist, vor allem bei vielen Daten. dieses einfache beispiel ist sinnbildlich für die funktionsweise von textklassifikationsverfahren. es war jedoch klar/ es wurde sich bewusst dazu entschieden, nicht für jedes weitere verfahren ein händisches rechenbeispiel zu liefern, da dies zu viel des guten wäre und sich zu vieles wiederholen würde. in nächsten verfahren, logistic regression, soll dies noch einmal wiederholt, damit sollte eine gute intuition gegeben sein, was der computer da eigentlich rechnet.\n",
    "    - sci kit learn:\n",
    "    - datensatz mit naive bayes:\n",
    "    - erweiterung mit tf idf und n-gramme: ICH: vllt erst nächstes kapitel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quellen:\n",
    "- https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "- https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
