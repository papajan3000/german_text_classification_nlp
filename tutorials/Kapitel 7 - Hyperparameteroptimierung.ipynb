{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 7 - Hyperparameteroptimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Kapitelübersicht <a class=\"anchor\" id=\"7-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "https://de.wikipedia.org/wiki/Hyperparameteroptimierung\n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[7.1. Kapitelübersicht](#7-1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO<br>\n",
    "Bis jetzt hatten wir nur die standard parameter von scikit learn bei unseren klassifizierungsverfahren genutzt. selbst hatten wir keine parameter übergeben. durch gezieltes parameter tuning können klassifizierungsverfahren jedoch noch einmal verbessert werden. in diesem abschnitt werden wir uns zunächst erneut die verfahren naive bayes und log reg angucken und deren parameter in der dokumentation. intuitionen hatten wir schon in den einzelnen Kapiteln besprochen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Hyperparameter von Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren wir zunächst ein weiteres Mal das <b>Multinomial Naive Bayes</b> Klassifizierungsverfahren. Hier haben wir es in einer Funktion zusammengefasst, um den Code kleiner zu halten (AF?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit Multinomial Naive Bayes ist 0.859.\n",
      "\n",
      "Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist 0.87.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
    "vector  = TfidfVectorizer().fit_transform(corpus[\"text\"])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "\n",
    "def classify_mnb(alpha=1.0):\n",
    "    \n",
    "    # Multinomial Naive Bayes\n",
    "    mnb_classifier = MultinomialNB(alpha)\n",
    "    mnb = mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # cross validation des Trainingsdatensatzes\n",
    "    mnb_scores = cross_val_score(mnb_classifier, vector, labels, cv=3)\n",
    "    mnb_mean = np.mean(mnb_scores)\n",
    "\n",
    "    print(\"Der Mittelwert der cross validation bei der  Klassifizierung \" \n",
    "          + f\" mit Multinomial Naive Bayes ist {str(np.around(mnb_mean, decimals=3))}.\"\n",
    "          + \"\\n\")\n",
    "\n",
    "\n",
    "    # F1-score des Testdatensatzes\n",
    "    y_pred = mnb_classifier.predict(X_test)\n",
    "    mnb_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "    print(\"Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist \"\n",
    "          + f\"{str(np.around(mnb_f1, decimals=3))}.\")\n",
    "classify_mnb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns zunächst die möglichen Parameter von Multinomial Naive Bayes in der <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">Dokumentation</a> an. Es gibt drei Parameter:\n",
    "- `alpha` (default = 1.0)\n",
    "- `fit_prior` (default = True)\n",
    "- `class_prior` (default = None)<br>\n",
    "\n",
    "Wir werden uns nur `alpha` anschauen. Ist `alpha = 1`, wir <b>Laplace Smoothing</b> angewandt (siehe Kapitel 3), d.h. jede Worthäufigkeit wird um 1 erhöht. Ist `alpha < 1`, wird <b>Lidstone Smoothing</b> angewandt, das im Grunde das Gleiche ist. Ist `alpha = 0`, wird gar kein Smoothing angewandt. Standardmäßig ist `alpha = 1` ausgewählt, weshalb wir hier zunächst `alpha = 0.5` setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit Multinomial Naive Bayes ist 0.876.\n",
      "\n",
      "Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist 0.889.\n"
     ]
    }
   ],
   "source": [
    "classify_mnb(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sowohl der Mittelwert der cross validation als auch der F1-score haben sich verbessert. Versuchen wir es nun mit `alpha = 0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit Multinomial Naive Bayes ist 0.898.\n",
      "\n",
      "Der F1-score für die Klassifizierung mit Multinomial Naive Bayes ist 0.906.\n"
     ]
    }
   ],
   "source": [
    "classify_mnb(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erneut können wir eine Verbesserung feststellen. Um weitere Parameter zu testen, können wir nun zwei Strategien verfolgen: Einfach weiter Parameter ausprobieren oder ein Hyperparameteroptimierungsverfahren benutzen, um die optimalen Hyperparameter zu finden (AF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Grid Search</b> (deutsch: Rastersuche) ist ein Suchverfahren, dass mithilfe der <b>Brute-Force-Methode</b> (auch <i>erschöpfende Suche</i> genannt) die optimalen Hyperparameter sucht. Dabei muss ein selbst erstelltes Dictionary von möglichen Hyperparametern als keys übergeben werden (hier `parameters`), deren values mögliche Parameterwerte sind. Grid Search nutzt als Evaluationsmethode die <b>cross validation</b>, weshalb man die Anzahl der Teildatensätze (<i>folds</i>) beim Parameter `cv` angeben muss. Zudem müssen wir, um den <b>F1-score nutzen</b> zu können, diesen beim Parameter `scoring` angeben. Grid Search liefert gute Werte, leidet aber unter dem <b>Fluch der Dimensionalität</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> Fluch der Dimensionalität\n",
    "    \n",
    "Der <b>Fluch der Dimensionalität</b> (englisch: curse of dimensionality) ist ein Begriff, der auf die Schwierigkeiten beim Anpassen von Modellen, bei der Schätzung von Parametern oder bei der Optimierung einer Funktion in vielen Dimensionen verweisen soll. Je mehr Dimensionen ein Eingabedatenraum hat, umso schwieriger wird es, optimale Parameter für diesen Raum zu finden. Bei den Klassifizierungsverfahren bedeutet dies konkret: Je mehr Hyperparameter ein Klassifizierungsverfahren hat, umso schwieriger wird es, diese zu optimieren, um bspw. einen idealen F1-score zu erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der beste Hyperparameter für alpha ist 0.01.\n",
      "Der beste Score ist 0.9302.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\"alpha\": np.array([0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, \n",
    "                0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(mnb_classifier, parameters, cv=5, scoring=\"f1_micro\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Ergebnisse\n",
    "print(f\"Der beste Hyperparameter für alpha ist {str(grid.best_estimator_.alpha)}.\")\n",
    "print(f\"Der beste Score ist {str(np.around(grid.best_score_, decimals=4))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Von unseren Hyperparametern scheint `0.01` der beste Hyperparameter zu sein. Wir können nun die umliegenden Werte zu `0.01` als neue Parameter auswählen und GridSearch übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der beste Hyperparameter für alpha ist 0.006.\n",
      "Der beste Score ist 0.9317.\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"alpha\": np.array([0.001, 0.002, 0.003, 0.004, 0.005, 0.006,\n",
    "                                 0.007, 0.008, 0.009, 0.01, 0.011, 0.012,\n",
    "                                0.013, 0.014, 0.015, 0.016, 0.017, 0.018, \n",
    "                                0.019, 0.2])}\n",
    "\n",
    "grid = GridSearchCV(mnb_classifier, parameters, cv=5, scoring=\"f1_micro\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Ergebnisse\n",
    "print(f\"Der beste Hyperparameter für alpha ist {str(grid.best_estimator_.alpha)}.\")\n",
    "print(f\"Der beste Score ist {str(np.around(grid.best_score_, decimals=4))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein noch besserer Wert als `0.01` scheint `0.006` zu sein, der Score hat sich von `0.9302` auf `0.9317` erhöht. Es stellt sich nun die Frage, ob sich der Aufwand für eine so geringe Verbesserung lohnt. Eine eindeutige Antwort gibt es darauf nicht, eine Verbesserung anzustreben, die kleiner als `0.01` ist, ist für unsere Zwecke nicht sonderlich sinnvoll. (ICH: hier was sagen zu überoptimierung zitat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns zunächst die möglichen Parameter von Multinomial Naive Bayes in der <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">Dokumentation</a> an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.array([0.000001, 0.00001, 0.0001, 0.001, 0.01, \n",
    "                0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "parameters = {'alpha': alphas, \n",
    "              'fit_prior' : [True, False], \n",
    "              'class_prior ' : [None, [.1,.9],[.2, .8]]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=mnb_classifier, param_grid=dict(alpha=alphas), cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.? Mögliche Fehler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>GridSearch</b>:\n",
    "- F1-score soll genutzt werden, `scoring` Parameter wurde nicht definiert → `scoring`-Parameter angeben mit Wert \"f1\", \"f1_micro\", \"f1_macro\" etc.\n",
    "- Fehlercode: `Target is multiclass but average='binary'. Please choose another average setting.` → Nur \"f1\" wurde beim Parameter `scoring` angegeben, obwohl die es mehr als zwei Klassen bei den Daten gibt. Standardmäßig geht \"f1\" von einer binären Klassifikation aus, deshalb muss z.B. \"f1_micro\" oder \"f1_macro\" angegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
