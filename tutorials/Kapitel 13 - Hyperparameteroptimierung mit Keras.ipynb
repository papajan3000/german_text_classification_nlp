{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 13 - Hyperparameteroptimierung mit Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if gpu is available\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1. Kapitelübersicht <a class=\"anchor\" id=\"13-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Kapitel schauen wir uns die Hyperparameteroptimierung bei Keras an. Dafür verwenden wir den Wrapper <a href=\"https://github.com/maxpumperla/hyperas\">hyperas</a>.\n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[13.1. Kapitelübersicht](#13-1)<br>\n",
    "\n",
    "\n",
    "Am Ende dieses Kapitel werden wir folgende Themen behandelt und/oder vertieft haben:\n",
    "- Alternativer Aufbau von Neuronalen Netzen in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2. Hyperparameteroptimierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hyperas.distributions import uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Hyperparameteroptimierung mit hyperas orientieren wir uns am Beispiel der <a href=\"https://github.com/maxpumperla/hyperas\">Github-Seite</a>, indem wir das Laden des Korpus, das Encoding der Kategorien, die Vektorisierung der Artikel und die Aufteilung in Training-, Validierungs- und Testdatensätze in einer Funktion `data` speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector = vectorizer.fit_transform(corpus[\"text\"])\n",
    "    labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
    "    vocab = vectorizer.vocabulary_\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                            labels, \n",
    "                                                            test_size=0.4, \n",
    "                                                            train_size=0.6,\n",
    "                                                            random_state=42)\n",
    "    X_val = X_test[:1200]\n",
    "    X_test = X_test[1200:]\n",
    "\n",
    "    y_val = y_test[:1200]\n",
    "    y_test = y_test[1200:]\n",
    "\n",
    "    y_val = to_categorical(y_val)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_train = to_categorical(y_train)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun definieren wir eine Funktion `create_model`, in der unser Neuronales Netz erstellt wird. Der Aufbau dieses Netzes unterscheidet sich vom Aufbau, den wir im letzten Kapitel gesehen hatten. Die Aktivierungsfunktion definieren wir hier in einer eigenen Zeile, um den Code übersichtlicher zu gestalten. \n",
    "\n",
    "Die Werte der verschiedenen Parameter, die wir optimieren wollen, werden hier in einer Liste gespeichert, die `{{choice()}}` übergeben wird. Hier wählen wir zunächst nur die Dense Layer und ihre Units-Größe sowie die Dropout-Layer und ihre Dropout-Quoten. Für den ersten Test verwenden wir eine Epochenzahl von 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, y_train, X_val, y_val, X_test, y_test, vocab, labels):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(Dense({{choice([8, 16])}}, input_shape=(len(vocab),)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{choice([0.2, 0.3])}}))\n",
    "    model.add(Dense({{choice([8, 16])}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{choice([0.2, 0.3])}})) \n",
    "    model.add(Dense(len(np.unique(labels))))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=2)\n",
    "              \n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    validation_acc = np.mean(history.history['val_acc']) \n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun rufen wir die eigentliche Optimierungsfunktion ein. Dieser übergeben wir unsere Funktion `create_model`, unser Funktion `data`, die Optimierunfsfunktion <b>Tree Parzen Estimator</b> (kurz: TPE), die maximale Anzahl der der Optimierungsdurchgänge mit `max_eval`, die Funktion `Trials` und der Name des Notebooks. Letzteres ist sehr wichtig, wenn man mit einem Jupyter Notebook arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import models\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, RMSprop, SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import regularizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import f1_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [8, 16]),\n",
      "        'Dropout': hp.choice('Dropout', [0.2, 0.3]),\n",
      "        'Dense_1': hp.choice('Dense_1', [8, 16]),\n",
      "        'Dropout_1': hp.choice('Dropout_1', [0.2, 0.3]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\")\n",
      "  4: \n",
      "  5: vectorizer = TfidfVectorizer()\n",
      "  6: vector = vectorizer.fit_transform(corpus[\"text\"])\n",
      "  7: labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
      "  8: vocab = vectorizer.vocabulary_\n",
      "  9: \n",
      " 10: X_train, X_test, y_train, y_test = train_test_split(vector, \n",
      " 11:                                                         labels, \n",
      " 12:                                                         test_size=0.4, \n",
      " 13:                                                         train_size=0.6,\n",
      " 14:                                                         random_state=42)\n",
      " 15: X_val = X_test[:1200]\n",
      " 16: X_test = X_test[1200:]\n",
      " 17: \n",
      " 18: y_val = y_test[:1200]\n",
      " 19: y_test = y_test[1200:]\n",
      " 20: \n",
      " 21: y_val = to_categorical(y_val)\n",
      " 22: y_test = to_categorical(y_test)\n",
      " 23: y_train = to_categorical(y_train)\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      " 27: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     \n",
      "  4:     model = models.Sequential()\n",
      "  5:     model.add(Dense(space['Dense'], input_shape=(len(vocab),)))\n",
      "  6:     model.add(Activation('relu'))\n",
      "  7:     model.add(Dropout(space['Dropout']))\n",
      "  8:     model.add(Dense(space['Dense_1']))\n",
      "  9:     model.add(Activation('relu'))\n",
      " 10:     model.add(Dropout(space['Dropout_1'])) \n",
      " 11:     model.add(Dense(len(np.unique(labels))))\n",
      " 12:     model.add(Activation('softmax'))\n",
      " 13:     \n",
      " 14: \n",
      " 15:     model.compile(optimizer='rmsprop',\n",
      " 16:                   loss=\"categorical_crossentropy\",\n",
      " 17:                   metrics=[\"accuracy\"])\n",
      " 18:     \n",
      " 19:     history = model.fit(X_train,\n",
      " 20:                         y_train,\n",
      " 21:                         epochs=10,\n",
      " 22:                         batch_size=16,\n",
      " 23:                         validation_data=(X_val, y_val),\n",
      " 24:                         verbose=2)\n",
      " 25:               \n",
      " 26:     score = model.evaluate(X_test, y_test, verbose=0)\n",
      " 27:     validation_acc = np.mean(history.history['val_acc']) \n",
      " 28:     return {'loss': -validation_acc, 'status': STATUS_OK}\n",
      " 29: \n",
      "Train on 3600 samples, validate on 1200 samples    \n",
      "Epoch 1/10                                         \n",
      "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[8,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/RMSprop/Variable_4/Assign = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_2/RMSprop/Variable_4, training_2/RMSprop/zeros_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_2/RMSprop/Variable_4/Assign', defined at:\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jan/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-e3032c2680bc>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', \"best_run, best_model = optim.minimize(model=create_model,\\n                                      data=data,\\n                                      algo=tpe.suggest,\\n                                      max_evals=5,\\n                                      trials=Trials(),\\n                                      notebook_name='Kapitel 13 - Hyperparameteroptimierung mit Keras')\\n\")\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2347, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"</home/jan/.local/lib/python3.6/site-packages/decorator.py:decorator-gen-61>\", line 2, in time\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1292, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 6, in <module>\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperas/optim.py\", line 69, in minimize\n    keep_temp=keep_temp)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperas/optim.py\", line 139, in base_minimizer\n    return_argmin=True),\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 388, in fmin\n    show_progressbar=show_progressbar,\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/base.py\", line 639, in fmin\n    show_progressbar=show_progressbar)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 407, in fmin\n    rval.exhaust()\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 262, in exhaust\n    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 227, in run\n    self.serial_evaluate()\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 141, in serial_evaluate\n    result = self.domain.evaluate(spec, ctrl)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/base.py\", line 844, in evaluate\n    rval = self.fn(pyll_rval)\n  File \"/home/jan/Desktop/informatik_programme/german_text_classification_nlp/tutorials/temp_model.py\", line 152, in keras_fmin_fnct\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 1148, in fit\n    self._make_train_function()\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 512, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/optimizers.py\", line 254, in get_updates\n    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/optimizers.py\", line 254, in <listcomp>\n    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 735, in zeros\n    return variable(v, dtype=dtype, name=name)\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 422, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 387, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 283, in assign\n    validate_shape=validate_shape)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 60, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[8,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/RMSprop/Variable_4/Assign = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_2/RMSprop/Variable_4, training_2/RMSprop/zeros_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[8,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/RMSprop/Variable_4/Assign = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_2/RMSprop/Variable_4, training_2/RMSprop/zeros_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/informatik_programme/german_text_classification_nlp/tutorials/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[8,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/RMSprop/Variable_4/Assign = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_2/RMSprop/Variable_4, training_2/RMSprop/zeros_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_2/RMSprop/Variable_4/Assign', defined at:\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jan/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/home/jan/anaconda3/envs/tf36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/jan/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jan/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-e3032c2680bc>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', \"best_run, best_model = optim.minimize(model=create_model,\\n                                      data=data,\\n                                      algo=tpe.suggest,\\n                                      max_evals=5,\\n                                      trials=Trials(),\\n                                      notebook_name='Kapitel 13 - Hyperparameteroptimierung mit Keras')\\n\")\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2347, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"</home/jan/.local/lib/python3.6/site-packages/decorator.py:decorator-gen-61>\", line 2, in time\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/jan/.local/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1292, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 6, in <module>\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperas/optim.py\", line 69, in minimize\n    keep_temp=keep_temp)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperas/optim.py\", line 139, in base_minimizer\n    return_argmin=True),\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 388, in fmin\n    show_progressbar=show_progressbar,\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/base.py\", line 639, in fmin\n    show_progressbar=show_progressbar)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 407, in fmin\n    rval.exhaust()\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 262, in exhaust\n    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 227, in run\n    self.serial_evaluate()\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/fmin.py\", line 141, in serial_evaluate\n    result = self.domain.evaluate(spec, ctrl)\n  File \"/home/jan/.local/lib/python3.6/site-packages/hyperopt/base.py\", line 844, in evaluate\n    rval = self.fn(pyll_rval)\n  File \"/home/jan/Desktop/informatik_programme/german_text_classification_nlp/tutorials/temp_model.py\", line 152, in keras_fmin_fnct\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 1148, in fit\n    self._make_train_function()\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 512, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/optimizers.py\", line 254, in get_updates\n    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/optimizers.py\", line 254, in <listcomp>\n    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 735, in zeros\n    return variable(v, dtype=dtype, name=name)\n  File \"/home/jan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 422, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 387, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 283, in assign\n    validate_shape=validate_shape)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 60, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/jan/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[8,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/RMSprop/Variable_4/Assign = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_2/RMSprop/Variable_4, training_2/RMSprop/zeros_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Kapitel 13 - Hyperparameteroptimierung mit Keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bdf4f250225d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best performing model chosen hyper-parameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\" )\n",
    "print(best_model.evaluate(X_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OOM https://github.com/maxpumperla/hyperas/issues/16\n",
    "- max eval: https://stackoverflow.com/questions/56745518/what-does-max-eval-parameter-in-hyperas-optim-minimize-function-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_model(X_train, y_train, X_val, y_val, X_test, y_test, vocab, labels):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(Dense({{choice([16, 32, 64])}}, input_shape=(len(vocab),)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32, 64])}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}})) #{{choice([0.2, 0.3])}}\n",
    "              \n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense({{choice([16, 32, 64])}}))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        model.add(Activation('relu'))          \n",
    "              \n",
    "    model.add(Dense(len(np.unique(labels))))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    rmsprop = RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    sgd = SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "   \n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    \n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    else:\n",
    "        optim = sgd\n",
    "    \n",
    "\n",
    "    model.compile(optimizer=optim,\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=2)\n",
    "              \n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracy = score[1]\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICH: persönliche Empfehlung, niedrige Batch size (zu fehlern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import models\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, RMSprop, SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import regularizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import f1_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [16, 32, 64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_1': hp.choice('Dense_1', [16, 32, 64]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dropout_2': hp.choice('Dropout_2', [0.2, 0.3]),\n",
      "        'Dropout_3': hp.choice('Dropout_3', ['three', 'four']),\n",
      "        'Dense_2': hp.choice('Dense_2', [16, 32, 64]),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'lr': hp.choice('lr', [10**-3, 10**-2, 10**-1]),\n",
      "        'lr_1': hp.choice('lr_1', [10**-3, 10**-2, 10**-1]),\n",
      "        'lr_2': hp.choice('lr_2', [10**-3, 10**-2, 10**-1]),\n",
      "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\")\n",
      "  4: \n",
      "  5: vectorizer = TfidfVectorizer()\n",
      "  6: vector = vectorizer.fit_transform(corpus[\"text\"])\n",
      "  7: labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
      "  8: vocab = vectorizer.vocabulary_\n",
      "  9: \n",
      " 10: X_train, X_test, y_train, y_test = train_test_split(vector, \n",
      " 11:                                                         labels, \n",
      " 12:                                                         test_size=0.4, \n",
      " 13:                                                         train_size=0.6,\n",
      " 14:                                                         random_state=42)\n",
      " 15: X_val = X_test[:1200]\n",
      " 16: X_test = X_test[1200:]\n",
      " 17: \n",
      " 18: y_val = y_test[:1200]\n",
      " 19: y_test = y_test[1200:]\n",
      " 20: \n",
      " 21: y_val = to_categorical(y_val)\n",
      " 22: y_test = to_categorical(y_test)\n",
      " 23: y_train = to_categorical(y_train)\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      " 27: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     model = models.Sequential()\n",
      "   5:     model.add(Dense(space['Dense'], input_shape=(len(vocab),)))\n",
      "   6:     model.add(Activation('relu'))\n",
      "   7:     model.add(Dropout(space['Dropout']))\n",
      "   8:     model.add(Dense(space['Dense_1']))\n",
      "   9:     model.add(Activation('relu'))\n",
      "  10:     model.add(Dropout(space['Dropout_1'])) #space['Dropout_2']\n",
      "  11:               \n",
      "  12:     if space['Dropout_3'] == 'four':\n",
      "  13:         model.add(Dense(space['Dense_2']))\n",
      "  14: \n",
      "  15:         model.add(Dropout(space['Dropout_4']))\n",
      "  16:         model.add(Activation('relu'))          \n",
      "  17:               \n",
      "  18:     model.add(Dense(len(np.unique(labels))))\n",
      "  19:     model.add(Activation('softmax'))\n",
      "  20:     \n",
      "  21:     adam = Adam(lr=space['lr'])\n",
      "  22:     rmsprop = RMSprop(lr=space['lr_1'])\n",
      "  23:     sgd = SGD(lr=space['lr_2'])\n",
      "  24:    \n",
      "  25:     choiceval = space['choiceval']\n",
      "  26:     \n",
      "  27:     if choiceval == 'adam':\n",
      "  28:         optim = adam\n",
      "  29:     elif choiceval == 'rmsprop':\n",
      "  30:         optim = rmsprop\n",
      "  31:     else:\n",
      "  32:         optim = sgd\n",
      "  33:     \n",
      "  34: \n",
      "  35:     model.compile(optimizer=optim,\n",
      "  36:                   loss=\"categorical_crossentropy\",\n",
      "  37:                   metrics=[\"accuracy\"])\n",
      "  38:     \n",
      "  39:     history = model.fit(X_train,\n",
      "  40:                         y_train,\n",
      "  41:                         epochs=50,\n",
      "  42:                         batch_size=32,\n",
      "  43:                         validation_data=(X_val, y_val),\n",
      "  44:                         verbose=2)\n",
      "  45:               \n",
      "  46:     score = model.evaluate(X_test, y_test, verbose=0)\n",
      "  47:     accuracy = score[1]\n",
      "  48:     return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}\n",
      "  49: \n",
      "Train on 3600 samples, validate on 1200 samples     \n",
      "Epoch 1/50                                          \n",
      "  0%|          | 0/30 [00:00<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[Node: _arg_dense_105_input_0_0/_3277 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_638__arg_dense_105_input_0_0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/informatik_programme/german_text_classification_nlp/tutorials/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: _arg_dense_105_input_0_0/_3277 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_638__arg_dense_105_input_0_0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=30,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='Kapitel 13 - Hyperparameteroptimierung mit Keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-725d6796b7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best performing model chosen hyper-parameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\" )\n",
    "print(best_model.evaluate(X_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOM when allocating tensor with shape[281396,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
    "\t [[Node: training_20/SGD/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](SGD_15/momentum/read, training_20/SGD/Variable/read)]]\n",
    "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\")\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(corpus[\"text\"])\n",
    "labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                        labels, \n",
    "                                                        test_size=0.4, \n",
    "                                                        train_size=0.6,\n",
    "                                                        random_state=42)\n",
    "X_val = X_test[:1200]\n",
    "X_test = X_test[1200:]\n",
    "\n",
    "y_val = y_test[:1200]\n",
    "y_test = y_test[1200:]\n",
    "\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3600 samples, validate on 1200 samples\n",
      "Epoch 1/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 3.3352 - acc: 0.1100 - val_loss: 3.1883 - val_acc: 0.2942\n",
      "Epoch 2/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 2.9586 - acc: 0.2236 - val_loss: 2.6675 - val_acc: 0.4450\n",
      "Epoch 3/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 2.4472 - acc: 0.3258 - val_loss: 2.0908 - val_acc: 0.6250\n",
      "Epoch 4/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 2.0015 - acc: 0.4183 - val_loss: 1.6045 - val_acc: 0.7933\n",
      "Epoch 5/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 1.6164 - acc: 0.5356 - val_loss: 1.2182 - val_acc: 0.8350\n",
      "Epoch 6/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 1.3183 - acc: 0.6144 - val_loss: 0.9609 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 1.1431 - acc: 0.6414 - val_loss: 0.7862 - val_acc: 0.8900\n",
      "Epoch 8/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.9449 - acc: 0.7175 - val_loss: 0.6523 - val_acc: 0.8942\n",
      "Epoch 9/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.8254 - acc: 0.7475 - val_loss: 0.5647 - val_acc: 0.8967\n",
      "Epoch 10/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.7212 - acc: 0.7789 - val_loss: 0.5006 - val_acc: 0.9025\n",
      "Epoch 11/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.6431 - acc: 0.8081 - val_loss: 0.4505 - val_acc: 0.9092\n",
      "Epoch 12/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.5652 - acc: 0.8228 - val_loss: 0.4147 - val_acc: 0.9117\n",
      "Epoch 13/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.5103 - acc: 0.8406 - val_loss: 0.3885 - val_acc: 0.9158\n",
      "Epoch 14/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.4661 - acc: 0.8533 - val_loss: 0.3654 - val_acc: 0.9142\n",
      "Epoch 15/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.4058 - acc: 0.8736 - val_loss: 0.3571 - val_acc: 0.9158\n",
      "Epoch 16/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.4000 - acc: 0.8775 - val_loss: 0.3460 - val_acc: 0.9175\n",
      "Epoch 17/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.3644 - acc: 0.8844 - val_loss: 0.3394 - val_acc: 0.9200\n",
      "Epoch 18/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.3322 - acc: 0.8978 - val_loss: 0.3298 - val_acc: 0.9183\n",
      "Epoch 19/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.3312 - acc: 0.8961 - val_loss: 0.3254 - val_acc: 0.9183\n",
      "Epoch 20/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.3047 - acc: 0.9050 - val_loss: 0.3310 - val_acc: 0.9200\n",
      "Epoch 21/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2888 - acc: 0.9092 - val_loss: 0.3239 - val_acc: 0.9225\n",
      "Epoch 22/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2786 - acc: 0.9153 - val_loss: 0.3340 - val_acc: 0.9175\n",
      "Epoch 23/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2528 - acc: 0.9192 - val_loss: 0.3214 - val_acc: 0.9208\n",
      "Epoch 24/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2421 - acc: 0.9258 - val_loss: 0.3289 - val_acc: 0.9192\n",
      "Epoch 25/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2364 - acc: 0.9286 - val_loss: 0.3296 - val_acc: 0.9192\n",
      "Epoch 26/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2365 - acc: 0.9242 - val_loss: 0.3217 - val_acc: 0.9217\n",
      "Epoch 27/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2085 - acc: 0.9364 - val_loss: 0.3182 - val_acc: 0.9217\n",
      "Epoch 28/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2081 - acc: 0.9392 - val_loss: 0.3196 - val_acc: 0.9258\n",
      "Epoch 29/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2025 - acc: 0.9336 - val_loss: 0.3135 - val_acc: 0.9217\n",
      "Epoch 30/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1857 - acc: 0.9386 - val_loss: 0.3201 - val_acc: 0.9267\n",
      "Epoch 31/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2060 - acc: 0.9283 - val_loss: 0.3132 - val_acc: 0.9217\n",
      "Epoch 32/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.2065 - acc: 0.9308 - val_loss: 0.3287 - val_acc: 0.9192\n",
      "Epoch 33/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1662 - acc: 0.9464 - val_loss: 0.3275 - val_acc: 0.9217\n",
      "Epoch 34/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1764 - acc: 0.9453 - val_loss: 0.3281 - val_acc: 0.9242\n",
      "Epoch 35/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1735 - acc: 0.9458 - val_loss: 0.3337 - val_acc: 0.9225\n",
      "Epoch 36/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1780 - acc: 0.9406 - val_loss: 0.3305 - val_acc: 0.9217\n",
      "Epoch 37/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1611 - acc: 0.9486 - val_loss: 0.3227 - val_acc: 0.9208\n",
      "Epoch 38/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1486 - acc: 0.9528 - val_loss: 0.3267 - val_acc: 0.9208\n",
      "Epoch 39/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1473 - acc: 0.9547 - val_loss: 0.3417 - val_acc: 0.9200\n",
      "Epoch 40/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1637 - acc: 0.9414 - val_loss: 0.3350 - val_acc: 0.9175\n",
      "Epoch 41/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1429 - acc: 0.9522 - val_loss: 0.3375 - val_acc: 0.9217\n",
      "Epoch 42/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1379 - acc: 0.9556 - val_loss: 0.3373 - val_acc: 0.9217\n",
      "Epoch 43/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1388 - acc: 0.9600 - val_loss: 0.3443 - val_acc: 0.9183\n",
      "Epoch 44/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1312 - acc: 0.9556 - val_loss: 0.3517 - val_acc: 0.9208\n",
      "Epoch 45/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1494 - acc: 0.9486 - val_loss: 0.3484 - val_acc: 0.9167\n",
      "Epoch 46/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1433 - acc: 0.9525 - val_loss: 0.3458 - val_acc: 0.9200\n",
      "Epoch 47/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1270 - acc: 0.9572 - val_loss: 0.3518 - val_acc: 0.9192\n",
      "Epoch 48/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1224 - acc: 0.9608 - val_loss: 0.3566 - val_acc: 0.9200\n",
      "Epoch 49/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1134 - acc: 0.9622 - val_loss: 0.3463 - val_acc: 0.9192\n",
      "Epoch 50/50\n",
      "3600/3600 [==============================] - 10s 3ms/step - loss: 0.1374 - acc: 0.9547 - val_loss: 0.3528 - val_acc: 0.9225\n",
      "CPU times: user 4min 27s, sys: 3min 42s, total: 8min 9s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\", input_shape=(len(vocab),)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(len(np.unique(labels)), activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                   batch_size=64,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1_keras = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average=\"micro\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Train_acc: {np.around(np.mean(history.history['acc']), decimals=3)}\")\n",
    "    print(f\"Val_acc: {np.around(np.mean(history.history['val_acc']), decimals=3)}\")\n",
    "    print(f\"Test_acc: {np.around(test_acc, decimals=3)}\") \n",
    "    print(f\"F1-score: {str(np.around(f1_keras, decimals=3))}\")\n",
    "          \n",
    "def plot_results(history):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"ro\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf() #clears the figure\n",
    "\n",
    "    acc = history.history[\"acc\"]\n",
    "    val_acc = history.history[\"val_acc\"]\n",
    "\n",
    "    plt.plot(epochs, acc, \"b\", label=\"Training acc\")\n",
    "    plt.plot(epochs, val_acc, \"ro\", label=\"Validation acc\")\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 2s 2ms/step\n",
      "\n",
      "Results:\n",
      "Train_acc: 0.841\n",
      "Val_acc: 0.885\n",
      "Test_acc: 0.922\n",
      "F1-score: 0.922\n"
     ]
    }
   ],
   "source": [
    "plot_results(history)\n",
    "evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.? Mögliche Fehler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fehlermeldung \"OOM when allocating tensor with...\" (siehe unten) → GPU ist <i>out of memory</i>, d.h. es ist nicht genug GPU Speicher vorhanden. Dieser Fehler ist mir sehr oft begegnet und es ist sehr frustrierend, wenn nach 4 Stunden Parameteroptimierung dieser Fehler die Suche abbricht. Deshalb sollte auf jeden Fall zu Beginn eine geringe Batch-Size gewählt werden und nicht zu viele Parameter auf einmal trainiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import models\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, RMSprop, SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import regularizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import f1_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [32, 64]),\n",
      "        'Dropout': hp.choice('Dropout', [0.2, 0.3]),\n",
      "        'Dense_1': hp.choice('Dense_1', [32, 64]),\n",
      "        'Dropout_1': hp.choice('Dropout_1', [0.2, 0.3]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\")\n",
      "  4: \n",
      "  5: vectorizer = TfidfVectorizer()\n",
      "  6: vector = vectorizer.fit_transform(corpus[\"text\"])\n",
      "  7: labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
      "  8: vocab = vectorizer.vocabulary_\n",
      "  9: \n",
      " 10: X_train, X_test, y_train, y_test = train_test_split(vector, \n",
      " 11:                                                         labels, \n",
      " 12:                                                         test_size=0.4, \n",
      " 13:                                                         train_size=0.6,\n",
      " 14:                                                         random_state=42)\n",
      " 15: X_val = X_test[:1200]\n",
      " 16: X_test = X_test[1200:]\n",
      " 17: \n",
      " 18: y_val = y_test[:1200]\n",
      " 19: y_test = y_test[1200:]\n",
      " 20: \n",
      " 21: y_val = to_categorical(y_val)\n",
      " 22: y_test = to_categorical(y_test)\n",
      " 23: y_train = to_categorical(y_train)\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      " 27: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     \n",
      "  4:     model = models.Sequential()\n",
      "  5:     model.add(Dense(space['Dense'], input_shape=(len(vocab),)))\n",
      "  6:     model.add(Activation('relu'))\n",
      "  7:     model.add(Dropout(space['Dropout']))\n",
      "  8:     model.add(Dense(space['Dense_1']))\n",
      "  9:     model.add(Activation('relu'))\n",
      " 10:     model.add(Dropout(space['Dropout_1'])) \n",
      " 11:     model.add(Dense(len(np.unique(labels))))\n",
      " 12:     model.add(Activation('softmax'))\n",
      " 13:     \n",
      " 14: \n",
      " 15:     model.compile(optimizer='rmsprop',\n",
      " 16:                   loss=\"categorical_crossentropy\",\n",
      " 17:                   metrics=[\"accuracy\"])\n",
      " 18:     \n",
      " 19:     history = model.fit(X_train,\n",
      " 20:                         y_train,\n",
      " 21:                         epochs=75,\n",
      " 22:                         batch_size=16,\n",
      " 23:                         validation_data=(X_val, y_val),\n",
      " 24:                         verbose=2)\n",
      " 25:               \n",
      " 26:     score = model.evaluate(X_test, y_test, verbose=0)\n",
      " 27:     validation_acc = np.mean(history.history['val_acc']) \n",
      " 28:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      " 29: \n",
      "Train on 3600 samples, validate on 1200 samples     \n",
      "Epoch 1/75                                          \n",
      " - 10s - loss: 3.3131 - acc: 0.1014 - val_loss: 3.1475 - val_acc: 0.1400\n",
      "\n",
      "Epoch 2/75                                          \n",
      " - 10s - loss: 2.8457 - acc: 0.2578 - val_loss: 2.4753 - val_acc: 0.5208\n",
      "\n",
      "Epoch 3/75                                          \n",
      " - 10s - loss: 2.1621 - acc: 0.4708 - val_loss: 1.7255 - val_acc: 0.7608\n",
      "\n",
      "Epoch 4/75                                          \n",
      " - 10s - loss: 1.5874 - acc: 0.6206 - val_loss: 1.1827 - val_acc: 0.8617\n",
      "\n",
      "Epoch 5/75                                          \n",
      " - 10s - loss: 1.1890 - acc: 0.6992 - val_loss: 0.8326 - val_acc: 0.8967\n",
      "\n",
      "Epoch 6/75                                          \n",
      " - 10s - loss: 0.8892 - acc: 0.7733 - val_loss: 0.6156 - val_acc: 0.9050\n",
      "\n",
      "Epoch 7/75                                          \n",
      " - 10s - loss: 0.7193 - acc: 0.8136 - val_loss: 0.4881 - val_acc: 0.9108\n",
      "\n",
      "Epoch 8/75                                          \n",
      " - 10s - loss: 0.5555 - acc: 0.8550 - val_loss: 0.4007 - val_acc: 0.9092\n",
      "\n",
      "Epoch 9/75                                          \n",
      " - 10s - loss: 0.4677 - acc: 0.8744 - val_loss: 0.3529 - val_acc: 0.9108\n",
      "\n",
      "Epoch 10/75                                         \n",
      " - 10s - loss: 0.4017 - acc: 0.8858 - val_loss: 0.3135 - val_acc: 0.9208\n",
      "\n",
      "Epoch 11/75                                         \n",
      " - 10s - loss: 0.3348 - acc: 0.9061 - val_loss: 0.2957 - val_acc: 0.9175\n",
      "\n",
      "Epoch 12/75                                         \n",
      " - 10s - loss: 0.2932 - acc: 0.9169 - val_loss: 0.2833 - val_acc: 0.9200\n",
      "\n",
      "Epoch 13/75                                         \n",
      " - 10s - loss: 0.2632 - acc: 0.9217 - val_loss: 0.2721 - val_acc: 0.9225\n",
      "\n",
      "Epoch 14/75                                         \n",
      " - 10s - loss: 0.2388 - acc: 0.9306 - val_loss: 0.2669 - val_acc: 0.9208\n",
      "\n",
      "Epoch 15/75                                         \n",
      " - 10s - loss: 0.2352 - acc: 0.9300 - val_loss: 0.2654 - val_acc: 0.9233\n",
      "\n",
      "Epoch 16/75                                         \n",
      " - 10s - loss: 0.2236 - acc: 0.9303 - val_loss: 0.2619 - val_acc: 0.9225\n",
      "\n",
      "Epoch 17/75                                         \n",
      " - 10s - loss: 0.1856 - acc: 0.9442 - val_loss: 0.2604 - val_acc: 0.9183\n",
      "\n",
      "Epoch 18/75                                         \n",
      " - 10s - loss: 0.1763 - acc: 0.9475 - val_loss: 0.2665 - val_acc: 0.9192\n",
      "\n",
      "Epoch 19/75                                         \n",
      " - 10s - loss: 0.1772 - acc: 0.9464 - val_loss: 0.2644 - val_acc: 0.9208\n",
      "\n",
      "Epoch 20/75                                         \n",
      " - 10s - loss: 0.1649 - acc: 0.9494 - val_loss: 0.2598 - val_acc: 0.9233\n",
      "\n",
      "Epoch 21/75                                         \n",
      " - 10s - loss: 0.1576 - acc: 0.9528 - val_loss: 0.2679 - val_acc: 0.9217\n",
      "\n",
      "Epoch 22/75                                         \n",
      " - 10s - loss: 0.1422 - acc: 0.9556 - val_loss: 0.2745 - val_acc: 0.9208\n",
      "\n",
      "Epoch 23/75                                         \n",
      " - 10s - loss: 0.1326 - acc: 0.9589 - val_loss: 0.2666 - val_acc: 0.9242\n",
      "\n",
      "Epoch 24/75                                         \n",
      " - 10s - loss: 0.1450 - acc: 0.9506 - val_loss: 0.2906 - val_acc: 0.9200\n",
      "\n",
      "Epoch 25/75                                         \n",
      " - 10s - loss: 0.1387 - acc: 0.9567 - val_loss: 0.2801 - val_acc: 0.9192\n",
      "\n",
      "Epoch 26/75                                         \n",
      " - 10s - loss: 0.1343 - acc: 0.9550 - val_loss: 0.2931 - val_acc: 0.9250\n",
      "\n",
      "Epoch 27/75                                         \n",
      " - 10s - loss: 0.1291 - acc: 0.9547 - val_loss: 0.2792 - val_acc: 0.9258\n",
      "\n",
      "Epoch 28/75                                         \n",
      " - 10s - loss: 0.1259 - acc: 0.9578 - val_loss: 0.2917 - val_acc: 0.9275\n",
      "\n",
      "Epoch 29/75                                         \n",
      " - 10s - loss: 0.1136 - acc: 0.9650 - val_loss: 0.2880 - val_acc: 0.9258\n",
      "\n",
      "Epoch 30/75                                         \n",
      " - 10s - loss: 0.1323 - acc: 0.9531 - val_loss: 0.2845 - val_acc: 0.9225\n",
      "\n",
      "Epoch 31/75                                         \n",
      " - 10s - loss: 0.1197 - acc: 0.9619 - val_loss: 0.3015 - val_acc: 0.9225\n",
      "\n",
      "Epoch 32/75                                         \n",
      " - 10s - loss: 0.1079 - acc: 0.9653 - val_loss: 0.2982 - val_acc: 0.9250\n",
      "\n",
      "Epoch 33/75                                         \n",
      " - 10s - loss: 0.1164 - acc: 0.9603 - val_loss: 0.3009 - val_acc: 0.9233\n",
      "\n",
      "Epoch 34/75                                         \n",
      " - 10s - loss: 0.1126 - acc: 0.9625 - val_loss: 0.3140 - val_acc: 0.9250\n",
      "\n",
      "Epoch 35/75                                         \n",
      " - 10s - loss: 0.1122 - acc: 0.9603 - val_loss: 0.3155 - val_acc: 0.9258\n",
      "\n",
      "Epoch 36/75                                         \n",
      " - 10s - loss: 0.1134 - acc: 0.9633 - val_loss: 0.3021 - val_acc: 0.9267\n",
      "\n",
      "Epoch 37/75                                         \n",
      " - 10s - loss: 0.1086 - acc: 0.9639 - val_loss: 0.3022 - val_acc: 0.9300\n",
      "\n",
      "Epoch 38/75                                         \n",
      " - 10s - loss: 0.1075 - acc: 0.9611 - val_loss: 0.3178 - val_acc: 0.9275\n",
      "\n",
      "Epoch 39/75                                         \n",
      " - 10s - loss: 0.0958 - acc: 0.9669 - val_loss: 0.3285 - val_acc: 0.9250\n",
      "\n",
      "Epoch 40/75                                         \n",
      " - 10s - loss: 0.1103 - acc: 0.9633 - val_loss: 0.3440 - val_acc: 0.9225\n",
      "\n",
      "Epoch 41/75                                         \n",
      " - 10s - loss: 0.0969 - acc: 0.9669 - val_loss: 0.3187 - val_acc: 0.9292\n",
      "\n",
      "Epoch 42/75                                         \n",
      " - 10s - loss: 0.0840 - acc: 0.9736 - val_loss: 0.3342 - val_acc: 0.9283\n",
      "\n",
      "Epoch 43/75                                         \n",
      " - 10s - loss: 0.0939 - acc: 0.9703 - val_loss: 0.3364 - val_acc: 0.9292\n",
      "\n",
      "Epoch 44/75                                         \n",
      " - 10s - loss: 0.1077 - acc: 0.9658 - val_loss: 0.3394 - val_acc: 0.9250\n",
      "\n",
      "Epoch 45/75                                         \n",
      " - 10s - loss: 0.1027 - acc: 0.9667 - val_loss: 0.3352 - val_acc: 0.9292\n",
      "\n",
      "Epoch 46/75                                         \n",
      " - 10s - loss: 0.0879 - acc: 0.9694 - val_loss: 0.3388 - val_acc: 0.9275\n",
      "\n",
      "Epoch 47/75                                         \n",
      " - 10s - loss: 0.1008 - acc: 0.9692 - val_loss: 0.3556 - val_acc: 0.9267\n",
      "\n",
      "Epoch 48/75                                         \n",
      " - 10s - loss: 0.0978 - acc: 0.9678 - val_loss: 0.3452 - val_acc: 0.9300\n",
      "\n",
      "Epoch 49/75                                         \n",
      " - 10s - loss: 0.0896 - acc: 0.9706 - val_loss: 0.3436 - val_acc: 0.9283\n",
      "\n",
      "Epoch 50/75                                         \n",
      " - 10s - loss: 0.0925 - acc: 0.9700 - val_loss: 0.3359 - val_acc: 0.9325\n",
      "\n",
      "Epoch 51/75                                         \n",
      " - 10s - loss: 0.0980 - acc: 0.9658 - val_loss: 0.3518 - val_acc: 0.9317\n",
      "\n",
      "Epoch 52/75                                         \n",
      " - 10s - loss: 0.0847 - acc: 0.9714 - val_loss: 0.3586 - val_acc: 0.9283\n",
      "\n",
      "Epoch 53/75                                         \n",
      " - 10s - loss: 0.0953 - acc: 0.9686 - val_loss: 0.3254 - val_acc: 0.9292\n",
      "\n",
      "Epoch 54/75                                         \n",
      " - 10s - loss: 0.0650 - acc: 0.9758 - val_loss: 0.3390 - val_acc: 0.9300\n",
      "\n",
      "Epoch 55/75                                         \n",
      " - 10s - loss: 0.0991 - acc: 0.9653 - val_loss: 0.3397 - val_acc: 0.9300\n",
      "\n",
      "Epoch 56/75                                         \n",
      " - 10s - loss: 0.0744 - acc: 0.9731 - val_loss: 0.3692 - val_acc: 0.9292\n",
      "\n",
      "Epoch 57/75                                         \n",
      " - 10s - loss: 0.0899 - acc: 0.9731 - val_loss: 0.3477 - val_acc: 0.9317\n",
      "\n",
      "Epoch 58/75                                         \n",
      " - 10s - loss: 0.0856 - acc: 0.9706 - val_loss: 0.3782 - val_acc: 0.9242\n",
      "\n",
      "Epoch 59/75                                         \n",
      " - 10s - loss: 0.0884 - acc: 0.9683 - val_loss: 0.3465 - val_acc: 0.9333\n",
      "\n",
      "Epoch 60/75                                         \n",
      " - 10s - loss: 0.0998 - acc: 0.9678 - val_loss: 0.3662 - val_acc: 0.9292\n",
      "\n",
      "Epoch 61/75                                         \n",
      " - 10s - loss: 0.0785 - acc: 0.9744 - val_loss: 0.3517 - val_acc: 0.9292\n",
      "\n",
      "Epoch 62/75                                         \n",
      " - 10s - loss: 0.0845 - acc: 0.9722 - val_loss: 0.3562 - val_acc: 0.9283\n",
      "\n",
      "Epoch 63/75                                         \n",
      " - 10s - loss: 0.0980 - acc: 0.9692 - val_loss: 0.3628 - val_acc: 0.9308\n",
      "\n",
      "Epoch 64/75                                         \n",
      " - 10s - loss: 0.0842 - acc: 0.9731 - val_loss: 0.3665 - val_acc: 0.9275\n",
      "\n",
      "Epoch 65/75                                         \n",
      " - 10s - loss: 0.0770 - acc: 0.9733 - val_loss: 0.3589 - val_acc: 0.9333\n",
      "\n",
      "Epoch 66/75                                         \n",
      " - 10s - loss: 0.0958 - acc: 0.9706 - val_loss: 0.3543 - val_acc: 0.9325\n",
      "\n",
      "Epoch 67/75                                         \n",
      " - 10s - loss: 0.0879 - acc: 0.9717 - val_loss: 0.3730 - val_acc: 0.9275\n",
      "\n",
      "Epoch 68/75                                         \n",
      " - 10s - loss: 0.0790 - acc: 0.9717 - val_loss: 0.3658 - val_acc: 0.9292\n",
      "\n",
      "Epoch 69/75                                         \n",
      " - 10s - loss: 0.0876 - acc: 0.9714 - val_loss: 0.3847 - val_acc: 0.9267\n",
      "\n",
      "Epoch 70/75                                         \n",
      " - 10s - loss: 0.0848 - acc: 0.9694 - val_loss: 0.3807 - val_acc: 0.9242\n",
      "\n",
      "Epoch 71/75                                         \n",
      " - 10s - loss: 0.0846 - acc: 0.9739 - val_loss: 0.3879 - val_acc: 0.9292\n",
      "\n",
      "Epoch 72/75                                         \n",
      " - 10s - loss: 0.0743 - acc: 0.9761 - val_loss: 0.4019 - val_acc: 0.9250\n",
      "\n",
      "Epoch 73/75                                         \n",
      " - 10s - loss: 0.0905 - acc: 0.9714 - val_loss: 0.3923 - val_acc: 0.9292\n",
      "\n",
      "Epoch 74/75                                         \n",
      " - 10s - loss: 0.0816 - acc: 0.9761 - val_loss: 0.4054 - val_acc: 0.9225\n",
      "\n",
      "Epoch 75/75                                         \n",
      " - 10s - loss: 0.0701 - acc: 0.9781 - val_loss: 0.4000 - val_acc: 0.9267\n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                       \n",
      " - 11s - loss: 3.2468 - acc: 0.2144 - val_loss: 2.9080 - val_acc: 0.4242         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 10s - loss: 2.3424 - acc: 0.4997 - val_loss: 1.7713 - val_acc: 0.7583         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 10s - loss: 1.3753 - acc: 0.7156 - val_loss: 0.9960 - val_acc: 0.8975         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 10s - loss: 0.7951 - acc: 0.8375 - val_loss: 0.6027 - val_acc: 0.9100         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 10s - loss: 0.4835 - acc: 0.8994 - val_loss: 0.4153 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 10s - loss: 0.3024 - acc: 0.9381 - val_loss: 0.3210 - val_acc: 0.9300         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 10s - loss: 0.2151 - acc: 0.9531 - val_loss: 0.2753 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 10s - loss: 0.1558 - acc: 0.9664 - val_loss: 0.2526 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 10s - loss: 0.1103 - acc: 0.9750 - val_loss: 0.2461 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 10s - loss: 0.0817 - acc: 0.9808 - val_loss: 0.2296 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 10s - loss: 0.0726 - acc: 0.9831 - val_loss: 0.2247 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 10s - loss: 0.0558 - acc: 0.9867 - val_loss: 0.2308 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 10s - loss: 0.0534 - acc: 0.9858 - val_loss: 0.2404 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 10s - loss: 0.0373 - acc: 0.9925 - val_loss: 0.2438 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 10s - loss: 0.0364 - acc: 0.9911 - val_loss: 0.2313 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 10s - loss: 0.0404 - acc: 0.9881 - val_loss: 0.2305 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 10s - loss: 0.0296 - acc: 0.9908 - val_loss: 0.2384 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 18/75                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.0277 - acc: 0.9908 - val_loss: 0.2504 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 10s - loss: 0.0293 - acc: 0.9906 - val_loss: 0.2377 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 10s - loss: 0.0231 - acc: 0.9936 - val_loss: 0.2347 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 10s - loss: 0.0192 - acc: 0.9956 - val_loss: 0.2541 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 10s - loss: 0.0166 - acc: 0.9961 - val_loss: 0.2461 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 10s - loss: 0.0149 - acc: 0.9961 - val_loss: 0.2584 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 10s - loss: 0.0202 - acc: 0.9925 - val_loss: 0.2630 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 10s - loss: 0.0173 - acc: 0.9944 - val_loss: 0.2541 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 10s - loss: 0.0204 - acc: 0.9936 - val_loss: 0.2774 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 10s - loss: 0.0188 - acc: 0.9953 - val_loss: 0.2753 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 28/75                                                                      \n",
      " - 10s - loss: 0.0189 - acc: 0.9931 - val_loss: 0.2686 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 29/75                                                                      \n",
      " - 10s - loss: 0.0157 - acc: 0.9950 - val_loss: 0.2924 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 30/75                                                                      \n",
      " - 10s - loss: 0.0179 - acc: 0.9958 - val_loss: 0.2688 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 31/75                                                                      \n",
      " - 10s - loss: 0.0183 - acc: 0.9944 - val_loss: 0.2987 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 32/75                                                                      \n",
      " - 10s - loss: 0.0162 - acc: 0.9936 - val_loss: 0.2972 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 33/75                                                                      \n",
      " - 10s - loss: 0.0165 - acc: 0.9939 - val_loss: 0.3025 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 34/75                                                                      \n",
      " - 10s - loss: 0.0171 - acc: 0.9947 - val_loss: 0.2933 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 35/75                                                                      \n",
      " - 10s - loss: 0.0123 - acc: 0.9956 - val_loss: 0.3041 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 36/75                                                                      \n",
      " - 10s - loss: 0.0159 - acc: 0.9942 - val_loss: 0.3099 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 37/75                                                                      \n",
      " - 10s - loss: 0.0121 - acc: 0.9975 - val_loss: 0.2962 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 38/75                                                                      \n",
      " - 10s - loss: 0.0129 - acc: 0.9956 - val_loss: 0.3129 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 39/75                                                                      \n",
      " - 10s - loss: 0.0131 - acc: 0.9953 - val_loss: 0.3146 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 40/75                                                                      \n",
      " - 10s - loss: 0.0154 - acc: 0.9958 - val_loss: 0.3211 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 41/75                                                                      \n",
      " - 10s - loss: 0.0123 - acc: 0.9956 - val_loss: 0.3212 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 42/75                                                                      \n",
      " - 10s - loss: 0.0121 - acc: 0.9956 - val_loss: 0.3203 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 43/75                                                                      \n",
      " - 10s - loss: 0.0168 - acc: 0.9950 - val_loss: 0.3055 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 44/75                                                                      \n",
      " - 10s - loss: 0.0090 - acc: 0.9972 - val_loss: 0.3227 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 45/75                                                                      \n",
      " - 10s - loss: 0.0126 - acc: 0.9944 - val_loss: 0.3139 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 46/75                                                                      \n",
      " - 10s - loss: 0.0127 - acc: 0.9958 - val_loss: 0.3278 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 47/75                                                                      \n",
      " - 10s - loss: 0.0133 - acc: 0.9956 - val_loss: 0.3327 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 48/75                                                                      \n",
      " - 10s - loss: 0.0112 - acc: 0.9964 - val_loss: 0.3234 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 49/75                                                                      \n",
      " - 10s - loss: 0.0088 - acc: 0.9972 - val_loss: 0.3395 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 50/75                                                                      \n",
      " - 10s - loss: 0.0109 - acc: 0.9972 - val_loss: 0.3500 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 51/75                                                                      \n",
      " - 10s - loss: 0.0101 - acc: 0.9964 - val_loss: 0.3546 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 52/75                                                                      \n",
      " - 10s - loss: 0.0107 - acc: 0.9967 - val_loss: 0.3322 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 53/75                                                                      \n",
      " - 10s - loss: 0.0103 - acc: 0.9969 - val_loss: 0.3441 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 54/75                                                                      \n",
      " - 10s - loss: 0.0108 - acc: 0.9969 - val_loss: 0.3454 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 55/75                                                                      \n",
      " - 10s - loss: 0.0085 - acc: 0.9978 - val_loss: 0.3424 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 56/75                                                                      \n",
      " - 10s - loss: 0.0127 - acc: 0.9964 - val_loss: 0.3439 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 57/75                                                                      \n",
      " - 10s - loss: 0.0101 - acc: 0.9975 - val_loss: 0.3525 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 58/75                                                                      \n",
      " - 10s - loss: 0.0093 - acc: 0.9958 - val_loss: 0.3461 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 59/75                                                                      \n",
      " - 10s - loss: 0.0125 - acc: 0.9950 - val_loss: 0.3512 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 60/75                                                                      \n",
      " - 10s - loss: 0.0125 - acc: 0.9947 - val_loss: 0.3531 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 61/75                                                                      \n",
      " - 10s - loss: 0.0121 - acc: 0.9969 - val_loss: 0.3497 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 62/75                                                                      \n",
      " - 10s - loss: 0.0134 - acc: 0.9956 - val_loss: 0.3428 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 63/75                                                                      \n",
      " - 10s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.3415 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 64/75                                                                      \n",
      " - 10s - loss: 0.0096 - acc: 0.9969 - val_loss: 0.3460 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 65/75                                                                      \n",
      " - 10s - loss: 0.0191 - acc: 0.9950 - val_loss: 0.3533 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 66/75                                                                      \n",
      " - 10s - loss: 0.0153 - acc: 0.9956 - val_loss: 0.3758 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 67/75                                                                      \n",
      " - 10s - loss: 0.0146 - acc: 0.9953 - val_loss: 0.3599 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 68/75                                                                      \n",
      " - 10s - loss: 0.0129 - acc: 0.9972 - val_loss: 0.3630 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 69/75                                                                      \n",
      " - 10s - loss: 0.0119 - acc: 0.9956 - val_loss: 0.3781 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 70/75                                                                      \n",
      " - 10s - loss: 0.0116 - acc: 0.9969 - val_loss: 0.3767 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 71/75                                                                      \n",
      " - 10s - loss: 0.0080 - acc: 0.9981 - val_loss: 0.3744 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 72/75                                                                      \n",
      " - 10s - loss: 0.0064 - acc: 0.9975 - val_loss: 0.3717 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 73/75                                                                      \n",
      " - 10s - loss: 0.0129 - acc: 0.9972 - val_loss: 0.3825 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 74/75                                                                      \n",
      " - 10s - loss: 0.0098 - acc: 0.9958 - val_loss: 0.3870 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 75/75                                                                      \n",
      " - 10s - loss: 0.0076 - acc: 0.9978 - val_loss: 0.3806 - val_acc: 0.9408         \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                       \n",
      " - 11s - loss: 3.2682 - acc: 0.1633 - val_loss: 3.0055 - val_acc: 0.4650         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 10s - loss: 2.6523 - acc: 0.3556 - val_loss: 2.1956 - val_acc: 0.7150         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 10s - loss: 1.9203 - acc: 0.5336 - val_loss: 1.4552 - val_acc: 0.8525         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 10s - loss: 1.3375 - acc: 0.6639 - val_loss: 0.9680 - val_acc: 0.8817         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 10s - loss: 0.9791 - acc: 0.7542 - val_loss: 0.6895 - val_acc: 0.8983         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 10s - loss: 0.7469 - acc: 0.8083 - val_loss: 0.5234 - val_acc: 0.9025         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 10s - loss: 0.5566 - acc: 0.8514 - val_loss: 0.4203 - val_acc: 0.9083         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 10s - loss: 0.4492 - acc: 0.8819 - val_loss: 0.3539 - val_acc: 0.9108         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 10s - loss: 0.3875 - acc: 0.8933 - val_loss: 0.3169 - val_acc: 0.9158         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 10s - loss: 0.3182 - acc: 0.9150 - val_loss: 0.2933 - val_acc: 0.9142         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 10s - loss: 0.2722 - acc: 0.9275 - val_loss: 0.2768 - val_acc: 0.9175         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 10s - loss: 0.2428 - acc: 0.9300 - val_loss: 0.2799 - val_acc: 0.9158         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 10s - loss: 0.2062 - acc: 0.9433 - val_loss: 0.2642 - val_acc: 0.9208         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 10s - loss: 0.1792 - acc: 0.9511 - val_loss: 0.2608 - val_acc: 0.9183         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 10s - loss: 0.1597 - acc: 0.9558 - val_loss: 0.2719 - val_acc: 0.9150         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 10s - loss: 0.1581 - acc: 0.9550 - val_loss: 0.2673 - val_acc: 0.9225         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 10s - loss: 0.1532 - acc: 0.9561 - val_loss: 0.2601 - val_acc: 0.9225         \n",
      "\n",
      "Epoch 18/75                                                                      \n",
      " - 10s - loss: 0.1302 - acc: 0.9589 - val_loss: 0.2689 - val_acc: 0.9200         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 10s - loss: 0.1281 - acc: 0.9586 - val_loss: 0.2706 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 10s - loss: 0.1217 - acc: 0.9631 - val_loss: 0.2749 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 10s - loss: 0.1169 - acc: 0.9672 - val_loss: 0.2873 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 10s - loss: 0.1229 - acc: 0.9642 - val_loss: 0.2795 - val_acc: 0.9200         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 10s - loss: 0.1018 - acc: 0.9683 - val_loss: 0.2772 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 10s - loss: 0.1101 - acc: 0.9617 - val_loss: 0.2905 - val_acc: 0.9192         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 10s - loss: 0.1090 - acc: 0.9633 - val_loss: 0.2946 - val_acc: 0.9208         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 10s - loss: 0.0982 - acc: 0.9703 - val_loss: 0.2950 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 10s - loss: 0.0847 - acc: 0.9714 - val_loss: 0.2924 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 28/75                                                                      \n",
      " - 10s - loss: 0.0920 - acc: 0.9683 - val_loss: 0.2947 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 29/75                                                                      \n",
      " - 10s - loss: 0.0877 - acc: 0.9714 - val_loss: 0.3102 - val_acc: 0.9200         \n",
      "\n",
      "Epoch 30/75                                                                      \n",
      " - 10s - loss: 0.0795 - acc: 0.9719 - val_loss: 0.3037 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 31/75                                                                      \n",
      " - 10s - loss: 0.1016 - acc: 0.9675 - val_loss: 0.3106 - val_acc: 0.9225         \n",
      "\n",
      "Epoch 32/75                                                                      \n",
      " - 10s - loss: 0.0807 - acc: 0.9725 - val_loss: 0.3020 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 33/75                                                                      \n",
      " - 10s - loss: 0.0781 - acc: 0.9756 - val_loss: 0.3103 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 34/75                                                                      \n",
      " - 10s - loss: 0.0771 - acc: 0.9742 - val_loss: 0.3200 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 35/75                                                                      \n",
      " - 10s - loss: 0.0737 - acc: 0.9758 - val_loss: 0.3113 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 36/75                                                                      \n",
      " - 10s - loss: 0.0761 - acc: 0.9744 - val_loss: 0.3112 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 37/75                                                                      \n",
      " - 10s - loss: 0.0768 - acc: 0.9764 - val_loss: 0.3250 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 38/75                                                                      \n",
      " - 10s - loss: 0.0787 - acc: 0.9733 - val_loss: 0.3211 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 39/75                                                                      \n",
      " - 10s - loss: 0.0815 - acc: 0.9736 - val_loss: 0.3149 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 40/75                                                                      \n",
      " - 10s - loss: 0.0709 - acc: 0.9750 - val_loss: 0.3366 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 41/75                                                                      \n",
      " - 10s - loss: 0.0823 - acc: 0.9758 - val_loss: 0.3227 - val_acc: 0.9308         \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/75                                                                      \n",
      " - 10s - loss: 0.0753 - acc: 0.9756 - val_loss: 0.3288 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 43/75                                                                      \n",
      " - 10s - loss: 0.0574 - acc: 0.9819 - val_loss: 0.3498 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 44/75                                                                      \n",
      " - 10s - loss: 0.0666 - acc: 0.9792 - val_loss: 0.3489 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 45/75                                                                      \n",
      " - 10s - loss: 0.0639 - acc: 0.9789 - val_loss: 0.3383 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 46/75                                                                      \n",
      " - 10s - loss: 0.0636 - acc: 0.9786 - val_loss: 0.3613 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 47/75                                                                      \n",
      " - 10s - loss: 0.0640 - acc: 0.9786 - val_loss: 0.3512 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 48/75                                                                      \n",
      " - 10s - loss: 0.0625 - acc: 0.9808 - val_loss: 0.3491 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 49/75                                                                      \n",
      " - 10s - loss: 0.0763 - acc: 0.9739 - val_loss: 0.3515 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 50/75                                                                      \n",
      " - 10s - loss: 0.0623 - acc: 0.9789 - val_loss: 0.3570 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 51/75                                                                      \n",
      " - 10s - loss: 0.0639 - acc: 0.9794 - val_loss: 0.3574 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 52/75                                                                      \n",
      " - 10s - loss: 0.0620 - acc: 0.9783 - val_loss: 0.3458 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 53/75                                                                      \n",
      " - 10s - loss: 0.0668 - acc: 0.9783 - val_loss: 0.3638 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 54/75                                                                      \n",
      " - 10s - loss: 0.0586 - acc: 0.9836 - val_loss: 0.3615 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 55/75                                                                      \n",
      " - 10s - loss: 0.0588 - acc: 0.9825 - val_loss: 0.3650 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 56/75                                                                      \n",
      " - 10s - loss: 0.0609 - acc: 0.9764 - val_loss: 0.3793 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 57/75                                                                      \n",
      " - 10s - loss: 0.0539 - acc: 0.9833 - val_loss: 0.3731 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 58/75                                                                      \n",
      " - 10s - loss: 0.0688 - acc: 0.9778 - val_loss: 0.3728 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 59/75                                                                      \n",
      " - 10s - loss: 0.0559 - acc: 0.9814 - val_loss: 0.3638 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 60/75                                                                      \n",
      " - 10s - loss: 0.0692 - acc: 0.9767 - val_loss: 0.3636 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 61/75                                                                      \n",
      " - 10s - loss: 0.0524 - acc: 0.9825 - val_loss: 0.3831 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 62/75                                                                      \n",
      " - 10s - loss: 0.0628 - acc: 0.9797 - val_loss: 0.3669 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 63/75                                                                      \n",
      " - 10s - loss: 0.0583 - acc: 0.9786 - val_loss: 0.3734 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 64/75                                                                      \n",
      " - 10s - loss: 0.0566 - acc: 0.9833 - val_loss: 0.3807 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 65/75                                                                      \n",
      " - 10s - loss: 0.0572 - acc: 0.9797 - val_loss: 0.3711 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 66/75                                                                      \n",
      " - 10s - loss: 0.0652 - acc: 0.9814 - val_loss: 0.3699 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 67/75                                                                      \n",
      " - 10s - loss: 0.0624 - acc: 0.9808 - val_loss: 0.3756 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 68/75                                                                      \n",
      " - 10s - loss: 0.0561 - acc: 0.9819 - val_loss: 0.3638 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 69/75                                                                      \n",
      " - 10s - loss: 0.0585 - acc: 0.9811 - val_loss: 0.3666 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 70/75                                                                      \n",
      " - 10s - loss: 0.0561 - acc: 0.9842 - val_loss: 0.3772 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 71/75                                                                      \n",
      " - 10s - loss: 0.0578 - acc: 0.9819 - val_loss: 0.3948 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 72/75                                                                      \n",
      " - 10s - loss: 0.0531 - acc: 0.9850 - val_loss: 0.4081 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 73/75                                                                      \n",
      " - 10s - loss: 0.0460 - acc: 0.9839 - val_loss: 0.3978 - val_acc: 0.9250         \n",
      "\n",
      "Epoch 74/75                                                                      \n",
      " - 10s - loss: 0.0483 - acc: 0.9847 - val_loss: 0.3859 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 75/75                                                                      \n",
      " - 10s - loss: 0.0503 - acc: 0.9836 - val_loss: 0.3853 - val_acc: 0.9283         \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                       \n",
      " - 14s - loss: 3.1731 - acc: 0.2283 - val_loss: 2.7220 - val_acc: 0.4983         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 13s - loss: 2.1661 - acc: 0.5189 - val_loss: 1.5618 - val_acc: 0.8408         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 13s - loss: 1.2318 - acc: 0.7422 - val_loss: 0.8426 - val_acc: 0.8967         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 13s - loss: 0.6963 - acc: 0.8450 - val_loss: 0.5096 - val_acc: 0.9167         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 13s - loss: 0.4041 - acc: 0.9197 - val_loss: 0.3490 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 13s - loss: 0.2400 - acc: 0.9517 - val_loss: 0.2764 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 13s - loss: 0.1720 - acc: 0.9614 - val_loss: 0.2545 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 13s - loss: 0.1078 - acc: 0.9797 - val_loss: 0.2369 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 13s - loss: 0.0691 - acc: 0.9858 - val_loss: 0.2169 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 13s - loss: 0.0535 - acc: 0.9892 - val_loss: 0.2237 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 13s - loss: 0.0392 - acc: 0.9917 - val_loss: 0.2308 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 13s - loss: 0.0329 - acc: 0.9931 - val_loss: 0.2309 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 13s - loss: 0.0253 - acc: 0.9950 - val_loss: 0.2160 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 13s - loss: 0.0218 - acc: 0.9942 - val_loss: 0.2522 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 13s - loss: 0.0193 - acc: 0.9958 - val_loss: 0.2441 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 13s - loss: 0.0174 - acc: 0.9956 - val_loss: 0.2526 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 13s - loss: 0.0135 - acc: 0.9964 - val_loss: 0.2586 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 18/75                                                                      \n",
      " - 13s - loss: 0.0145 - acc: 0.9969 - val_loss: 0.2644 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 13s - loss: 0.0097 - acc: 0.9978 - val_loss: 0.2592 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 13s - loss: 0.0102 - acc: 0.9972 - val_loss: 0.2791 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 13s - loss: 0.0110 - acc: 0.9972 - val_loss: 0.2628 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 13s - loss: 0.0113 - acc: 0.9958 - val_loss: 0.2773 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 13s - loss: 0.0083 - acc: 0.9969 - val_loss: 0.3146 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 13s - loss: 0.0119 - acc: 0.9961 - val_loss: 0.2932 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 13s - loss: 0.0084 - acc: 0.9964 - val_loss: 0.2993 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 13s - loss: 0.0092 - acc: 0.9975 - val_loss: 0.2820 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 13s - loss: 0.0087 - acc: 0.9978 - val_loss: 0.3013 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 28/75                                                                      \n",
      " - 13s - loss: 0.0077 - acc: 0.9964 - val_loss: 0.3211 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 29/75                                                                      \n",
      " - 13s - loss: 0.0066 - acc: 0.9981 - val_loss: 0.3025 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 30/75                                                                      \n",
      " - 13s - loss: 0.0057 - acc: 0.9975 - val_loss: 0.2886 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 31/75                                                                      \n",
      " - 13s - loss: 0.0071 - acc: 0.9981 - val_loss: 0.3091 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 32/75                                                                      \n",
      " - 13s - loss: 0.0085 - acc: 0.9983 - val_loss: 0.3282 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 33/75                                                                      \n",
      " - 13s - loss: 0.0077 - acc: 0.9978 - val_loss: 0.3452 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 34/75                                                                      \n",
      " - 13s - loss: 0.0137 - acc: 0.9969 - val_loss: 0.3624 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 35/75                                                                      \n",
      " - 13s - loss: 0.0054 - acc: 0.9986 - val_loss: 0.3351 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 36/75                                                                      \n",
      " - 13s - loss: 0.0067 - acc: 0.9978 - val_loss: 0.3427 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 37/75                                                                      \n",
      " - 13s - loss: 0.0054 - acc: 0.9978 - val_loss: 0.3469 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 38/75                                                                      \n",
      " - 13s - loss: 0.0041 - acc: 0.9986 - val_loss: 0.3752 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 39/75                                                                      \n",
      " - 13s - loss: 0.0051 - acc: 0.9983 - val_loss: 0.3285 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 40/75                                                                      \n",
      " - 13s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.3530 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 41/75                                                                      \n",
      " - 13s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.3365 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 42/75                                                                      \n",
      " - 13s - loss: 0.0039 - acc: 0.9992 - val_loss: 0.3602 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 43/75                                                                      \n",
      " - 13s - loss: 0.0053 - acc: 0.9983 - val_loss: 0.3669 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 44/75                                                                      \n",
      " - 13s - loss: 0.0046 - acc: 0.9992 - val_loss: 0.3676 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 45/75                                                                      \n",
      " - 13s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.3719 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 46/75                                                                      \n",
      " - 13s - loss: 0.0037 - acc: 0.9986 - val_loss: 0.3756 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 47/75                                                                      \n",
      " - 13s - loss: 0.0058 - acc: 0.9986 - val_loss: 0.3782 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 48/75                                                                      \n",
      " - 13s - loss: 0.0062 - acc: 0.9981 - val_loss: 0.3970 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 49/75                                                                      \n",
      " - 13s - loss: 0.0031 - acc: 0.9986 - val_loss: 0.3948 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 50/75                                                                      \n",
      " - 13s - loss: 0.0046 - acc: 0.9981 - val_loss: 0.3891 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 51/75                                                                      \n",
      " - 13s - loss: 0.0070 - acc: 0.9978 - val_loss: 0.3878 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 52/75                                                                      \n",
      " - 13s - loss: 0.0063 - acc: 0.9975 - val_loss: 0.3740 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 53/75                                                                      \n",
      " - 13s - loss: 0.0045 - acc: 0.9992 - val_loss: 0.3847 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 54/75                                                                      \n",
      " - 13s - loss: 0.0078 - acc: 0.9983 - val_loss: 0.3720 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 55/75                                                                      \n",
      " - 13s - loss: 0.0045 - acc: 0.9989 - val_loss: 0.3662 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 56/75                                                                      \n",
      " - 13s - loss: 0.0047 - acc: 0.9981 - val_loss: 0.4101 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 57/75                                                                      \n",
      " - 13s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.3958 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 58/75                                                                      \n",
      " - 13s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.4408 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 59/75                                                                      \n",
      " - 13s - loss: 0.0022 - acc: 0.9992 - val_loss: 0.4082 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 60/75                                                                      \n",
      " - 13s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.3988 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 61/75                                                                      \n",
      " - 13s - loss: 0.0037 - acc: 0.9986 - val_loss: 0.4281 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 62/75                                                                      \n",
      " - 13s - loss: 0.0056 - acc: 0.9978 - val_loss: 0.4150 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 63/75                                                                      \n",
      " - 13s - loss: 0.0015 - acc: 0.9994 - val_loss: 0.4125 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 64/75                                                                      \n",
      " - 13s - loss: 0.0020 - acc: 0.9989 - val_loss: 0.4207 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 65/75                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 0.0102 - acc: 0.9975 - val_loss: 0.4323 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 66/75                                                                      \n",
      " - 13s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.4426 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 67/75                                                                      \n",
      " - 13s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.4273 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 68/75                                                                      \n",
      " - 13s - loss: 0.0036 - acc: 0.9981 - val_loss: 0.4305 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 69/75                                                                      \n",
      " - 13s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.4694 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 70/75                                                                      \n",
      " - 13s - loss: 0.0019 - acc: 0.9992 - val_loss: 0.4130 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 71/75                                                                      \n",
      " - 13s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.4386 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 72/75                                                                      \n",
      " - 13s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.4418 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 73/75                                                                      \n",
      " - 13s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.4388 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 74/75                                                                      \n",
      " - 13s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.4501 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 75/75                                                                      \n",
      " - 13s - loss: 0.0021 - acc: 0.9989 - val_loss: 0.4424 - val_acc: 0.9408         \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                       \n",
      " - 11s - loss: 3.2665 - acc: 0.1683 - val_loss: 3.0045 - val_acc: 0.3383         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 10s - loss: 2.5296 - acc: 0.3883 - val_loss: 2.0505 - val_acc: 0.6208         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 10s - loss: 1.6847 - acc: 0.5944 - val_loss: 1.2562 - val_acc: 0.8442         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 10s - loss: 1.0909 - acc: 0.7414 - val_loss: 0.8112 - val_acc: 0.8850         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 10s - loss: 0.7136 - acc: 0.8383 - val_loss: 0.5481 - val_acc: 0.9050         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 10s - loss: 0.4959 - acc: 0.8836 - val_loss: 0.4060 - val_acc: 0.9200         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 10s - loss: 0.3499 - acc: 0.9206 - val_loss: 0.3250 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 10s - loss: 0.2610 - acc: 0.9386 - val_loss: 0.2796 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 10s - loss: 0.1966 - acc: 0.9528 - val_loss: 0.2586 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 10s - loss: 0.1605 - acc: 0.9606 - val_loss: 0.2549 - val_acc: 0.9233         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 10s - loss: 0.1325 - acc: 0.9658 - val_loss: 0.2321 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 10s - loss: 0.1004 - acc: 0.9750 - val_loss: 0.2287 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 10s - loss: 0.0864 - acc: 0.9775 - val_loss: 0.2289 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 10s - loss: 0.0848 - acc: 0.9778 - val_loss: 0.2229 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 10s - loss: 0.0749 - acc: 0.9783 - val_loss: 0.2352 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 10s - loss: 0.0649 - acc: 0.9808 - val_loss: 0.2181 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 10s - loss: 0.0650 - acc: 0.9817 - val_loss: 0.2411 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 18/75                                                                      \n",
      " - 10s - loss: 0.0579 - acc: 0.9825 - val_loss: 0.2382 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 10s - loss: 0.0535 - acc: 0.9869 - val_loss: 0.2321 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 10s - loss: 0.0507 - acc: 0.9847 - val_loss: 0.2370 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 10s - loss: 0.0436 - acc: 0.9864 - val_loss: 0.2495 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 10s - loss: 0.0400 - acc: 0.9878 - val_loss: 0.2591 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 10s - loss: 0.0437 - acc: 0.9869 - val_loss: 0.2468 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 10s - loss: 0.0362 - acc: 0.9900 - val_loss: 0.2478 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 10s - loss: 0.0290 - acc: 0.9908 - val_loss: 0.2546 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 10s - loss: 0.0315 - acc: 0.9894 - val_loss: 0.2739 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 10s - loss: 0.0357 - acc: 0.9886 - val_loss: 0.2533 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 28/75                                                                        \n",
      " - 10s - loss: 0.0274 - acc: 0.9922 - val_loss: 0.2566 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 29/75                                                                        \n",
      " - 10s - loss: 0.0333 - acc: 0.9908 - val_loss: 0.2641 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 30/75                                                                        \n",
      " - 10s - loss: 0.0393 - acc: 0.9886 - val_loss: 0.2731 - val_acc: 0.9283           \n",
      "\n",
      "Epoch 31/75                                                                        \n",
      " - 10s - loss: 0.0310 - acc: 0.9889 - val_loss: 0.2679 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 32/75                                                                        \n",
      " - 10s - loss: 0.0319 - acc: 0.9914 - val_loss: 0.2837 - val_acc: 0.9300           \n",
      "\n",
      "Epoch 33/75                                                                        \n",
      " - 10s - loss: 0.0299 - acc: 0.9897 - val_loss: 0.2903 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 34/75                                                                        \n",
      " - 10s - loss: 0.0267 - acc: 0.9897 - val_loss: 0.2865 - val_acc: 0.9375           \n",
      "\n",
      "Epoch 35/75                                                                        \n",
      " - 10s - loss: 0.0218 - acc: 0.9922 - val_loss: 0.2824 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 36/75                                                                        \n",
      " - 10s - loss: 0.0227 - acc: 0.9906 - val_loss: 0.2832 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 37/75                                                                        \n",
      " - 10s - loss: 0.0197 - acc: 0.9928 - val_loss: 0.2879 - val_acc: 0.9292           \n",
      "\n",
      "Epoch 38/75                                                                        \n",
      " - 10s - loss: 0.0157 - acc: 0.9939 - val_loss: 0.2911 - val_acc: 0.9375           \n",
      "\n",
      "Epoch 39/75                                                                        \n",
      " - 10s - loss: 0.0248 - acc: 0.9922 - val_loss: 0.2884 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 40/75                                                                        \n",
      " - 10s - loss: 0.0227 - acc: 0.9936 - val_loss: 0.2998 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 41/75                                                                        \n",
      " - 10s - loss: 0.0309 - acc: 0.9908 - val_loss: 0.2949 - val_acc: 0.9383           \n",
      "\n",
      "Epoch 42/75                                                                        \n",
      " - 10s - loss: 0.0227 - acc: 0.9908 - val_loss: 0.3189 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 43/75                                                                        \n",
      " - 10s - loss: 0.0242 - acc: 0.9925 - val_loss: 0.3092 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 44/75                                                                        \n",
      " - 10s - loss: 0.0209 - acc: 0.9928 - val_loss: 0.3284 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 45/75                                                                        \n",
      " - 10s - loss: 0.0179 - acc: 0.9931 - val_loss: 0.3042 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 46/75                                                                        \n",
      " - 10s - loss: 0.0228 - acc: 0.9936 - val_loss: 0.3238 - val_acc: 0.9283           \n",
      "\n",
      "Epoch 47/75                                                                        \n",
      " - 10s - loss: 0.0234 - acc: 0.9914 - val_loss: 0.3206 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 48/75                                                                        \n",
      " - 10s - loss: 0.0248 - acc: 0.9919 - val_loss: 0.3329 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 49/75                                                                        \n",
      " - 10s - loss: 0.0227 - acc: 0.9917 - val_loss: 0.3308 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 50/75                                                                        \n",
      " - 10s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.3349 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 51/75                                                                        \n",
      " - 10s - loss: 0.0253 - acc: 0.9933 - val_loss: 0.3396 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 52/75                                                                        \n",
      " - 10s - loss: 0.0208 - acc: 0.9925 - val_loss: 0.3318 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 53/75                                                                        \n",
      " - 10s - loss: 0.0218 - acc: 0.9944 - val_loss: 0.3318 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 54/75                                                                        \n",
      " - 10s - loss: 0.0225 - acc: 0.9939 - val_loss: 0.3415 - val_acc: 0.9275           \n",
      "\n",
      "Epoch 55/75                                                                        \n",
      " - 10s - loss: 0.0200 - acc: 0.9939 - val_loss: 0.3417 - val_acc: 0.9292           \n",
      "\n",
      "Epoch 56/75                                                                        \n",
      " - 10s - loss: 0.0234 - acc: 0.9942 - val_loss: 0.3409 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 57/75                                                                        \n",
      " - 10s - loss: 0.0194 - acc: 0.9950 - val_loss: 0.3424 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 58/75                                                                        \n",
      " - 10s - loss: 0.0301 - acc: 0.9906 - val_loss: 0.3578 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 59/75                                                                        \n",
      " - 10s - loss: 0.0164 - acc: 0.9936 - val_loss: 0.3486 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 60/75                                                                        \n",
      " - 10s - loss: 0.0171 - acc: 0.9936 - val_loss: 0.3451 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 61/75                                                                        \n",
      " - 10s - loss: 0.0173 - acc: 0.9939 - val_loss: 0.3645 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 62/75                                                                        \n",
      " - 10s - loss: 0.0163 - acc: 0.9939 - val_loss: 0.3521 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 63/75                                                                        \n",
      " - 10s - loss: 0.0177 - acc: 0.9939 - val_loss: 0.3466 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 64/75                                                                        \n",
      " - 10s - loss: 0.0119 - acc: 0.9958 - val_loss: 0.3531 - val_acc: 0.9392           \n",
      "\n",
      "Epoch 65/75                                                                        \n",
      " - 10s - loss: 0.0173 - acc: 0.9950 - val_loss: 0.3515 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 66/75                                                                        \n",
      " - 10s - loss: 0.0197 - acc: 0.9931 - val_loss: 0.3581 - val_acc: 0.9375           \n",
      "\n",
      "Epoch 67/75                                                                        \n",
      " - 10s - loss: 0.0154 - acc: 0.9953 - val_loss: 0.3799 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 68/75                                                                        \n",
      " - 10s - loss: 0.0207 - acc: 0.9942 - val_loss: 0.3664 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 69/75                                                                        \n",
      " - 10s - loss: 0.0193 - acc: 0.9939 - val_loss: 0.3579 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 70/75                                                                        \n",
      " - 10s - loss: 0.0281 - acc: 0.9919 - val_loss: 0.3596 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 71/75                                                                        \n",
      " - 10s - loss: 0.0116 - acc: 0.9964 - val_loss: 0.3884 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 72/75                                                                        \n",
      " - 10s - loss: 0.0165 - acc: 0.9950 - val_loss: 0.3807 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 73/75                                                                        \n",
      " - 10s - loss: 0.0113 - acc: 0.9958 - val_loss: 0.3731 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 74/75                                                                        \n",
      " - 10s - loss: 0.0163 - acc: 0.9950 - val_loss: 0.3838 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 75/75                                                                        \n",
      " - 10s - loss: 0.0088 - acc: 0.9967 - val_loss: 0.3834 - val_acc: 0.9375           \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                    \n",
      "Epoch 1/75                                                                         \n",
      " - 11s - loss: 3.3086 - acc: 0.0986 - val_loss: 3.1512 - val_acc: 0.1550           \n",
      "\n",
      "Epoch 2/75                                                                         \n",
      " - 10s - loss: 2.8455 - acc: 0.2614 - val_loss: 2.5274 - val_acc: 0.5700           \n",
      "\n",
      "Epoch 3/75                                                                         \n",
      " - 10s - loss: 2.1748 - acc: 0.4742 - val_loss: 1.7827 - val_acc: 0.7958           \n",
      "\n",
      "Epoch 4/75                                                                         \n",
      " - 10s - loss: 1.5619 - acc: 0.6303 - val_loss: 1.2127 - val_acc: 0.8750           \n",
      "\n",
      "Epoch 5/75                                                                         \n",
      " - 10s - loss: 1.1624 - acc: 0.7222 - val_loss: 0.8459 - val_acc: 0.9042           \n",
      "\n",
      "Epoch 6/75                                                                         \n",
      " - 10s - loss: 0.8754 - acc: 0.7847 - val_loss: 0.6260 - val_acc: 0.9117           \n",
      "\n",
      "Epoch 7/75                                                                         \n",
      " - 10s - loss: 0.6937 - acc: 0.8233 - val_loss: 0.4883 - val_acc: 0.9175           \n",
      "\n",
      "Epoch 8/75                                                                         \n",
      " - 10s - loss: 0.5497 - acc: 0.8597 - val_loss: 0.4083 - val_acc: 0.9167           \n",
      "\n",
      "Epoch 9/75                                                                         \n",
      " - 10s - loss: 0.4684 - acc: 0.8744 - val_loss: 0.3535 - val_acc: 0.9208           \n",
      "\n",
      "Epoch 10/75                                                                        \n",
      " - 10s - loss: 0.3968 - acc: 0.8936 - val_loss: 0.3151 - val_acc: 0.9267           \n",
      "\n",
      "Epoch 11/75                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.3337 - acc: 0.9097 - val_loss: 0.2896 - val_acc: 0.9250           \n",
      "\n",
      "Epoch 12/75                                                                        \n",
      " - 10s - loss: 0.2905 - acc: 0.9200 - val_loss: 0.2705 - val_acc: 0.9242           \n",
      "\n",
      "Epoch 13/75                                                                        \n",
      " - 10s - loss: 0.2583 - acc: 0.9225 - val_loss: 0.2632 - val_acc: 0.9267           \n",
      "\n",
      "Epoch 14/75                                                                        \n",
      " - 10s - loss: 0.2417 - acc: 0.9322 - val_loss: 0.2559 - val_acc: 0.9283           \n",
      "\n",
      "Epoch 15/75                                                                        \n",
      " - 10s - loss: 0.2384 - acc: 0.9292 - val_loss: 0.2493 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 16/75                                                                        \n",
      " - 10s - loss: 0.2124 - acc: 0.9336 - val_loss: 0.2415 - val_acc: 0.9292           \n",
      "\n",
      "Epoch 17/75                                                                        \n",
      " - 10s - loss: 0.2053 - acc: 0.9408 - val_loss: 0.2387 - val_acc: 0.9300           \n",
      "\n",
      "Epoch 18/75                                                                        \n",
      " - 10s - loss: 0.1858 - acc: 0.9428 - val_loss: 0.2469 - val_acc: 0.9300           \n",
      "\n",
      "Epoch 19/75                                                                        \n",
      " - 10s - loss: 0.1626 - acc: 0.9500 - val_loss: 0.2441 - val_acc: 0.9275           \n",
      "\n",
      "Epoch 20/75                                                                        \n",
      " - 10s - loss: 0.1872 - acc: 0.9389 - val_loss: 0.2482 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 21/75                                                                        \n",
      " - 10s - loss: 0.1657 - acc: 0.9467 - val_loss: 0.2523 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 22/75                                                                        \n",
      " - 10s - loss: 0.1501 - acc: 0.9528 - val_loss: 0.2504 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 23/75                                                                        \n",
      " - 10s - loss: 0.1612 - acc: 0.9511 - val_loss: 0.2545 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 24/75                                                                        \n",
      " - 10s - loss: 0.1563 - acc: 0.9492 - val_loss: 0.2531 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 25/75                                                                        \n",
      " - 10s - loss: 0.1318 - acc: 0.9583 - val_loss: 0.2646 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 26/75                                                                        \n",
      " - 10s - loss: 0.1436 - acc: 0.9522 - val_loss: 0.2399 - val_acc: 0.9392           \n",
      "\n",
      "Epoch 27/75                                                                        \n",
      " - 10s - loss: 0.1359 - acc: 0.9558 - val_loss: 0.2522 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 28/75                                                                        \n",
      " - 10s - loss: 0.1435 - acc: 0.9556 - val_loss: 0.2517 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 29/75                                                                        \n",
      " - 10s - loss: 0.1256 - acc: 0.9578 - val_loss: 0.2541 - val_acc: 0.9383           \n",
      "\n",
      "Epoch 30/75                                                                        \n",
      " - 10s - loss: 0.1136 - acc: 0.9606 - val_loss: 0.2517 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 31/75                                                                        \n",
      " - 10s - loss: 0.1364 - acc: 0.9536 - val_loss: 0.2759 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 32/75                                                                        \n",
      " - 10s - loss: 0.1191 - acc: 0.9622 - val_loss: 0.2751 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 33/75                                                                        \n",
      " - 10s - loss: 0.1169 - acc: 0.9603 - val_loss: 0.2799 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 34/75                                                                        \n",
      " - 10s - loss: 0.1230 - acc: 0.9600 - val_loss: 0.2855 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 35/75                                                                        \n",
      " - 10s - loss: 0.1047 - acc: 0.9636 - val_loss: 0.2796 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 36/75                                                                        \n",
      " - 10s - loss: 0.1224 - acc: 0.9597 - val_loss: 0.2898 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 37/75                                                                        \n",
      " - 10s - loss: 0.1182 - acc: 0.9619 - val_loss: 0.2627 - val_acc: 0.9375           \n",
      "\n",
      "Epoch 38/75                                                                        \n",
      " - 10s - loss: 0.1102 - acc: 0.9617 - val_loss: 0.2820 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 39/75                                                                        \n",
      " - 10s - loss: 0.1074 - acc: 0.9608 - val_loss: 0.2858 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 40/75                                                                        \n",
      " - 10s - loss: 0.1011 - acc: 0.9669 - val_loss: 0.2805 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 41/75                                                                        \n",
      " - 10s - loss: 0.1152 - acc: 0.9594 - val_loss: 0.2777 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 42/75                                                                        \n",
      " - 10s - loss: 0.1042 - acc: 0.9667 - val_loss: 0.2907 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 43/75                                                                        \n",
      " - 10s - loss: 0.1215 - acc: 0.9608 - val_loss: 0.3003 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 44/75                                                                        \n",
      " - 10s - loss: 0.1027 - acc: 0.9633 - val_loss: 0.2898 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 45/75                                                                        \n",
      " - 10s - loss: 0.1015 - acc: 0.9653 - val_loss: 0.2952 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 46/75                                                                        \n",
      " - 10s - loss: 0.1186 - acc: 0.9622 - val_loss: 0.2953 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 47/75                                                                        \n",
      " - 10s - loss: 0.0947 - acc: 0.9681 - val_loss: 0.3001 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 48/75                                                                        \n",
      " - 10s - loss: 0.1143 - acc: 0.9636 - val_loss: 0.2985 - val_acc: 0.9358           \n",
      "\n",
      "Epoch 49/75                                                                        \n",
      " - 10s - loss: 0.1000 - acc: 0.9706 - val_loss: 0.3023 - val_acc: 0.9375           \n",
      "\n",
      "Epoch 50/75                                                                        \n",
      " - 10s - loss: 0.0883 - acc: 0.9747 - val_loss: 0.2974 - val_acc: 0.9367           \n",
      "\n",
      "Epoch 51/75                                                                        \n",
      " - 10s - loss: 0.1035 - acc: 0.9664 - val_loss: 0.3126 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 52/75                                                                        \n",
      " - 10s - loss: 0.0936 - acc: 0.9703 - val_loss: 0.3223 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 53/75                                                                        \n",
      " - 10s - loss: 0.1047 - acc: 0.9650 - val_loss: 0.3178 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 54/75                                                                        \n",
      " - 10s - loss: 0.0961 - acc: 0.9642 - val_loss: 0.3341 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 55/75                                                                        \n",
      " - 10s - loss: 0.0874 - acc: 0.9719 - val_loss: 0.3096 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 56/75                                                                        \n",
      " - 10s - loss: 0.1008 - acc: 0.9667 - val_loss: 0.3292 - val_acc: 0.9383           \n",
      "\n",
      "Epoch 57/75                                                                        \n",
      " - 10s - loss: 0.0970 - acc: 0.9683 - val_loss: 0.3416 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 58/75                                                                        \n",
      " - 10s - loss: 0.0908 - acc: 0.9686 - val_loss: 0.3285 - val_acc: 0.9350           \n",
      "\n",
      "Epoch 59/75                                                                        \n",
      " - 10s - loss: 0.0914 - acc: 0.9708 - val_loss: 0.3544 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 60/75                                                                        \n",
      " - 10s - loss: 0.0952 - acc: 0.9686 - val_loss: 0.3266 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 61/75                                                                        \n",
      " - 10s - loss: 0.0904 - acc: 0.9700 - val_loss: 0.3445 - val_acc: 0.9300           \n",
      "\n",
      "Epoch 62/75                                                                        \n",
      " - 10s - loss: 0.0929 - acc: 0.9686 - val_loss: 0.3337 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 63/75                                                                        \n",
      " - 10s - loss: 0.0921 - acc: 0.9669 - val_loss: 0.3291 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 64/75                                                                        \n",
      " - 10s - loss: 0.0906 - acc: 0.9725 - val_loss: 0.3462 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 65/75                                                                        \n",
      " - 10s - loss: 0.0983 - acc: 0.9689 - val_loss: 0.3378 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 66/75                                                                        \n",
      " - 10s - loss: 0.0951 - acc: 0.9675 - val_loss: 0.3384 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 67/75                                                                        \n",
      " - 10s - loss: 0.0851 - acc: 0.9711 - val_loss: 0.3538 - val_acc: 0.9342           \n",
      "\n",
      "Epoch 68/75                                                                        \n",
      " - 10s - loss: 0.0862 - acc: 0.9717 - val_loss: 0.3351 - val_acc: 0.9300           \n",
      "\n",
      "Epoch 69/75                                                                        \n",
      " - 10s - loss: 0.1077 - acc: 0.9642 - val_loss: 0.3419 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 70/75                                                                        \n",
      " - 10s - loss: 0.0781 - acc: 0.9733 - val_loss: 0.3429 - val_acc: 0.9308           \n",
      "\n",
      "Epoch 71/75                                                                        \n",
      " - 10s - loss: 0.0885 - acc: 0.9725 - val_loss: 0.3478 - val_acc: 0.9333           \n",
      "\n",
      "Epoch 72/75                                                                        \n",
      " - 10s - loss: 0.0872 - acc: 0.9725 - val_loss: 0.3629 - val_acc: 0.9317           \n",
      "\n",
      "Epoch 73/75                                                                        \n",
      " - 10s - loss: 0.0813 - acc: 0.9731 - val_loss: 0.3755 - val_acc: 0.9258           \n",
      "\n",
      "Epoch 74/75                                                                        \n",
      " - 10s - loss: 0.0973 - acc: 0.9675 - val_loss: 0.3613 - val_acc: 0.9325           \n",
      "\n",
      "Epoch 75/75                                                                        \n",
      " - 10s - loss: 0.0801 - acc: 0.9758 - val_loss: 0.3553 - val_acc: 0.9333           \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                    \n",
      "Epoch 1/75                                                                       \n",
      " - 14s - loss: 3.2408 - acc: 0.1569 - val_loss: 2.9189 - val_acc: 0.4900         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 13s - loss: 2.4821 - acc: 0.4269 - val_loss: 1.9549 - val_acc: 0.7917         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 13s - loss: 1.6502 - acc: 0.6058 - val_loss: 1.1788 - val_acc: 0.8742         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 13s - loss: 1.0804 - acc: 0.7472 - val_loss: 0.7560 - val_acc: 0.9058         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 13s - loss: 0.7413 - acc: 0.8219 - val_loss: 0.5155 - val_acc: 0.9183         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 13s - loss: 0.5308 - acc: 0.8667 - val_loss: 0.4017 - val_acc: 0.9208         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 13s - loss: 0.3874 - acc: 0.9011 - val_loss: 0.3321 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 13s - loss: 0.3163 - acc: 0.9189 - val_loss: 0.2955 - val_acc: 0.9208         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 13s - loss: 0.2328 - acc: 0.9442 - val_loss: 0.2630 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 13s - loss: 0.1970 - acc: 0.9489 - val_loss: 0.2621 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 13s - loss: 0.1680 - acc: 0.9575 - val_loss: 0.2656 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 13s - loss: 0.1496 - acc: 0.9614 - val_loss: 0.2519 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 13s - loss: 0.1249 - acc: 0.9664 - val_loss: 0.2487 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 13s - loss: 0.1048 - acc: 0.9697 - val_loss: 0.2555 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 13s - loss: 0.1042 - acc: 0.9675 - val_loss: 0.2643 - val_acc: 0.9242         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 13s - loss: 0.0847 - acc: 0.9775 - val_loss: 0.2617 - val_acc: 0.9225         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 13s - loss: 0.0940 - acc: 0.9714 - val_loss: 0.2494 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 18/75                                                                      \n",
      " - 13s - loss: 0.0920 - acc: 0.9742 - val_loss: 0.2643 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 13s - loss: 0.0687 - acc: 0.9825 - val_loss: 0.2538 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 13s - loss: 0.0773 - acc: 0.9769 - val_loss: 0.2826 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 13s - loss: 0.0674 - acc: 0.9808 - val_loss: 0.2720 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 13s - loss: 0.0769 - acc: 0.9781 - val_loss: 0.2711 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 13s - loss: 0.0664 - acc: 0.9781 - val_loss: 0.2757 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 13s - loss: 0.0567 - acc: 0.9831 - val_loss: 0.2844 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 13s - loss: 0.0542 - acc: 0.9806 - val_loss: 0.2783 - val_acc: 0.9300         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 13s - loss: 0.0501 - acc: 0.9844 - val_loss: 0.2870 - val_acc: 0.9300         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 13s - loss: 0.0498 - acc: 0.9831 - val_loss: 0.2870 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 28/75                                                                      \n",
      " - 13s - loss: 0.0499 - acc: 0.9842 - val_loss: 0.2932 - val_acc: 0.9267         \n",
      "\n",
      "Epoch 29/75                                                                      \n",
      " - 13s - loss: 0.0450 - acc: 0.9853 - val_loss: 0.2925 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 30/75                                                                      \n",
      " - 13s - loss: 0.0389 - acc: 0.9867 - val_loss: 0.2874 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 31/75                                                                      \n",
      " - 13s - loss: 0.0414 - acc: 0.9858 - val_loss: 0.3072 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 32/75                                                                      \n",
      " - 13s - loss: 0.0421 - acc: 0.9869 - val_loss: 0.3089 - val_acc: 0.9283         \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/75                                                                      \n",
      " - 13s - loss: 0.0488 - acc: 0.9853 - val_loss: 0.2996 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 34/75                                                                      \n",
      " - 13s - loss: 0.0464 - acc: 0.9842 - val_loss: 0.3125 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 35/75                                                                      \n",
      " - 13s - loss: 0.0506 - acc: 0.9822 - val_loss: 0.3176 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 36/75                                                                      \n",
      " - 13s - loss: 0.0460 - acc: 0.9844 - val_loss: 0.3125 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 37/75                                                                      \n",
      " - 13s - loss: 0.0338 - acc: 0.9886 - val_loss: 0.3186 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 38/75                                                                      \n",
      " - 13s - loss: 0.0427 - acc: 0.9864 - val_loss: 0.3103 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 39/75                                                                      \n",
      " - 13s - loss: 0.0341 - acc: 0.9872 - val_loss: 0.3258 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 40/75                                                                      \n",
      " - 13s - loss: 0.0301 - acc: 0.9894 - val_loss: 0.3373 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 41/75                                                                      \n",
      " - 13s - loss: 0.0502 - acc: 0.9833 - val_loss: 0.3304 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 42/75                                                                      \n",
      " - 13s - loss: 0.0361 - acc: 0.9856 - val_loss: 0.3295 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 43/75                                                                      \n",
      " - 13s - loss: 0.0306 - acc: 0.9875 - val_loss: 0.3313 - val_acc: 0.9275         \n",
      "\n",
      "Epoch 44/75                                                                      \n",
      " - 13s - loss: 0.0377 - acc: 0.9856 - val_loss: 0.3533 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 45/75                                                                      \n",
      " - 13s - loss: 0.0370 - acc: 0.9883 - val_loss: 0.3507 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 46/75                                                                      \n",
      " - 13s - loss: 0.0408 - acc: 0.9864 - val_loss: 0.3486 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 47/75                                                                      \n",
      " - 13s - loss: 0.0312 - acc: 0.9900 - val_loss: 0.3414 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 48/75                                                                      \n",
      " - 13s - loss: 0.0389 - acc: 0.9875 - val_loss: 0.3366 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 49/75                                                                      \n",
      " - 13s - loss: 0.0282 - acc: 0.9908 - val_loss: 0.3341 - val_acc: 0.9350         \n",
      "\n",
      "Epoch 50/75                                                                      \n",
      " - 13s - loss: 0.0318 - acc: 0.9886 - val_loss: 0.3481 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 51/75                                                                      \n",
      " - 13s - loss: 0.0370 - acc: 0.9856 - val_loss: 0.3648 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 52/75                                                                      \n",
      " - 13s - loss: 0.0271 - acc: 0.9903 - val_loss: 0.3787 - val_acc: 0.9300         \n",
      "\n",
      "Epoch 53/75                                                                      \n",
      " - 13s - loss: 0.0286 - acc: 0.9906 - val_loss: 0.3591 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 54/75                                                                      \n",
      " - 13s - loss: 0.0326 - acc: 0.9878 - val_loss: 0.3866 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 55/75                                                                      \n",
      " - 13s - loss: 0.0295 - acc: 0.9894 - val_loss: 0.3563 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 56/75                                                                      \n",
      " - 13s - loss: 0.0268 - acc: 0.9914 - val_loss: 0.3512 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 57/75                                                                      \n",
      " - 13s - loss: 0.0335 - acc: 0.9889 - val_loss: 0.3525 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 58/75                                                                      \n",
      " - 13s - loss: 0.0410 - acc: 0.9881 - val_loss: 0.3490 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 59/75                                                                      \n",
      " - 13s - loss: 0.0340 - acc: 0.9883 - val_loss: 0.3498 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 60/75                                                                      \n",
      " - 13s - loss: 0.0226 - acc: 0.9931 - val_loss: 0.3626 - val_acc: 0.9283         \n",
      "\n",
      "Epoch 61/75                                                                      \n",
      " - 13s - loss: 0.0349 - acc: 0.9875 - val_loss: 0.3503 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 62/75                                                                      \n",
      " - 13s - loss: 0.0301 - acc: 0.9911 - val_loss: 0.3426 - val_acc: 0.9367         \n",
      "\n",
      "Epoch 63/75                                                                      \n",
      " - 13s - loss: 0.0362 - acc: 0.9861 - val_loss: 0.3667 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 64/75                                                                      \n",
      " - 13s - loss: 0.0285 - acc: 0.9900 - val_loss: 0.3964 - val_acc: 0.9300         \n",
      "\n",
      "Epoch 65/75                                                                      \n",
      " - 13s - loss: 0.0286 - acc: 0.9900 - val_loss: 0.3889 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 66/75                                                                      \n",
      " - 13s - loss: 0.0286 - acc: 0.9911 - val_loss: 0.3705 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 67/75                                                                      \n",
      " - 13s - loss: 0.0367 - acc: 0.9886 - val_loss: 0.3732 - val_acc: 0.9325         \n",
      "\n",
      "Epoch 68/75                                                                      \n",
      " - 13s - loss: 0.0394 - acc: 0.9861 - val_loss: 0.3750 - val_acc: 0.9333         \n",
      "\n",
      "Epoch 69/75                                                                      \n",
      " - 13s - loss: 0.0265 - acc: 0.9925 - val_loss: 0.3704 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 70/75                                                                      \n",
      " - 13s - loss: 0.0316 - acc: 0.9908 - val_loss: 0.3864 - val_acc: 0.9317         \n",
      "\n",
      "Epoch 71/75                                                                      \n",
      " - 13s - loss: 0.0386 - acc: 0.9883 - val_loss: 0.3715 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 72/75                                                                      \n",
      " - 13s - loss: 0.0439 - acc: 0.9883 - val_loss: 0.3923 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 73/75                                                                      \n",
      " - 13s - loss: 0.0312 - acc: 0.9906 - val_loss: 0.3940 - val_acc: 0.9308         \n",
      "\n",
      "Epoch 74/75                                                                      \n",
      " - 13s - loss: 0.0300 - acc: 0.9900 - val_loss: 0.4082 - val_acc: 0.9292         \n",
      "\n",
      "Epoch 75/75                                                                      \n",
      " - 13s - loss: 0.0266 - acc: 0.9906 - val_loss: 0.3994 - val_acc: 0.9308         \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                       \n",
      " - 12s - loss: 3.1681 - acc: 0.2889 - val_loss: 2.7145 - val_acc: 0.5742         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 11s - loss: 2.0056 - acc: 0.6258 - val_loss: 1.3860 - val_acc: 0.8392         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 11s - loss: 0.9756 - acc: 0.8275 - val_loss: 0.6880 - val_acc: 0.9258         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 11s - loss: 0.4717 - acc: 0.9225 - val_loss: 0.4080 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 11s - loss: 0.2627 - acc: 0.9528 - val_loss: 0.2910 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 11s - loss: 0.1470 - acc: 0.9758 - val_loss: 0.2515 - val_acc: 0.9375         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 11s - loss: 0.0845 - acc: 0.9858 - val_loss: 0.2163 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 11s - loss: 0.0540 - acc: 0.9908 - val_loss: 0.2053 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 11s - loss: 0.0374 - acc: 0.9931 - val_loss: 0.1936 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 11s - loss: 0.0254 - acc: 0.9950 - val_loss: 0.1996 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 11s - loss: 0.0151 - acc: 0.9975 - val_loss: 0.1972 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 11s - loss: 0.0168 - acc: 0.9961 - val_loss: 0.1970 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 11s - loss: 0.0151 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 11s - loss: 0.0090 - acc: 0.9986 - val_loss: 0.2019 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 11s - loss: 0.0071 - acc: 0.9992 - val_loss: 0.2160 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 11s - loss: 0.0081 - acc: 0.9978 - val_loss: 0.2100 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 11s - loss: 0.0061 - acc: 0.9986 - val_loss: 0.2243 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 18/75                                                                      \n",
      " - 11s - loss: 0.0048 - acc: 0.9989 - val_loss: 0.2116 - val_acc: 0.9483         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 11s - loss: 0.0052 - acc: 0.9992 - val_loss: 0.2339 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 11s - loss: 0.0079 - acc: 0.9978 - val_loss: 0.2196 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 11s - loss: 0.0052 - acc: 0.9983 - val_loss: 0.2344 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 11s - loss: 0.0057 - acc: 0.9981 - val_loss: 0.2504 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 11s - loss: 0.0062 - acc: 0.9978 - val_loss: 0.2384 - val_acc: 0.9458         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 11s - loss: 0.0028 - acc: 0.9989 - val_loss: 0.2526 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 11s - loss: 0.0056 - acc: 0.9978 - val_loss: 0.2603 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 11s - loss: 0.0039 - acc: 0.9986 - val_loss: 0.2550 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 11s - loss: 0.0040 - acc: 0.9989 - val_loss: 0.2518 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 28/75                                                                      \n",
      " - 11s - loss: 0.0044 - acc: 0.9978 - val_loss: 0.2730 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 29/75                                                                      \n",
      " - 11s - loss: 0.0036 - acc: 0.9983 - val_loss: 0.2625 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 30/75                                                                      \n",
      " - 11s - loss: 0.0021 - acc: 0.9992 - val_loss: 0.2842 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 31/75                                                                      \n",
      " - 11s - loss: 0.0028 - acc: 0.9986 - val_loss: 0.2588 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 32/75                                                                      \n",
      " - 11s - loss: 0.0030 - acc: 0.9983 - val_loss: 0.2645 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 33/75                                                                      \n",
      " - 11s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.2730 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 34/75                                                                      \n",
      " - 11s - loss: 0.0051 - acc: 0.9983 - val_loss: 0.2848 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 35/75                                                                      \n",
      " - 11s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.2886 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 36/75                                                                      \n",
      " - 11s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.2943 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 37/75                                                                      \n",
      " - 11s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.3022 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 38/75                                                                      \n",
      " - 11s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.2889 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 39/75                                                                      \n",
      " - 11s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.2892 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 40/75                                                                      \n",
      " - 11s - loss: 0.0028 - acc: 0.9989 - val_loss: 0.3047 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 41/75                                                                      \n",
      " - 11s - loss: 6.7472e-04 - acc: 1.0000 - val_loss: 0.2984 - val_acc: 0.9417     \n",
      "\n",
      "Epoch 42/75                                                                      \n",
      " - 11s - loss: 0.0025 - acc: 0.9989 - val_loss: 0.2885 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 43/75                                                                      \n",
      " - 11s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.2994 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 44/75                                                                      \n",
      " - 11s - loss: 0.0038 - acc: 0.9992 - val_loss: 0.3012 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 45/75                                                                      \n",
      " - 11s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.2962 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 46/75                                                                      \n",
      " - 11s - loss: 0.0014 - acc: 0.9992 - val_loss: 0.3005 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 47/75                                                                      \n",
      " - 11s - loss: 0.0038 - acc: 0.9986 - val_loss: 0.3018 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 48/75                                                                      \n",
      " - 11s - loss: 7.4408e-04 - acc: 0.9997 - val_loss: 0.3266 - val_acc: 0.9358     \n",
      "\n",
      "Epoch 49/75                                                                      \n",
      " - 11s - loss: 0.0045 - acc: 0.9981 - val_loss: 0.3118 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 50/75                                                                      \n",
      " - 11s - loss: 0.0015 - acc: 0.9989 - val_loss: 0.3048 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 51/75                                                                      \n",
      " - 11s - loss: 0.0022 - acc: 0.9992 - val_loss: 0.3005 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 52/75                                                                      \n",
      " - 11s - loss: 0.0025 - acc: 0.9989 - val_loss: 0.3106 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 53/75                                                                      \n",
      " - 11s - loss: 3.6970e-04 - acc: 1.0000 - val_loss: 0.3144 - val_acc: 0.9425     \n",
      "\n",
      "Epoch 54/75                                                                      \n",
      " - 11s - loss: 4.6066e-04 - acc: 1.0000 - val_loss: 0.3125 - val_acc: 0.9425     \n",
      "\n",
      "Epoch 55/75                                                                      \n",
      " - 11s - loss: 6.5322e-04 - acc: 0.9997 - val_loss: 0.3205 - val_acc: 0.9425     \n",
      "\n",
      "Epoch 56/75                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 11s - loss: 0.0021 - acc: 0.9992 - val_loss: 0.3175 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 57/75                                                                      \n",
      " - 11s - loss: 0.0015 - acc: 0.9994 - val_loss: 0.3422 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 58/75                                                                      \n",
      " - 11s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.3506 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 59/75                                                                      \n",
      " - 11s - loss: 0.0043 - acc: 0.9994 - val_loss: 0.3219 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 60/75                                                                      \n",
      " - 11s - loss: 0.0034 - acc: 0.9986 - val_loss: 0.3253 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 61/75                                                                      \n",
      " - 11s - loss: 4.1164e-04 - acc: 1.0000 - val_loss: 0.3497 - val_acc: 0.9417     \n",
      "\n",
      "Epoch 62/75                                                                      \n",
      " - 11s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.3557 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 63/75                                                                      \n",
      " - 11s - loss: 0.0018 - acc: 0.9992 - val_loss: 0.3409 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 64/75                                                                      \n",
      " - 11s - loss: 6.3180e-04 - acc: 0.9994 - val_loss: 0.3376 - val_acc: 0.9400     \n",
      "\n",
      "Epoch 65/75                                                                      \n",
      " - 11s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.3308 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 66/75                                                                      \n",
      " - 11s - loss: 0.0012 - acc: 0.9994 - val_loss: 0.3486 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 67/75                                                                      \n",
      " - 11s - loss: 0.0026 - acc: 0.9994 - val_loss: 0.3329 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 68/75                                                                      \n",
      " - 11s - loss: 9.2833e-04 - acc: 0.9994 - val_loss: 0.3445 - val_acc: 0.9425     \n",
      "\n",
      "Epoch 69/75                                                                      \n",
      " - 12s - loss: 3.3705e-04 - acc: 1.0000 - val_loss: 0.3469 - val_acc: 0.9383     \n",
      "\n",
      "Epoch 70/75                                                                      \n",
      " - 12s - loss: 0.0020 - acc: 0.9989 - val_loss: 0.3385 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 71/75                                                                      \n",
      " - 11s - loss: 4.5631e-04 - acc: 1.0000 - val_loss: 0.3363 - val_acc: 0.9383     \n",
      "\n",
      "Epoch 72/75                                                                      \n",
      " - 11s - loss: 5.1967e-04 - acc: 0.9997 - val_loss: 0.3484 - val_acc: 0.9433     \n",
      "\n",
      "Epoch 73/75                                                                      \n",
      " - 11s - loss: 0.0012 - acc: 0.9992 - val_loss: 0.3543 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 74/75                                                                      \n",
      " - 11s - loss: 6.0876e-04 - acc: 1.0000 - val_loss: 0.3638 - val_acc: 0.9400     \n",
      "\n",
      "Epoch 75/75                                                                      \n",
      " - 11s - loss: 0.0010 - acc: 0.9997 - val_loss: 0.3500 - val_acc: 0.9425         \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                       \n",
      " - 14s - loss: 3.1555 - acc: 0.2344 - val_loss: 2.6981 - val_acc: 0.5317         \n",
      "\n",
      "Epoch 2/75                                                                       \n",
      " - 13s - loss: 2.0556 - acc: 0.5803 - val_loss: 1.4474 - val_acc: 0.8300         \n",
      "\n",
      "Epoch 3/75                                                                       \n",
      " - 13s - loss: 1.0961 - acc: 0.7944 - val_loss: 0.7584 - val_acc: 0.9175         \n",
      "\n",
      "Epoch 4/75                                                                       \n",
      " - 14s - loss: 0.5642 - acc: 0.8933 - val_loss: 0.4705 - val_acc: 0.9217         \n",
      "\n",
      "Epoch 5/75                                                                       \n",
      " - 13s - loss: 0.3199 - acc: 0.9422 - val_loss: 0.3199 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 6/75                                                                       \n",
      " - 14s - loss: 0.1774 - acc: 0.9658 - val_loss: 0.2523 - val_acc: 0.9383         \n",
      "\n",
      "Epoch 7/75                                                                       \n",
      " - 14s - loss: 0.1061 - acc: 0.9803 - val_loss: 0.2331 - val_acc: 0.9358         \n",
      "\n",
      "Epoch 8/75                                                                       \n",
      " - 14s - loss: 0.0638 - acc: 0.9906 - val_loss: 0.2131 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 9/75                                                                       \n",
      " - 13s - loss: 0.0434 - acc: 0.9931 - val_loss: 0.1954 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 10/75                                                                      \n",
      " - 13s - loss: 0.0358 - acc: 0.9942 - val_loss: 0.2202 - val_acc: 0.9342         \n",
      "\n",
      "Epoch 11/75                                                                      \n",
      " - 13s - loss: 0.0240 - acc: 0.9950 - val_loss: 0.2048 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 12/75                                                                      \n",
      " - 13s - loss: 0.0231 - acc: 0.9950 - val_loss: 0.2044 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 13/75                                                                      \n",
      " - 13s - loss: 0.0140 - acc: 0.9967 - val_loss: 0.2021 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 14/75                                                                      \n",
      " - 13s - loss: 0.0128 - acc: 0.9975 - val_loss: 0.2086 - val_acc: 0.9458         \n",
      "\n",
      "Epoch 15/75                                                                      \n",
      " - 13s - loss: 0.0130 - acc: 0.9953 - val_loss: 0.2205 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 16/75                                                                      \n",
      " - 13s - loss: 0.0084 - acc: 0.9969 - val_loss: 0.2118 - val_acc: 0.9475         \n",
      "\n",
      "Epoch 17/75                                                                      \n",
      " - 13s - loss: 0.0066 - acc: 0.9978 - val_loss: 0.2279 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 18/75                                                                      \n",
      " - 13s - loss: 0.0098 - acc: 0.9975 - val_loss: 0.2258 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 19/75                                                                      \n",
      " - 13s - loss: 0.0109 - acc: 0.9978 - val_loss: 0.2401 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 20/75                                                                      \n",
      " - 13s - loss: 0.0045 - acc: 0.9989 - val_loss: 0.2336 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 21/75                                                                      \n",
      " - 13s - loss: 0.0044 - acc: 0.9983 - val_loss: 0.2396 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 22/75                                                                      \n",
      " - 13s - loss: 0.0036 - acc: 0.9992 - val_loss: 0.2364 - val_acc: 0.9475         \n",
      "\n",
      "Epoch 23/75                                                                      \n",
      " - 13s - loss: 0.0051 - acc: 0.9983 - val_loss: 0.2349 - val_acc: 0.9475         \n",
      "\n",
      "Epoch 24/75                                                                      \n",
      " - 13s - loss: 0.0059 - acc: 0.9978 - val_loss: 0.2489 - val_acc: 0.9458         \n",
      "\n",
      "Epoch 25/75                                                                      \n",
      " - 13s - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2561 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 26/75                                                                      \n",
      " - 13s - loss: 0.0056 - acc: 0.9975 - val_loss: 0.2671 - val_acc: 0.9392         \n",
      "\n",
      "Epoch 27/75                                                                      \n",
      " - 13s - loss: 0.0036 - acc: 0.9994 - val_loss: 0.2484 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 28/75                                                                      \n",
      " - 13s - loss: 0.0071 - acc: 0.9986 - val_loss: 0.2696 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 29/75                                                                      \n",
      " - 13s - loss: 0.0045 - acc: 0.9981 - val_loss: 0.2600 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 30/75                                                                      \n",
      " - 13s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.2888 - val_acc: 0.9400         \n",
      "\n",
      "Epoch 31/75                                                                      \n",
      " - 13s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.2764 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 32/75                                                                      \n",
      " - 13s - loss: 0.0021 - acc: 0.9997 - val_loss: 0.2766 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 33/75                                                                      \n",
      " - 13s - loss: 0.0030 - acc: 0.9989 - val_loss: 0.2838 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 34/75                                                                      \n",
      " - 13s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.2736 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 35/75                                                                      \n",
      " - 13s - loss: 0.0040 - acc: 0.9986 - val_loss: 0.2788 - val_acc: 0.9508         \n",
      "\n",
      "Epoch 36/75                                                                      \n",
      " - 13s - loss: 0.0030 - acc: 0.9986 - val_loss: 0.2804 - val_acc: 0.9508         \n",
      "\n",
      "Epoch 37/75                                                                      \n",
      " - 13s - loss: 0.0051 - acc: 0.9981 - val_loss: 0.2961 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 38/75                                                                      \n",
      " - 13s - loss: 0.0056 - acc: 0.9983 - val_loss: 0.3016 - val_acc: 0.9458         \n",
      "\n",
      "Epoch 39/75                                                                      \n",
      " - 13s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.2934 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 40/75                                                                      \n",
      " - 13s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.2886 - val_acc: 0.9475         \n",
      "\n",
      "Epoch 41/75                                                                      \n",
      " - 13s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.2914 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 42/75                                                                      \n",
      " - 13s - loss: 0.0033 - acc: 0.9992 - val_loss: 0.2912 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 43/75                                                                      \n",
      " - 13s - loss: 0.0014 - acc: 0.9994 - val_loss: 0.2860 - val_acc: 0.9483         \n",
      "\n",
      "Epoch 44/75                                                                      \n",
      " - 13s - loss: 0.0018 - acc: 0.9997 - val_loss: 0.3159 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 45/75                                                                      \n",
      " - 13s - loss: 0.0016 - acc: 0.9992 - val_loss: 0.3282 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 46/75                                                                      \n",
      " - 13s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.2977 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 47/75                                                                      \n",
      " - 13s - loss: 0.0026 - acc: 0.9989 - val_loss: 0.3231 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 48/75                                                                      \n",
      " - 13s - loss: 0.0039 - acc: 0.9983 - val_loss: 0.3105 - val_acc: 0.9500         \n",
      "\n",
      "Epoch 49/75                                                                      \n",
      " - 13s - loss: 8.1258e-04 - acc: 1.0000 - val_loss: 0.3254 - val_acc: 0.9458     \n",
      "\n",
      "Epoch 50/75                                                                      \n",
      " - 13s - loss: 7.4661e-04 - acc: 1.0000 - val_loss: 0.3160 - val_acc: 0.9492     \n",
      "\n",
      "Epoch 51/75                                                                      \n",
      " - 13s - loss: 7.8297e-04 - acc: 0.9997 - val_loss: 0.3421 - val_acc: 0.9408     \n",
      "\n",
      "Epoch 52/75                                                                      \n",
      " - 13s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.3206 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 53/75                                                                      \n",
      " - 13s - loss: 0.0028 - acc: 0.9989 - val_loss: 0.3405 - val_acc: 0.9408         \n",
      "\n",
      "Epoch 54/75                                                                      \n",
      " - 13s - loss: 3.7723e-04 - acc: 1.0000 - val_loss: 0.3192 - val_acc: 0.9467     \n",
      "\n",
      "Epoch 55/75                                                                      \n",
      " - 13s - loss: 0.0029 - acc: 0.9994 - val_loss: 0.3063 - val_acc: 0.9458         \n",
      "\n",
      "Epoch 56/75                                                                      \n",
      " - 13s - loss: 0.0014 - acc: 0.9992 - val_loss: 0.3132 - val_acc: 0.9458         \n",
      "\n",
      "Epoch 57/75                                                                      \n",
      " - 13s - loss: 0.0071 - acc: 0.9981 - val_loss: 0.3179 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 58/75                                                                      \n",
      " - 13s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.3176 - val_acc: 0.9475         \n",
      "\n",
      "Epoch 59/75                                                                      \n",
      " - 13s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.3262 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 60/75                                                                      \n",
      " - 13s - loss: 0.0018 - acc: 0.9997 - val_loss: 0.3263 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 61/75                                                                      \n",
      " - 13s - loss: 0.0019 - acc: 0.9992 - val_loss: 0.3416 - val_acc: 0.9417         \n",
      "\n",
      "Epoch 62/75                                                                      \n",
      " - 13s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.3237 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 63/75                                                                      \n",
      " - 13s - loss: 0.0042 - acc: 0.9986 - val_loss: 0.3217 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 64/75                                                                      \n",
      " - 13s - loss: 0.0011 - acc: 0.9997 - val_loss: 0.3193 - val_acc: 0.9483         \n",
      "\n",
      "Epoch 65/75                                                                      \n",
      " - 13s - loss: 8.8263e-04 - acc: 0.9997 - val_loss: 0.3283 - val_acc: 0.9458     \n",
      "\n",
      "Epoch 66/75                                                                      \n",
      " - 13s - loss: 0.0030 - acc: 0.9992 - val_loss: 0.3262 - val_acc: 0.9483         \n",
      "\n",
      "Epoch 67/75                                                                      \n",
      " - 13s - loss: 0.0017 - acc: 0.9997 - val_loss: 0.3072 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 68/75                                                                      \n",
      " - 13s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.3527 - val_acc: 0.9433         \n",
      "\n",
      "Epoch 69/75                                                                      \n",
      " - 13s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.3364 - val_acc: 0.9425         \n",
      "\n",
      "Epoch 70/75                                                                      \n",
      " - 13s - loss: 3.2081e-04 - acc: 1.0000 - val_loss: 0.3300 - val_acc: 0.9442     \n",
      "\n",
      "Epoch 71/75                                                                      \n",
      " - 13s - loss: 0.0014 - acc: 0.9992 - val_loss: 0.3321 - val_acc: 0.9442         \n",
      "\n",
      "Epoch 72/75                                                                      \n",
      " - 13s - loss: 6.4479e-04 - acc: 0.9997 - val_loss: 0.3488 - val_acc: 0.9433     \n",
      "\n",
      "Epoch 73/75                                                                      \n",
      " - 13s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.3299 - val_acc: 0.9467         \n",
      "\n",
      "Epoch 74/75                                                                      \n",
      " - 13s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.3414 - val_acc: 0.9450         \n",
      "\n",
      "Epoch 75/75                                                                      \n",
      " - 13s - loss: 7.7836e-04 - acc: 0.9997 - val_loss: 0.3512 - val_acc: 0.9450     \n",
      "\n",
      "Train on 3600 samples, validate on 1200 samples                                  \n",
      "Epoch 1/75                                                                      \n",
      " 90%|█████████ | 9/10 [2:08:34<15:00, 900.39s/it, best loss: -0.936911111111111]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[281396,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_19/RMSprop/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](RMSprop_19/lr/read, training_19/RMSprop/gradients/dense_58/MatMul_grad/MatMul_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/informatik_programme/german_text_classification_nlp/tutorials/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[281396,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_19/RMSprop/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](RMSprop_19/lr/read, training_19/RMSprop/gradients/dense_58/MatMul_grad/MatMul_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Kapitel 13 - Hyperparameteroptimierung mit Keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
