{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 9 - XGBoost & CARTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Kapitelübersicht <a class=\"anchor\" id=\"9-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Bis jetzt hatten wir probabilistische Klassifizierungsverfahren wie Logistic Regression und Naive Bayes sowie die Kernel-Methode SVM kennengelernt. Eine letzte wichtige Art von Klassifizierungsmethoden sind die sogenannten <b>decision trees</b> (= Entscheidungsbäume).\n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[9.1. Kapitelübersicht](#9-1)<br>\n",
    "\n",
    "Am Ende dieses Kapitel werden wir folgende Themen behandelt und/oder vertieft haben:\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Decision trees (CARTs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Decision trees</b> (deutsch: Entscheidungsbäume) sind geordnete und gerichtete Bäume und dienen der Darstellung von Entscheidungsregeln. Oft werden diese Verfahren auch als <b>CART</b> (= Classification and Regression Trees) bezeichnet, welches ein Algorithmus für decision trees für Klassifikation und Regression ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> Baum (Datenstruktur)\n",
    "    \n",
    "<b>Bäume</b> sind Datenstrukturen, mit der sich hierarchische Strukturen abbilden lassen. Sie besitzen <b>Knoten</b> (englisch: nodes) und <b>Kanten</b> (englisch: edges). Der erste Knoten nennt sich <b>Wurzel</b> (englisch: root). In dem folgenden Beispiel ist \"CD\" die Wurzel. Von ihr aus gehen Kanten zu anderen Knoten wie \"Rock\", \"Folk\", \"Jazz\" und \"Klassik\". Untergeordnete Knoten nennt man auch <b>Kindknoten</b> (englisch: child nodes) und übergeordnete Knoten <b>Elternknoten</b> (englisch: parent nodes). \"Rock\", \"Folk\", \"Jazz\" und \"Klassik\" sind die Kindknoten von \"CD\" und \"CD\" ist der Elternknoten von \"Rock\", \"Folk\", \"Jazz\" und \"Klassik\". Die Kindknoten von \"Rock\" nennt man <b>Blätter</b> (englisch: leaves), da diese selbst keine Kindknoten besitzen.<br>\n",
    "\n",
    "<img src=\"https://wvsg.schulen2.regensburg.de/joomla/images/Faecher/Informatik/Informatik_11/Bilder/2_1_Von_der_Liste_zum_Baum/baum_allg.png\" alt=\"Tree\" width=\"450px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Klassifikationsvorgang von Decision trees lässt sich am besten an einem graphischen Beispiel erläutern:\n",
    "\n",
    "<img src=\"tutorialdata/img/decision_tree_german.png\" alt=\"Decision Tree\" width=\"350px\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu sehen ist ein Binärbaum. Jeder Wurzelknoten repräsentiert eine Eingabevariable $x$ dar und einen Punkt, an der sich die Variable teilt. Die Blattknoten des Baums enthalten eine Ausgabevariable $y$, die zur Vorhersage verwendet wird. Die möglichen Ausgaben sind hier \"Erwachsener\", \"Erwachsener\", \"Kind\". In diesem Beispiel wird anhand der Körpergröße und des Gewichts bestimmt, ob eine Person ein Kind oder ein Erwachsener ist. Wir können diesen Baum auch als Regeln modellieren:<br>\n",
    "```\n",
    "If Größe > 180cm Then Erwachsener\n",
    "If Größe <= 180cm AND Gewicht > 70kg Then Erwachsener\n",
    "If Größe <= 180cm AND Gewicht <= 70kg Then Kind\n",
    "```\n",
    "\n",
    "Vorhersagen können somit relativ einfach getroffen werden. Für den Input wird vom Wurzelknoten aus gesehen der Baum durchlaufen und der Blattknoten als Vorhersage ausgegeben. Dies könnte so aussehen:\n",
    "\n",
    "```\n",
    "Größe > 180cm: False\n",
    "Gewicht > 70 kg: False\n",
    "Therefore: Kind\n",
    "```\n",
    "\n",
    "Die Schwierigkeit bei der Erstellung eines Decision trees ist die Modellierung. Ein Binärbaum wie aus dem Beispiel ist eigentlich ein aufgeteilter Eingaberaum. Jede Eingabevariable $x$ ist eine Dimension des x-dimensionalen Raums. Der Decision tree wird dann mithilfe einer Aufteilung des Eingaberaums erstellt. Diese Aufteilung erfolgt <b>greedy</b> (deutsch: gierig), d.h. es werden immer die besten Punkte gesucht, an denen der Raum aufgeteilt werden kann. Mathematisch geschieht dies durch eine <b>Cost Function</b>[<sup>1</sup>](#fn1). Bei der Klassifikation wird diese Cost Function durch die <b>Gini Index Funktion</b> repräsentiert. Diese wählt die Entscheidungsregeln aus, bei der die Kindknoten möglichst <u>rein</u>[<sup>2</sup>](#fn2) werden.\n",
    "\n",
    "HIER WEITER: https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<span id=\"fn1\" style=\"font-size:8pt; line-height:1\"><sup style=\"font-size:5pt\">1</sup> &nbsp; Cost Function werden uns später bei Neuronalen Netzen im Bereich des Deep Learning erneut begegnen (siehe Kapitel 11).</span><br>\n",
    "<span id=\"fn2\" style=\"font-size:8pt; line-height:1\"><sup style=\"font-size:5pt\">2</sup> &nbsp; Was <b>Reinheit</b> (= purity) bedeutet, wird bei dieser <a href=\"https://stats.stackexchange.com/questions/220321/what-is-node-impurity-purity-in-decision-trees-in-plain-english-why-do-we-need\">Antwort</a> bei StackExchange erklärt.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://de.wikipedia.org/wiki/CART_(Algorithmus)\n",
    "- https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/\n",
    "- https://de.wikipedia.org/wiki/Entscheidungsbaum\n",
    "- https://de.wikipedia.org/wiki/Bin%C3%A4rbaum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Korpus laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufteilung in Trainings- und Testdatensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "labels = LabelEncoder().fit_transform(corpus[\"category\"])\n",
    "vector  = TfidfVectorizer().fit_transform(corpus[\"text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "classes = corpus[\"category\"].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-improve-machine-learning-results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Aufgabe:</b> XGBoost Paper lesen\n",
    "  \n",
    "Machine Learning ist ein Feld ist, in dem viel geforscht wird und es ständig neue Errungenschaften gibt. Diese Errungenschaften werden in Form von wissenschaftlichen Papers veröffentlicht. Implementierungen wie in Scikit learn erfolgen erst Jahre später, im Falle von XGBoost steht eine direkte Implementierung wie von Verfahren wie Naive Bayes oder Logistic Regression noch aus (Stand: 05.09.2019). Um die aktuellesten Verfahren nutzen zu können, müssen neue Machine Learning Verfahren entweder selbst implementiert werden oder auf noch nicht perfektionierte(?) Implementierungen zurückgegriffen werden. Neben fehlenden Implementierungen ist ein weiteres Problem das Fehlen von Guides oder Tutorials. Deshalb ist das Lesen von wissenschaftlichen Papers und der Umgang mit diesen eine Fähigkeit, die für eine Arbeit im Bereich des Machine Learnings unabdingbar ist.<br>\n",
    "  \n",
    "Lesen Sie zum Einstieg das <a href=\"https://arxiv.org/abs/1603.02754\">Paper</a> zu XGBoost. Versuchen Sie, wichtige Errungenschaften, Details und Formeln nachzuvollziehen und zu verstehen. Machen Sie sich Notizen und schlagen Sie Unbekanntes nach. Lassen Sie sich nicht von den Formeln abschrecken, Sie müssen nicht alles 100% verstehen AF!). Der Vorteil bei diesem Paper ist, dass es sehr verständliche Beispiele bietet.\n",
    "\n",
    "\n",
    "Wenn nicht alles sofort verständlich ist, ist das kein Problem, da die wichtigsten Punkte in diesem Abschnitt nochmal erläutert werden. XGBoost ist aber komplexer als andere Klassifikationsverfahren, weshalb Vorwissen in Form des Papers hilfreich für das Verständnis und die Übung vom Lesen von wissenschaftlichen Papers ist (AF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb = xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der F1-score für die Klassifizierung mit XGBoost ist 0.888.\n"
     ]
    }
   ],
   "source": [
    "# F1-score des Testdatensatzes\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "xgb_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(\"Der F1-score für die Klassifizierung mit XGBoost ist \"\n",
    "      + f\"{str(np.around(xgb_f1, decimals=3))}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Mittelwert der cross validation bei der  Klassifizierung  mit XGBoost ist 0.873.\n",
      "\n",
      "CPU times: user 35min 48s, sys: 396 ms, total: 35min 49s\n",
      "Wall time: 35min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cross validation des Trainingsdatensatzes\n",
    "xgb_scores = cross_val_score(xgb_classifier, vector, labels, cv=3)\n",
    "xgb_mean = np.mean(xgb_scores)\n",
    "\n",
    "print(\"Der Mittelwert der cross validation bei der  Klassifizierung \" \n",
    "      + f\" mit XGBoost ist {str(np.around(xgb_mean, decimals=3))}.\"\n",
    "      + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Dokumentation: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost mit scikit learn: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "- xgboost vs sklearn gradient boosting: https://stats.stackexchange.com/questions/282459/xgboost-vs-python-sklearn-gradient-boosted-trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICH: kein GridSearch machen, zu langsam (siehe medium \"catboost vs light gbm vs xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
